{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Student: Dorin Doncenco"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the binarized MNIST database to:\n",
    "\n",
    "·        Generate images with autoregressive models available in the pytorch library\n",
    "\n",
    "·        What are the encountered difficulties (initialization…)\n",
    "\n",
    "·        Comment on the obtained results according to the used settings \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import matplotlib\n",
    "import math\n",
    "\n",
    "\n",
    "import dataset_loader\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "mnist_path = \"./mnist.pkl.gz\"\n",
    "\n",
    "# the dataset contains 3 splits (train/dev/test),\n",
    "# each one containing two vectors (pixels and classes)\n",
    "(train_data_pixels, train_data_classes), \\\n",
    "(dev_data_pixels, dev_data_classes), _ = dataset_loader.load_mnist(mnist_path)\n",
    "\n",
    "#convert data to pytorch tensors\n",
    "train_data_pixels = torch.from_numpy(train_data_pixels)\n",
    "train_data_classes = torch.from_numpy(train_data_classes)\n",
    "dev_data_pixels = torch.from_numpy(dev_data_pixels)\n",
    "dev_data_classes = torch.from_numpy(dev_data_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Masked Autoencoder for Distribution Estimation\n",
    "\n",
    "class MADE(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes, num_masks=1, natural_ordering=False):\n",
    "        \"\"\"\n",
    "        :param input_size: integer specifying the size of the input\n",
    "        :param hidden_sizes: a list of integers specifying the size of each hidden layer\n",
    "        :param num_masks: the number of orderings to use over the inputs. Paper says 1 is enough, more is faster.\n",
    "        :param natural_ordering: force natural ordering of dimensions, don't use random permutations.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_sizes = hidden_sizes\n",
    "        self.num_masks = num_masks\n",
    "        self.natural_ordering = natural_ordering\n",
    "\n",
    "        # define a simple MLP neural net\n",
    "        self.net = []\n",
    "        hs = [input_size] + hidden_sizes + [input_size * num_masks]\n",
    "        for h0, h1 in zip(hs, hs[1:]):\n",
    "            self.net.extend([\n",
    "                nn.Linear(h0, h1),\n",
    "                nn.ReLU(),\n",
    "            ])\n",
    "        self.net.pop()  # pop the last ReLU for the output layer\n",
    "        self.net = nn.Sequential(*self.net)\n",
    "\n",
    "        # seeds for orders and masks\n",
    "        self.seed = 0\n",
    "        self.m = {}\n",
    "        self.m[-1] = torch.arange(self.input_size)\n",
    "\n",
    "        # masks is a dictionary of masks\n",
    "        self.masks = {}\n",
    "        for i in range(self.num_masks):\n",
    "            self.masks[i] = {}\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        :param x: Tensor of shape [batch_size, input_size]\n",
    "        :return: Tensor of shape [batch_size, input_size, num_masks]\n",
    "        \"\"\"\n",
    "        # forward the MADE model\n",
    "        logits = self.net(x)\n",
    "\n",
    "        # reshape the output\n",
    "        logits = logits.view(logits.size(0), self.num_masks, self.input_size)\n",
    "\n",
    "        # apply softmax to the last dimension\n",
    "        # and return the probabilities\n",
    "        return F.softmax(logits, dim=-1)\n",
    "\n",
    "    def sample(self, x):\n",
    "        \"\"\"\n",
    "        :param x: Tensor of shape [batch_size, input_size]\n",
    "        :return: Tensor of shape [batch_size, input_size]\n",
    "        \"\"\"\n",
    "        # get the probabilities\n",
    "        probs = self.forward(x)\n",
    "\n",
    "        # sample from the distribution\n",
    "        # and return the sample\n",
    "        return torch.multinomial(probs[:, -1], 1).squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyperparameters\n",
    "input_size = 784\n",
    "hidden_sizes = [512, 512]\n",
    "num_masks = 1\n",
    "natural_ordering = False\n",
    "\n",
    "# create the model\n",
    "model = MADE(input_size, hidden_sizes, num_masks, natural_ordering)\n",
    "# define the loss function\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "# define the optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "# number of epochs\n",
    "num_epochs = 10\n",
    "# batch size\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "expected scalar type Long but found Float",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mw:\\Paris-Saclay\\m2-paris-saclay\\ProbGenModels\\lab_autoregressive\\lab_auto.ipynb Cell 6\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/w%3A/Paris-Saclay/m2-paris-saclay/ProbGenModels/lab_autoregressive/lab_auto.ipynb#X10sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m# forward + backward + optimize\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/w%3A/Paris-Saclay/m2-paris-saclay/ProbGenModels/lab_autoregressive/lab_auto.ipynb#X10sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m outputs \u001b[39m=\u001b[39m model(batch_x)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/w%3A/Paris-Saclay/m2-paris-saclay/ProbGenModels/lab_autoregressive/lab_auto.ipynb#X10sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m loss \u001b[39m=\u001b[39m loss_function(outputs, batch_x)\n\u001b[0;32m     <a href='vscode-notebook-cell:/w%3A/Paris-Saclay/m2-paris-saclay/ProbGenModels/lab_autoregressive/lab_auto.ipynb#X10sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m     <a href='vscode-notebook-cell:/w%3A/Paris-Saclay/m2-paris-saclay/ProbGenModels/lab_autoregressive/lab_auto.ipynb#X10sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\Users\\deus-diabolus\\miniconda3\\envs\\uni_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\deus-diabolus\\miniconda3\\envs\\uni_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\deus-diabolus\\miniconda3\\envs\\uni_env\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:1179\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m   1178\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor, target: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m-> 1179\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mcross_entropy(\u001b[39minput\u001b[39;49m, target, weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight,\n\u001b[0;32m   1180\u001b[0m                            ignore_index\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mignore_index, reduction\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreduction,\n\u001b[0;32m   1181\u001b[0m                            label_smoothing\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlabel_smoothing)\n",
      "File \u001b[1;32mc:\\Users\\deus-diabolus\\miniconda3\\envs\\uni_env\\Lib\\site-packages\\torch\\nn\\functional.py:3053\u001b[0m, in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[0;32m   3051\u001b[0m \u001b[39mif\u001b[39;00m size_average \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m reduce \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   3052\u001b[0m     reduction \u001b[39m=\u001b[39m _Reduction\u001b[39m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m-> 3053\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_nn\u001b[39m.\u001b[39;49mcross_entropy_loss(\u001b[39minput\u001b[39;49m, target, weight, _Reduction\u001b[39m.\u001b[39;49mget_enum(reduction), ignore_index, label_smoothing)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: expected scalar type Long but found Float"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "for epoch in range(num_epochs):\n",
    "    # shuffle the data\n",
    "    permutation = torch.randperm(train_data_pixels.size()[0])\n",
    "    # get the inputs\n",
    "    for i in range(0, train_data_pixels.size()[0], batch_size):\n",
    "        indices = permutation[i:i+batch_size]\n",
    "        batch_x, batch_y = train_data_pixels[indices], train_data_classes[indices]\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(batch_x)\n",
    "        loss = loss_function(outputs, batch_x)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    # print the loss\n",
    "    print('epoch [{}/{}], loss:{:.4f}'.format(epoch+1, num_epochs, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 1, 784])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uni_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
