{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP 2 - Bandit Algorithms (November 23)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignement, we will focus on the MultiArmed Bandits problem.\n",
    "You will implement:\n",
    "\n",
    "\n",
    "- Deterministic policy UCB1 normal  https://homes.di.unimi.it/~cesabian/Pubblicazioni/ml-02.pdf or Chapter 7 of Bandit Algorithms\n",
    "\n",
    "- Boltzmann Exploration (Softmax) https://www.cs.mcgill.ca/~vkules/bandits.pdf\n",
    "\n",
    "- KL-UCB https://hal.archives-ouvertes.fr/hal-00738209v2 or Chapter 10.2 of Bandits Algorithm\n",
    "\n",
    "- Best Empirical Sampled Average (BESA) https://hal.archives-ouvertes.fr/hal-01025651v1/document \n",
    "\n",
    "- Thompson Sampling Agent  https://web.stanford.edu/~bvr/pubs/TS_Tutorial.pdf, https://en.wikipedia.org/wiki/Thompson_sampling or Chapter 35 and 36 of Bandits Algorithms\n",
    "\n",
    "\n",
    "![image not found:](multiarmedbandit.jpg \"Bandits\")\n",
    "\n",
    "You will be evaluated on:\n",
    "* Implementation of the agents. Points will be granted to clean, scalable code.\n",
    "* A small paragraph with an analysis of the behavior of the agent and your understanding of the algorithm\n",
    "* Answering this question -> for each implemented agent, give 1 pros and 1 cons ?\n",
    "\n",
    "Send this notebook  to cyriaque.rousselot@inria.fr before next course.\n",
    "\n",
    "Good Luck !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example : Random Agent and Epsilon Greedy Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In `choose`, prescribe how the agent selects its\n",
    "actions (interact must return an action, that is\n",
    "an index in `[0, ..., n_a]`).\n",
    "\n",
    "In `update`, implement how the agent updates\n",
    "its knowledge, using the newly observed `action` and `reward`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import runner, environment\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomAgent:\n",
    "    def __init__(self, n_a):\n",
    "        self.n_a = n_a  # number of possible actions\n",
    "        \"\"\"Init a new agent.\n",
    "        \"\"\"\n",
    "\n",
    "    def choose(self):\n",
    "        \"\"\"Acts in the environment.\n",
    "\n",
    "        returns the chosen action.\n",
    "        \"\"\"\n",
    "        return np.random.randint(0, self.n_a)\n",
    "\n",
    "    def update(self, action, reward):\n",
    "        \"\"\"Receive a reward for performing given action on\n",
    "        given observation.\n",
    "\n",
    "        This is where your agent can learn.\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class epsGreedyAgent:\n",
    "    def __init__(self, n_a, epsilon):\n",
    "        self.n_a = n_a  # number of possible actions\n",
    "        self.A = range(n_a)\n",
    "        self.mu = {a: [] for a in self.A}\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def choose(self):\n",
    "        \"\"\"Acts in the environment.\n",
    "\n",
    "        returns the chosen action.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def update(self, action, reward):\n",
    "        \"\"\"Receive a reward for performing given action on\n",
    "        given observation.\n",
    "\n",
    "        This is where your agent can learn.\n",
    "        \"\"\"\n",
    "\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There exists two classes from the ```environment``` package, ```environment.EasyEnvironment``` and ```environment.HardEnvironement```. Try the agents on both for your analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Environment Class takes two arguments: ```n_a``` the number of actions and ```variability```, a parameter between 0 and 1 that influence the spreading of distribution of the reward between bandits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looping on a single environement and a single agent\n",
    "niter = 100\n",
    "n_actions = 10\n",
    "my_agent = RandomAgent(n_actions)\n",
    "my_env = environment.EasyEnvironment(n_actions, variability=1)\n",
    "my_runner = runner.Runner(\n",
    "    my_env, my_agent, verbose=True\n",
    ")  # The verbose parameter allows displaying every step\n",
    "final_reward, list_cumul = my_runner.loop(niter)\n",
    "print(\"Obtained a final reward of {}\".format(final_reward))\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.bar(range(niter),list_cumul)\n",
    "plt.plot(list_cumul,marker='^',color='red')\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"Cum. Reward\")\n",
    "plt.title(\"Agent: Random, Environment: Easy, Variability: 1\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running a batched simulation with n_agents agents in parallel\n",
    "\n",
    "niter = 1000\n",
    "n_agents = 10\n",
    "n_actions = 10\n",
    "\n",
    "my_runner = runner.BatchRunner(\n",
    "    environment.EasyEnvironment, RandomAgent, n_agents, n_actions, False\n",
    ")\n",
    "final_reward_rd, list_cumul_rd = my_runner.loop(niter)\n",
    "print(\"Random Agent obtained a final average reward of {}\".format(final_reward_rd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.plot(list_cumul_rd, label=\"Agent: Random\")\n",
    "plt.xlabel(\"Iter\")\n",
    "plt.ylabel(\"Cum. Reward\")\n",
    "plt.title(\"Random Agent cumulative reward on one simulation on the EasyEnvironment \")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can play with the complexity of the environment by modifying the number of possible actions and variances of the reward distributions . I suggest that you begin with 10 possible actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running a batched simulation with n_agents agents in parallel\n",
    "\n",
    "niter = 1000\n",
    "n_agents = 10\n",
    "n_actions = 30\n",
    "\n",
    "my_runner = runner.BatchRunner(\n",
    "    environment.Environment, RandomAgent, n_agents, n_actions, False\n",
    ")\n",
    "final_reward_rd, list_cumul_rd = my_runner.loop(niter)\n",
    "print(\"Random Agent obtained a final average reward of {}\".format(final_reward_rd))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Analysis of Epsilon Greedy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UCB-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UCBAgent:\n",
    "    # https://homes.di.unimi.it/~cesabian/Pubblicazioni/ml-02.pdf\n",
    "    def __init__(self, n_actions):\n",
    "        self.A = range(n_actions)\n",
    "\n",
    "    def choose(self):\n",
    "        \"\"\"Acts in the environment.\n",
    "\n",
    "        returns the chosen action.\n",
    "        \"\"\"\n",
    "        raise NotImplemented\n",
    "\n",
    "    def update(self, a, r):\n",
    "        \"\"\"Receive a reward for performing given action on\n",
    "        given observation.\n",
    "\n",
    "        This is where your agent can learn.\n",
    "        \"\"\"\n",
    "        raise NotImplemented"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Analysis of UCB1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boltzmann Exploration (Softmax) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxAgent:  # Chose a Temperature between 0.01 and 1\n",
    "    # https://www.cs.mcgill.ca/~vkules/bandits.pdf\n",
    "    def __init__(self, n_actions):\n",
    "        self.A = range(n_actions)\n",
    "\n",
    "    def choose(self):\n",
    "        \"\"\"Acts in the environment.\n",
    "\n",
    "        returns the chosen action.\n",
    "        \"\"\"\n",
    "        raise NotImplemented\n",
    "\n",
    "    def update(self, action, reward):\n",
    "        \"\"\"Receive a reward for performing given action on\n",
    "        given observation.\n",
    "\n",
    "        This is where your agent can learn.\n",
    "        \"\"\"\n",
    "        raise NotImplemented"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Analysis of Boltzmann Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BESA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BesaAgent:\n",
    "    # https://hal.archives-ouvertes.fr/file/index/docid/1025651/filename/BESA2.pdf\n",
    "    def __init__(self, n_actions):\n",
    "        self.A = range(n_actions)\n",
    "\n",
    "    def choose(self):\n",
    "        \"\"\"Acts in the environment.\n",
    "\n",
    "        returns the chosen action.\n",
    "        \"\"\"\n",
    "        raise NotImplemented\n",
    "\n",
    "    def update(self, action, reward):\n",
    "        \"\"\"Receive a reward for performing given action on\n",
    "        given observation.\n",
    "\n",
    "        This is where your agent can learn.\n",
    "        \"\"\"\n",
    "        raise NotImplemented"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Analysis of BESA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KL-UCB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KLUCBAgent:\n",
    "    # See: https://hal.archives-ouvertes.fr/hal-00738209v2\n",
    "    def __init__(self, n_actions):\n",
    "        self.A = range(n_actions)\n",
    "\n",
    "    def choose(self):\n",
    "        \"\"\"Acts in the environment.\n",
    "\n",
    "        returns the chosen action.\n",
    "        \"\"\"\n",
    "        raise NotImplemented\n",
    "\n",
    "    def update(self, a, r):\n",
    "        \"\"\"Receive a reward for performing given action on\n",
    "        given observation.\n",
    "\n",
    "        This is where your agent can learn.\n",
    "        \"\"\"\n",
    "        raise NotImplemented"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Analysis of KL-UCB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thompson Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Try first to implement the Bernoulli Bandit Thompson Sampling algorithm on the ```EasyEnvironment```. https://web.stanford.edu/~bvr/pubs/TS_Tutorial.pdf\n",
    "Additional points will be granted for a Thompson Agent on the ```HardEnvironment```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThompsonAgent:\n",
    "    # https://en.wikipedia.org/wiki/Thompson_sampling\n",
    "    def __init__(self, n_actions):\n",
    "        self.A = range(n_actions)\n",
    "\n",
    "    def choose(self):\n",
    "        \"\"\"Acts in the environment.\n",
    "\n",
    "        returns the chosen action.\n",
    "        \"\"\"\n",
    "        raise NotImplemented\n",
    "\n",
    "    def update(self, a, r):\n",
    "        \"\"\"Receive a reward for performing given action on\n",
    "        given observation.\n",
    "\n",
    "        This is where your agent can learn.\n",
    "        \"\"\"\n",
    "        raise NotImplemented"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Analysis of Thompson Sampling"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('rl_master')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6d3bda9ce93bcf6253578f72b020ff104c4a49c94ced054907684331a902bb6b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
