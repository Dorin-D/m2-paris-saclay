{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Student: Dorin Doncenco\n",
    "\n",
    "Todo: Learn MCTSA maybe read some youtube vids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP 3 - Planning (November 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![ChessUrl](https://gymnasium.farama.org/_images/frozen_lake.gif \"Frozen Lake\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignement, we focus on algorithms that require a **model** of the environment behavior. You will implement :\n",
    "\n",
    "- A Monte Carlo Tree Search Algorithm\n",
    "- A Tabular Dyna-Q Algorithm\n",
    "\n",
    "You will be evaluated on:\n",
    "* Implementation of the agents. Points will be granted to clean, scalable code.\n",
    "* A Paragraph of analysis of the behavior of the algorithms . \n",
    "\n",
    "Send this notebook  to cyriaque.rousselot@inria.fr before next course.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "%aimport utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Snapshots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the sake of planning algorithm, we will introduce the possibility of taking snapshots of the environment. Snapshots allows to return to a previously visited state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = utils.WithSnapshots(gym.make(\"FrozenLake-v1\",map_name=\"8x8\",\n",
    "                             render_mode=\"ansi\",\n",
    "                             max_episode_steps=200))\n",
    "env.reset()\n",
    "n_actions = env.action_space.n\n",
    "n_states = env.observation_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial_state:\n",
      "\n",
      "\u001b[41mS\u001b[0mFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "print(\"initial_state:\")\n",
    "print(env.render())\n",
    "# plt.axis('off')\n",
    "env.close()\n",
    "\n",
    "# create first snapshot\n",
    "snap0 = env.get_snapshot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whoops! We died!\n",
      "final state:\n",
      "  (Right)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFF\u001b[41mH\u001b[0mFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    _, _, terminated, truncated, _ = env.step(env.action_space.sample())\n",
    "    if terminated:\n",
    "        print(\"Whoops! We died!\")\n",
    "        break\n",
    "    if truncated:\n",
    "        print(\"Time is over!\")\n",
    "        break\n",
    "\n",
    "print(\"final state:\")\n",
    "print(env.render())\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After loading snapshot\n",
      "\n",
      "\u001b[41mS\u001b[0mFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# reload initial state\n",
    "env.load_snapshot(snap0)\n",
    "\n",
    "print(\"After loading snapshot\")\n",
    "print(env.render())\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte Carlo Tree Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://en.wikipedia.org/wiki/Monte_Carlo_tree_search ; Sutton-Barto Chapter 8.11\n",
    "\n",
    "The MCTS algorithm we will implement can be divided in 4 steps:\n",
    "- Selection \n",
    "- Expansion\n",
    "- Simulation\n",
    "- Backpropagation\n",
    "\n",
    "The first step is exploring the current tree using a UCB-1 rule until we get to a leaf L .\n",
    "\n",
    "The second is creating a child C from feasable moves after the leaf L if the game is not finished.\n",
    "\n",
    "The third is simulating the end of the game with an unbiased method to get an estimate of the value of the position C.\n",
    "\n",
    "The fourth is updating the value estimation of the position of all nodes visited during the exploration of the tree.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![image.png](https://i.postimg.cc/6QmwnjPS/image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use snapshots to simulate the effect of a sample model:\n",
    "1. Saving a snapshot of state S\n",
    "2. sending S,A to the environement\n",
    "3. Getting back R and S'\n",
    "4. When needed, loading the snapshot of state S"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the agent:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Fill the blanks in the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    \"\"\"A node in the Monte Carlo Tree Search (MCTS) algorithm.\"\"\"\n",
    "    \n",
    "    #metadata:\n",
    "    parent = None          #parent Node\n",
    "    qvalue_sum = 0.         #sum of state values from all visits (numerator)\n",
    "    times_visited = 0      #counter of visits (denominator)\n",
    "\n",
    "    def __init__(self, parent, action):\n",
    "        \"\"\"\n",
    "        Initializes a tree node with a parent, action, and environment.\n",
    "\n",
    "        :param parent: parent TreeNode\n",
    "        :param action: action to commit from parent Node\n",
    "        \"\"\"\n",
    "\n",
    "        self.parent = parent\n",
    "        self.action = action\n",
    "        self.children = set()\n",
    "\n",
    "        # Capture the outcome after performing the action in the parent's state\n",
    "        result = env.get_result(parent.snapshot, action)\n",
    "        (\n",
    "            self.snapshot,\n",
    "            self.observation,\n",
    "            self.immediate_reward,\n",
    "            self.is_done,\n",
    "            _,\n",
    "        ) = result\n",
    "\n",
    "    def is_leaf(self):\n",
    "        return not self.children\n",
    "\n",
    "    def is_root(self):\n",
    "        return self.parent is None\n",
    "\n",
    "    def get_qvalue_estimate(self):\n",
    "        if self.times_visited !=0:\n",
    "            return self.qvalue_sum / self.times_visited\n",
    "        return 0\n",
    "\n",
    "    def ucb_score(self, scale=10, max_value=float(\"inf\")):\n",
    "        \"\"\"\n",
    "        Computes the Upper Confidence Bound (UCB) score for the node.\n",
    "\n",
    "        :param scale: Multiplies the upper bound by this value. Assumes reward range to be [0, scale].\n",
    "        :param max_value: a value representing infinity (for unvisited nodes).\n",
    "        \"\"\"\n",
    "        if self.times_visited == 0:\n",
    "            return max_value\n",
    "\n",
    "        return self.get_qvalue_estimate() + scale * np.sqrt(2*np.log(self.parent.times_visited) / self.times_visited)\n",
    "\n",
    "    # MCTS steps\n",
    "\n",
    "    def select_best_leaf(self):\n",
    "        \"\"\"\n",
    "        Selects the leaf with the highest priority to expand.\n",
    "\n",
    "        Recursively picks nodes with the best UCB score until it reaches a leaf.\n",
    "        \"\"\"\n",
    "        # Using the UCB valuation, select the best possible child among children of a node\n",
    "        if self.is_leaf():\n",
    "            return self\n",
    "        children = self.children\n",
    "        \n",
    "        best_child = max(children, key=lambda child: child.ucb_score())\n",
    "        return best_child.select_best_leaf()\n",
    "\n",
    "    def expand(self):\n",
    "        \"\"\"\n",
    "        Expands the current node by creating all possible child nodes.\n",
    "\n",
    "        Returns one of those children.\n",
    "        \"\"\"\n",
    "        # You can't generate a child if there is already an existing child with the same associated action.\n",
    "\n",
    "        assert not self.is_done, \"Can't expand from terminal state\"\n",
    "\n",
    "        return self.select_best_leaf()\n",
    "\n",
    "    def rollout(self, t_max=10**4):\n",
    "        \"\"\"\n",
    "        Plays the game from this state to the end (done) or for t_max steps.\n",
    "\n",
    "        On each step, picks an action at random.\n",
    "\n",
    "        Computes the sum of rewards from the current state until the end of the episode.\n",
    "\n",
    "        If the node is terminal, return the immediate reward\n",
    "        \"\"\"\n",
    "\n",
    "        env.load_snapshot(self.snapshot)\n",
    "        obs = self.observation\n",
    "        is_done = self.is_done\n",
    "\n",
    "        rollout_reward = 0\n",
    "        while not is_done and t_max>0:\n",
    "            action = env.action_space.sample()\n",
    "            # check env step return docs\n",
    "            obs, reward, is_done, truncated, _ = env.step(action)\n",
    "            rollout_reward += reward\n",
    "            t_max -= 1\n",
    "\n",
    "\n",
    "        return rollout_reward\n",
    "\n",
    "    def propagate(self, child_qvalue):\n",
    "        \"\"\"\n",
    "        Uses the child Q-value to update parents number of visits and qvalue recursively.\n",
    "        \"\"\"\n",
    "        my_qvalue = self.immediate_reward + child_qvalue\n",
    "\n",
    "        # Update qvalue_sum and times_visited\n",
    "        self.qvalue_sum += my_qvalue\n",
    "        self.times_visited += 1\n",
    "\n",
    "        # Propagate upwards\n",
    "        if not self.is_root():\n",
    "            self.parent.propagate(my_qvalue)\n",
    "    def safe_delete(self):\n",
    "        \"\"\"safe delete to prevent memory leak in some python versions\"\"\"\n",
    "        del self.parent\n",
    "        for child in self.children:\n",
    "            child.safe_delete()\n",
    "            del child\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 5)\n"
     ]
    }
   ],
   "source": [
    "children = set()\n",
    "children.add((1,3))\n",
    "children.add((2,1))\n",
    "children.add((3,5))\n",
    "\n",
    "#select the child with one value highest\n",
    "max_value = max(children, key=lambda x: max(x))\n",
    "print(max_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Root(Node):\n",
    "    \"\"\"The root node\"\"\"\n",
    "\n",
    "    def __init__(self, snapshot, observation):\n",
    "        self.parent = self.action = None\n",
    "        self.children = set()\n",
    "        self.snapshot = snapshot\n",
    "        self.observation = observation\n",
    "        self.immediate_reward = 0\n",
    "        self.is_done = False\n",
    "\n",
    "    @staticmethod\n",
    "    def from_node(node):\n",
    "        root = Root(node.snapshot, node.observation)\n",
    "        # Copy data\n",
    "        copied_fields = [\"qvalue_sum\", \"times_visited\", \"children\", \"is_done\"]\n",
    "        for field in copied_fields:\n",
    "            setattr(root, field, getattr(node, field))\n",
    "        return root\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the MCTS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plan_mcts(root, n_iters=10):\n",
    "    \"\"\"\n",
    "    Builds a tree with Monte-Carlo Tree Search for n_iters iterations.\n",
    "    :param root: Tree node to plan from.\n",
    "    :param n_iters: Number of select-expand-simulate-propagate loops to make.\n",
    "    \"\"\"\n",
    "    for _ in range(n_iters):\n",
    "        node = #To complete\n",
    "\n",
    "        if node.is_done:\n",
    "            # All rollouts from a terminal node are empty, and thus have 0 reward.\n",
    "            node.propagate(0)\n",
    "        else:\n",
    "            # Expand the best leaf, perform a rollout from it, and propagate the results upwards.\n",
    "\n",
    "            #TO COMPLETE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = utils.WithSnapshots(gym.make(\"FrozenLake-v1\",map_name=\"8x8\",\n",
    "                             render_mode=\"ansi\",\n",
    "                             max_episode_steps=200))\n",
    "root_observation = env.reset()\n",
    "root_snapshot = env.get_snapshot()\n",
    "root = Root(root_snapshot, root_observation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Use the MCTS implementation to find the optimal policy and show it. Bonus point will be given to a clear display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Try it also on the Cartpole problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = utils.WithSnapshots(gym.make(\"CartPole-v1\", render_mode=\"rgb_array\", max_episode_steps=200))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (BONUS) Introducing some Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Planning on each iteration can be costly. You can speed things up drastically if you train a classifier to predict which action will turn out to be best according to MCTS.\n",
    "\n",
    ">To do so, adapt the code and record which action did the MCTS agent take on each step and fit a classifier to [state, mcts_optimal_action]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Learning:  Dyna-Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Implement a Tabular Dyna-Q algorithm ( Chapter 8.2 Barto-Sutton) for the Frozen Lake environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![\"Description of Dyna Algorithm\"](dyna.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DynaAgent:\n",
    "    def __init__(self,env,epsilon=1e-3):\n",
    "        \"\"\"Step (a)\"\"\"\n",
    "        self.n_actions = env.action_space.n\n",
    "        self.n_states = env.observation_space.n\n",
    "        self.epsilon =epsilon\n",
    "        self.q = np.zeros((n_states,n_actions))\n",
    "        self.model = np.zeros((n_states,n_actions,2)) # self.model[s,a] return r and s'\n",
    "        self.env = env\n",
    "        self.current_state,_ = env.reset()\n",
    "         \n",
    "    def choose(self):\n",
    "        \"\"\"Step (b)\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def observe(self,action):\n",
    "        \"\"\"Step (c)\"\"\"\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def update_model(self,s1,a1,r1,s2):\n",
    "        \"\"\"Step (e)\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def update_value(self,s1,a1,r1,s2):\n",
    "        \"\"\"Step (d)\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def planning(self,n_steps):\n",
    "        \"\"\"Step (f)\"\"\"\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> What are some limits of the algorithm ? Does it scale ? Explain. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Parts of the code for this practical has been inspired by https://github.com/yandexdataschool/Practical_RL/_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl_master",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
