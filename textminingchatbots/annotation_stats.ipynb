{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries to read BRAT and conll files\n",
    "import glob\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_brat(folder_location):\n",
    "    ann_files = glob.glob(folder_location + \"*.ann\")\n",
    "\n",
    "    #read ann files and ignore lines starting with #\n",
    "    ann_lines = []\n",
    "    for ann_file in ann_files:\n",
    "        with open(ann_file, 'r') as f:\n",
    "            for line in f:\n",
    "                if not line.startswith(\"#\"):\n",
    "                    ann_lines.append(line)\n",
    "\n",
    "    #split lines into things by tab, we want the second column as the type, third column as the entity\n",
    "    ann_lines_split = []\n",
    "    for line in ann_lines:\n",
    "        splits = line.split(\"\\t\")\n",
    "        ann_lines_split.append([splits[1].split(\" \")[0], splits[2].split(\"\\n\")[0]])\n",
    "    return ann_lines_split\n",
    "\n",
    "def read_conll(file_location):\n",
    "    conll_files = glob.glob(file_location + \"*.conll\")\n",
    "    if len(conll_files) > 1:\n",
    "        raise Exception(\"More than one conll file found in folder\")\n",
    "    elif len(conll_files) == 0:\n",
    "        raise Exception(\"No conll file found in folder\")\n",
    "    else:\n",
    "        file_location = conll_files[0]\n",
    "    lines = []\n",
    "    with open(file_location, 'r') as f:\n",
    "        for line in f:\n",
    "            lines.append(line)\n",
    "\n",
    "    # each line represents a token, where the first tab is the type and the fourth is the token\n",
    "    lines_split = []\n",
    "    last_type = None\n",
    "    for line in lines:\n",
    "        if line == \"\\n\":\n",
    "            last_type = None\n",
    "            continue\n",
    "        splits = line.split(\"\\t\")\n",
    "        entity_type, token = splits[0], splits[3].split(\"\\n\")[0]\n",
    "        if entity_type != \"O\":\n",
    "            if (last_type != entity_type):\n",
    "                lines_split.append([entity_type, token.split(\"\\n\")[0]])\n",
    "            else:\n",
    "                lines_split[-1][1] += \" \" + token\n",
    "\n",
    "        last_type = entity_type\n",
    "    return lines_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_annotations_brat(ann_lines_split):\n",
    "    #process the annotations into a dictionary of lists\n",
    "    #we want to get a dictionary count of tokens (entites split by space), a dictionary count of entities, and a dictionary count of entity types\n",
    "    count_entities = {}\n",
    "    count_tokens = {}\n",
    "    count_entity_types = {}\n",
    "\n",
    "    for line in ann_lines_split:\n",
    "        entity = line[1]\n",
    "        entity_type = line[0]\n",
    "        tokens = entity.split(\" \")\n",
    "        if entity in count_entities:\n",
    "            count_entities[entity] += 1\n",
    "        else:\n",
    "            count_entities[entity] = 1\n",
    "        for token in tokens:\n",
    "            if token in count_tokens:\n",
    "                count_tokens[token] += 1\n",
    "            else:\n",
    "                count_tokens[token] = 1\n",
    "        if entity_type in count_entity_types:\n",
    "            count_entity_types[entity_type] += 1\n",
    "        else:\n",
    "            count_entity_types[entity_type] = 1\n",
    "\n",
    "    # return total of tokens, total of entitities, total of unique entities, how many types we have and a dictionary of tokens per type count\n",
    "    return sum(count_tokens.values()), sum(count_entities.values()), len(count_entities), len(count_entity_types), count_entity_types, count_entities, count_tokens, count_entity_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read BRAT corpus EMEA trainset\n",
    "train_EMEA_folder_location = \"./QUAERO_FrenchMed_TP2021/QUAERO_FrenchMed_BRAT/corpus/train/EMEA/\"\n",
    "train_MEDLINE_folder_location = \"./QUAERO_FrenchMed_TP2021/QUAERO_FrenchMed_BRAT/corpus/train/MEDLINE/\"\n",
    "dev_EMEA_folder_location = \"./QUAERO_FrenchMed_TP2021/QUAERO_FrenchMed_BRAT/corpus/dev/EMEA/\"\n",
    "dev_MEDLINE_folder_location = \"./QUAERO_FrenchMed_TP2021/QUAERO_FrenchMed_BRAT/corpus/dev/MEDLINE/\"\n",
    "test_EMEA_folder_location = \"./QUAERO_FrenchMed_TP2021/QUAERO_FrenchMed_BRAT/corpus/test/EMEA/\"\n",
    "test_MEDLINE_folder_location = \"./QUAERO_FrenchMed_TP2021/QUAERO_FrenchMed_BRAT/corpus/test/MEDLINE/\"\n",
    "\n",
    "folder_base = \"./QUAERO_FrenchMed_TP2021/QUAERO_FrenchMed\"\n",
    "\n",
    "formats = [\"conll\", \"brat\"]\n",
    "splits = [\"train\", \"dev\", \"test\"]\n",
    "subsets = [\"EMEA\", \"MEDLINE\"]\n",
    "\n",
    "# create pandas dataframe to store results of number of tokens, number of entities, number of unique entities, number of entity types, and dictionary of entity types\n",
    "df = pd.DataFrame(columns=[\"format\", \"split\", \"subset\", \"tokens\", \"entities\", \"unique_entities\", \"entity_types\"])\n",
    "df_entity_types = pd.DataFrame(columns=[\"format\", \"split\", \"subset\", \"entity_type\", \"entity_types_count\"])\n",
    "# add results to df\n",
    "for format in formats:\n",
    "    for split in splits:\n",
    "        for subset in subsets:\n",
    "            if format == \"conll\":\n",
    "                folder = folder_base + \"_conll/corpus/\" + split + \"/\" + subset + \"/\"\n",
    "                ann_lines_split = read_conll(folder)\n",
    "            elif format == \"brat\":\n",
    "                folder = folder_base + \"_BRAT/corpus/\" + split + \"/\" + subset + \"/\"\n",
    "                ann_lines_split = read_brat(folder)\n",
    "            tokens, entities, unique_entities, entity_types, entity_types_count, _, _, _ = process_annotations_brat(ann_lines_split)\n",
    "            output = pd.DataFrame({\"format\": format, \"split\": split, \"subset\": subset, \"tokens\": tokens, \"entities\": entities, \"unique_entities\": unique_entities, \"entity_types\": entity_types}, index=[0])\n",
    "            df = pd.concat([df, output], ignore_index=True)\n",
    "            output_per_type = pd.DataFrame([{\"format\": format, \"split\": split, \"subset\": subset, \"entity_type\": entity_type, \"entity_types_count\": entity_type_count} for entity_type, entity_type_count in entity_types_count.items()])\n",
    "            df_entity_types = pd.concat([df_entity_types, output_per_type], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = \"./QUAERO_FrenchMed_TP2021/QUAERO_FrenchMed_conll/corpus/train/EMEA/\"\n",
    "ann_lines_split = read_conll(folder)\n",
    "tokens, entities, unique_entities, entity_types, entity_types_count, count_entities, count_tokens, count_entity_types = process_annotations_brat(ann_lines_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3198,\n",
       " 2841,\n",
       " 898,\n",
       " 20,\n",
       " {'CHEM-B': 676,\n",
       "  'PROC-B': 390,\n",
       "  'CHEM-I': 114,\n",
       "  'DISO-B': 618,\n",
       "  'DISO-I': 246,\n",
       "  'LIVB-B': 253,\n",
       "  'PROC-I': 61,\n",
       "  'ANAT-B': 124,\n",
       "  'ANAT-I': 28,\n",
       "  'DEVI-B': 48,\n",
       "  'OBJC-B': 70,\n",
       "  'LIVB-I': 26,\n",
       "  'PHEN-B': 19,\n",
       "  'PHYS-B': 107,\n",
       "  'PHYS-I': 29,\n",
       "  'PHEN-I': 2,\n",
       "  'GEOG-B': 21,\n",
       "  'GEOG-I': 7,\n",
       "  'OBJC-I': 1,\n",
       "  'DEVI-I': 1})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens, entities, unique_entities, entity_types, entity_types_count#, count_entities, count_tokens, count_entity_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'CHEM-B': 676,\n",
       " 'PROC-B': 390,\n",
       " 'CHEM-I': 114,\n",
       " 'DISO-B': 618,\n",
       " 'DISO-I': 246,\n",
       " 'LIVB-B': 253,\n",
       " 'PROC-I': 61,\n",
       " 'ANAT-B': 124,\n",
       " 'ANAT-I': 28,\n",
       " 'DEVI-B': 48,\n",
       " 'OBJC-B': 70,\n",
       " 'LIVB-I': 26,\n",
       " 'PHEN-B': 19,\n",
       " 'PHYS-B': 107,\n",
       " 'PHYS-I': 29,\n",
       " 'PHEN-I': 2,\n",
       " 'GEOG-B': 21,\n",
       " 'GEOG-I': 7,\n",
       " 'OBJC-I': 1,\n",
       " 'DEVI-I': 1}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_entity_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # write results to csv\n",
    "# df.to_csv(\"results.csv\")\n",
    "# df_entity_types.to_csv(\"results_entity_types.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uni_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
