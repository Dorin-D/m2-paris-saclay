{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bytepair tokenization\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\"this is my cat\", \"i see a dog\", \"i see a cat\", \"the rainbow is shining\", \"i catched the ball\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/media/dorin/DualBootPart/University_Others/Paris-Saclay/m2-paris-saclay/deeplearningNLP/lab_work.ipynb Cell 3\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/dorin/DualBootPart/University_Others/Paris-Saclay/m2-paris-saclay/deeplearningNLP/lab_work.ipynb#W1sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m prev_max_pair \u001b[39m=\u001b[39m max_pair\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/dorin/DualBootPart/University_Others/Paris-Saclay/m2-paris-saclay/deeplearningNLP/lab_work.ipynb#W1sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m iterations_allowed \u001b[39m=\u001b[39m \u001b[39m10\u001b[39m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/media/dorin/DualBootPart/University_Others/Paris-Saclay/m2-paris-saclay/deeplearningNLP/lab_work.ipynb#W1sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m \u001b[39mwhile\u001b[39;00m iterations_allowed \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/dorin/DualBootPart/University_Others/Paris-Saclay/m2-paris-saclay/deeplearningNLP/lab_work.ipynb#W1sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m     \u001b[39mwhile\u001b[39;00m max_count \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m prev_max_count:\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/dorin/DualBootPart/University_Others/Paris-Saclay/m2-paris-saclay/deeplearningNLP/lab_work.ipynb#W1sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m         \u001b[39m# create new char_pairs of max_pair and all char_set\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/dorin/DualBootPart/University_Others/Paris-Saclay/m2-paris-saclay/deeplearningNLP/lab_work.ipynb#W1sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m         char_pairs \u001b[39m=\u001b[39m []\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "corpus = [\"this is my cat\", \"i see a dog\", \"i see a cat\", \"the rainbow is shining\", \"i catched the ball\"]\n",
    "\n",
    "word_dict = {}\n",
    "char_set = set()\n",
    "bytepair_dict = {}\n",
    "for sentence in corpus:\n",
    "    words = sentence.split()\n",
    "    for word in words:\n",
    "        for char in word:\n",
    "            char_set.add(char)\n",
    "        if word in word_dict:\n",
    "            word_dict[word] += 1\n",
    "        else:\n",
    "            word_dict[word] = 1\n",
    "# create all pairs of characters\n",
    "char_list = list(char_set)\n",
    "\n",
    "#create characters which we will be able to use for bytefication\n",
    "bytefication_characters = [chr(order) for order in list(range(90, 64, -1)) + list(range(122, 96, -1))]\n",
    "for char in char_list:\n",
    "    bytefication_characters.remove(char)\n",
    "\n",
    "char_pairs = []\n",
    "for i in range(len(char_list)):\n",
    "    for j in range(len(char_list)):\n",
    "        char_pairs.append(\"\".join([char_list[i], char_list[j]]))\n",
    "\n",
    "for char_pair in char_pairs:\n",
    "    count = 0\n",
    "    for word in word_dict:\n",
    "        count += word_dict[word] * word.count(char_pair)\n",
    "    if count > 0:\n",
    "        bytepair_dict[char_pair] = count\n",
    "\n",
    "# find the most frequent pairs\n",
    "max_count = 0\n",
    "max_pair = []\n",
    "for char_pair in bytepair_dict:\n",
    "    if bytepair_dict[char_pair] > max_count:\n",
    "        max_pair = [char_pair]\n",
    "        max_count = bytepair_dict[char_pair]\n",
    "    elif bytepair_dict[char_pair] == max_count:\n",
    "        max_pair.append(char_pair)\n",
    "\n",
    "prev_max_count = max_count\n",
    "prev_max_pair = max_pair\n",
    "\n",
    "iterations_allowed = 10\n",
    "while iterations_allowed > 0:\n",
    "    while max_count >= prev_max_count:\n",
    "        # create new char_pairs of max_pair and all char_set\n",
    "        char_pairs = []\n",
    "        for char in char_set:\n",
    "            for pair in max_pair:\n",
    "                char_pairs.append(\"\".join([char, pair]))\n",
    "                char_pairs.append(\"\".join([pair, char]))\n",
    "\n",
    "        # find the most frequent pairs from new char_pairs\n",
    "        bytepair_dict = {}\n",
    "        for char_pair in char_pairs:\n",
    "            count = 0\n",
    "            for word in word_dict:\n",
    "                count += word_dict[word] * word.count(char_pair)\n",
    "            if count > 0:\n",
    "                bytepair_dict[char_pair] = count\n",
    "\n",
    "        max_count = 0\n",
    "        max_pair = []\n",
    "        for char_pair in bytepair_dict:\n",
    "            if bytepair_dict[char_pair] > max_count:\n",
    "                max_pair = [char_pair]\n",
    "                max_count = bytepair_dict[char_pair]\n",
    "            elif bytepair_dict[char_pair] == max_count:\n",
    "                max_pair.append(char_pair)\n",
    "\n",
    "        if max_count >= prev_max_count:\n",
    "            prev_max_count = max_count\n",
    "            prev_max_pair = max_pair\n",
    "        else:\n",
    "            encoding_characters = bytefication_characters[0:len(prev_max_pair)]\n",
    "            bytefication_characters = bytefication_characters[len(prev_max_pair):]\n",
    "\n",
    "            encoded_corpus = []\n",
    "            for word in corpus:\n",
    "                for char_pair in prev_max_pair:\n",
    "                    word = word.replace(char_pair, encoding_characters[prev_max_pair.index(char_pair)])\n",
    "                encoded_corpus.append(word)\n",
    "            \n",
    "            corpus = encoded_corpus\n",
    "            char_set.update(encoding_characters)\n",
    "\n",
    "            iterations_allowed -= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this is my Z',\n",
       " 'i see a dog',\n",
       " 'i see a Z',\n",
       " 'the rainbow is shining',\n",
       " 'i Zched the ball']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'is'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prev_max_pair[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Z', 'Y']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding_characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uni_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
