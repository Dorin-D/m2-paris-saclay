{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b29fe24a-4c18-47c9-88c3-1152dd366599",
   "metadata": {},
   "source": [
    "# Out of domain and cross-lingual part-of-speech tagging\n",
    "\n",
    "The goal of this lab exercise is to build a part-of-speech tagger that should be tested in the two following settings:\n",
    "- out-of-domain generalization: test the tagger on a different domain than the one used for training (but in the same language).\n",
    "- cross-lingual generalization: test the tagger on a different language that has been used for training.\n",
    "\n",
    "To this end, you will rely on the aligned fast-text embeddings: https://fasttext.cc/docs/en/aligned-vectors.html\n",
    "However, the original files are quite big, so I uploaded on the website a filtered version of them that contains only words data appears in the data we use.\n",
    "Two important points:\n",
    "- you must not fine tune these embeddings, they are fixed\n",
    "- you need to have a special embedding for unknown words (the ones that don't have embeddings in fasttext) that you initialize and fix to a vector full of 0 (e.g. add an unk word to your dictionnary)\n",
    "To take care of this, the best way is to build the dictionnary when you read the fasttext embedding file, and then when you read the data, replace the words that don't have embeddings with the UNK word.\n",
    "\n",
    "**Warning:** do not use any external library or tools to load the fast text embeddings. Do it yourself, it is just a few lines of Python.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read embeddings\n",
    "en_embeddings_file = \"./data_pos/fasttext_en\"\n",
    "fr_embeddings_file = \"./data_pos/fasttext_fr\"\n",
    "en_embeddings = {}\n",
    "fr_embeddings = {}\n",
    "with open(en_embeddings_file, 'r') as f:\n",
    "    for line in f:\n",
    "        line = line.strip().split()\n",
    "        en_embeddings[line[0]] = [float(x) for x in line[1:]]\n",
    "with open(fr_embeddings_file, 'r') as f:\n",
    "    for line in f:\n",
    "        line = line.strip().split()\n",
    "        fr_embeddings[line[0]] = [float(x) for x in line[1:]]\n",
    "\n",
    "en_embeddings['<unk>'] = [0.0] * 300\n",
    "fr_embeddings['<unk>'] = [0.0] * 300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Reading data\n",
    "\n",
    "The data is in the conllu format: https://universaldependencies.org/format.html\n",
    "\n",
    "Basically:\n",
    "- comments are lines starting with a #\n",
    "- a blank line separate sentences\n",
    "- the ID column can contain 3 types of values:\n",
    "  - a single number\n",
    "  - a \"empty\" token, these IDs contains a \".\", for example 4.1 --- ignore these lines\n",
    "  - multiwords, these IDs contains a \"-\", for example 4-5 --- ignore these lines\n",
    "- you must convert all word to lowercase (we only have embeddings for lowercased words)\n",
    "\n",
    "You must start by writting a function that reads a conllu files and returns the list of sentences and the list of part-of-speech tags (i.e. keep only columns \"form\" and \"UPOS\").\n",
    "\n",
    "**Warning:** do not use any library to read these files, it is just a few lines of Python.\n",
    "\n",
    "Dataset files:\n",
    "- in domain english data: en_ewt-ud-*.conllu\n",
    "- out of domain english data: en_pud-ud-test.conllu\n",
    "- French test data: fr_gsd-ud-test.conllu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_conllu(file_path):\n",
    "    lines = open(file_path, 'r').readlines()\n",
    "    sentences = []\n",
    "    sentence_words = []\n",
    "    sentence_pos = []\n",
    "    for line in lines:\n",
    "        if line.startswith('#'):\n",
    "            continue\n",
    "        if line == '\\n':\n",
    "            sentences.append((sentence_words, sentence_pos))\n",
    "            sentence_words = []\n",
    "            sentence_pos = []\n",
    "            continue\n",
    "        else:\n",
    "            value_list = line.split('\\t')\n",
    "            if '-' in value_list[0] or '.' in value_list[0]:\n",
    "                continue\n",
    "            # if value_list[3] == 'PUNCT':\n",
    "            #     continue\n",
    "            sentence_words.append(value_list[1].lower())\n",
    "            sentence_pos.append(value_list[3])\n",
    "    return sentences\n",
    "\n",
    "# files to read\n",
    "en_train_file = \"./data_pos/en_ewt-ud-train.conllu\"\n",
    "en_dev_file = \"./data_pos/en_ewt-ud-dev.conllu\"\n",
    "en_test_file = \"./data_pos/en_ewt-ud-test.conllu\"\n",
    "en_pud_test_file = \"./data_pos/en_pud-ud-test.conllu\"\n",
    "fr_gsd_test_file = \"./data_pos/fr_gsd-ud-test.conllu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c642177-6b3e-4111-a1c6-2d35a0f49f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_train = read_conllu(en_train_file)\n",
    "en_dev = read_conllu(en_dev_file)\n",
    "en_test = read_conllu(en_test_file)\n",
    "en_pud_test = read_conllu(en_pud_test_file)\n",
    "fr_gsd_test = read_conllu(fr_gsd_test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('NOUN', 34797),\n",
       " ('PUNCT', 23620),\n",
       " ('VERB', 22978),\n",
       " ('PRON', 18586),\n",
       " ('ADP', 17729),\n",
       " ('DET', 16309),\n",
       " ('ADJ', 13126),\n",
       " ('AUX', 12435),\n",
       " ('PROPN', 12316),\n",
       " ('ADV', 9709),\n",
       " ('CCONJ', 6703),\n",
       " ('PART', 5567),\n",
       " ('SCONJ', 4502),\n",
       " ('NUM', 4112),\n",
       " ('X', 709),\n",
       " ('SYM', 698),\n",
       " ('INTJ', 688)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find all types of tags\n",
    "tags = dict()\n",
    "for sentence in en_train:\n",
    "    for tag in sentence[1]:\n",
    "        if tag in tags:\n",
    "            tags[tag] += 1\n",
    "        else:\n",
    "            tags[tag] = 1\n",
    "\n",
    "# sort tags by frequency\n",
    "sorted_tags = sorted(tags.items(), key=lambda x: x[1], reverse=True)\n",
    "sorted_tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural network\n",
    "\n",
    "You must build a very simple neural network:\n",
    "- word embeddings from fast-text\n",
    "- a bilstm to construct context sensitive representations of words\n",
    "- to predict the POS of each word, use a very simple and shallow MLP (even a simple linear projection is sufficient) at each position\n",
    "\n",
    "Importantly, word embeddings will be different when you test in the cross-lingual settings. Therefore, I strongly recommend you to have two separate modules:\n",
    "- one that retrieves word embeddings, that you instantiate two times (one time with English embeddings, one time with French embeddings) --- this also means that you need two dictionnaries that maps words to integers\n",
    "- one that does the rest of the computation\n",
    "\n",
    "In order to correctly batch you data during training, you will need to use pack_padded_sequence and pad_packed_sequence (check the lecture slides). Explain in the report why you needs them in this case (and why you didn't need them in the language model lab exercise) and what they do.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a bilstm model to predict pos tags\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "# build a bilstm model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Evaluation:\n",
    "\n",
    "Returns the tagging accuracy, i.e. the number of correctly predicted tags.\n",
    "\n",
    "If you have time, you can also explore more fine-grained metrics, especially in the cross-lingual case: accuracy per tag type, recall/precision/F1 per tag type, ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
