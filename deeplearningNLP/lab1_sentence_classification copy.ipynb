{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import torch as th\n",
    "import torch.autograd as ag\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning for NLP - lab exercise 1\n",
    "\n",
    "In this first lab exercise we will implement a simple bag-of-word\n",
    "classifier, i.e. a classifier that ignores the sequential structure of\n",
    "the sentence, and a classifier based on a convolutional neural network\n",
    "(CNN). The goal is to predict if a sentence is a positive or negative\n",
    "review of a movie. We will use a dataset constructed from IMDB.\n",
    "\n",
    "1.  Load and clean the data\n",
    "2.  Preprocess the data for the NN\n",
    "3.  Module definition\n",
    "4.  Train the network!\n",
    "\n",
    "We will implement this model with Pytorch, the most popular deep\n",
    "learning framework for Natural Language Processing. You can use the\n",
    "following links for help:\n",
    "\n",
    "-   turorials: <http://pytorch.org/tutorials/>\n",
    "-   documentation: <http://pytorch.org/docs/master/>\n",
    "\n",
    "## Data\n",
    "\n",
    "The data can be download here: <http://caio-corro.fr/dl4nlp/imdb.zip>\n",
    "\n",
    "There are two files: one with positive reviews (imdb.pos) and one with\n",
    "negative reviews (imdb.neg). Each file contains 300000 reviews, one per\n",
    "line.\n",
    "\n",
    "The following functions can be used to load and clean the data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize a sentence\n",
    "def clean_str(string, tolower=True):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning.\n",
    "    Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = re.sub(r\",\", \" , \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \" \\( \", string)\n",
    "    string = re.sub(r\"\\)\", \" \\) \", string)\n",
    "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    if tolower:\n",
    "        string = string.lower()\n",
    "    return string.strip()\n",
    "\n",
    "\n",
    "# reads the content of the file passed as an argument.\n",
    "# if limit > 0, this function will return only the first \"limit\" sentences in the file.\n",
    "def loadTexts(filename, limit=-1):\n",
    "    dataset=[]\n",
    "    with open(filename) as f:\n",
    "        line = f.readline()\n",
    "        cpt=1\n",
    "        skip=0\n",
    "        while line :\n",
    "            cleanline = clean_str(f.readline()).split()\n",
    "            if cleanline: \n",
    "                dataset.append(cleanline)\n",
    "            else: \n",
    "                line = f.readline()\n",
    "                skip+=1\n",
    "                continue\n",
    "            if limit > 0 and cpt >= limit: \n",
    "                break\n",
    "            line = f.readline()\n",
    "            cpt+=1        \n",
    "\n",
    "        print(\"Load \", cpt, \" lines from \", filename , \" / \", skip ,\" lines discarded\")\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell load the first 5000 sentences in each review set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load  5000  lines from  ./imdb/imdb.pos  /  1  lines discarded\n",
      "Load  5000  lines from  ./imdb/imdb.neg  /  1  lines discarded\n"
     ]
    }
   ],
   "source": [
    "LIM = 5000\n",
    "txtfile = \"./imdb/imdb.pos\"\n",
    "postxt = loadTexts(txtfile,limit=LIM)\n",
    "\n",
    "txtfile = \"./imdb/imdb.neg\"\n",
    "negtxt = loadTexts(txtfile,limit=LIM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data between train / dev / test, for example by creating lists\n",
    "txt_train, label_train, txt_dev, ... You should take care to keep a\n",
    "50/50 ratio between positive and negative instances in each set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train / dev / test\n",
    "train_pos_indices = np.random.choice(len(postxt), size=int(0.6*LIM), replace=False)\n",
    "# create dev excluding train\n",
    "dev_pos_indices = np.random.choice(list(set(range(len(postxt))) - set(train_pos_indices)), size=int(0.2*LIM), replace=False)\n",
    "# create test excluding train and dev\n",
    "test_pos_indices = list(set(range(len(postxt))) - set(train_pos_indices) - set(dev_pos_indices))\n",
    "\n",
    "train_neg_indices = np.random.choice(len(negtxt), size=int(0.6*LIM), replace=False)\n",
    "# create dev excluding train\n",
    "dev_neg_indices = np.random.choice(list(set(range(len(negtxt))) - set(train_neg_indices)), size=int(0.2*LIM), replace=False)\n",
    "# create test excluding train and dev\n",
    "test_neg_indices = list(set(range(len(negtxt))) - set(train_neg_indices) - set(dev_neg_indices))\n",
    "\n",
    "train_pos = [postxt[i] for i in train_pos_indices]\n",
    "dev_pos = [postxt[i] for i in dev_pos_indices]\n",
    "test_pos = [postxt[i] for i in test_pos_indices]\n",
    "\n",
    "train_neg = [negtxt[i] for i in train_neg_indices]\n",
    "dev_neg = [negtxt[i] for i in dev_neg_indices]\n",
    "test_neg = [negtxt[i] for i in test_neg_indices]\n",
    "\n",
    "# create train / dev / test sets\n",
    "train = [(x,1) for x in train_pos] + [(x,0) for x in train_neg]\n",
    "dev = [(x,1) for x in dev_pos] + [(x,0) for x in dev_neg]\n",
    "test = [(x,1) for x in test_pos] + [(x,0) for x in test_neg]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting data to Pytorch tensors\n",
    "\n",
    "We will first convert data to Pytorch tensors so they can be used in a\n",
    "neural network. To do that, you must first create a dictionnary that\n",
    "will map words to integers. Add to the dictionnary only words that are\n",
    "in the training set (be sure to understand why we do that!).\n",
    "\n",
    "Then, you can convert the data to tensors:\n",
    "\n",
    "-   use tensors of longs: both the sentence and the label will be\n",
    "    represented as integers, not floats!\n",
    "-   these tensors do not require a gradient\n",
    "\n",
    "A tensor representing a sentence is composed of the integer\n",
    "representation of each word, e.g. \\[10, 256, 3, 4\\]. Note that some\n",
    "words in the dev and test sets may not be in the dictionnary! (i.e.\n",
    "unknown words) You can just skip them, even if this is a bad idea in\n",
    "general."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a dictionary of all words in the training set\n",
    "word_dict = {}\n",
    "for sent, _ in train:\n",
    "    for word in sent:\n",
    "        if word not in word_dict:\n",
    "            word_dict[word] = len(word_dict)\n",
    "\n",
    "def sent2tensor(sent, word_dict):\n",
    "    # convert sentence to list of indices, if a word is not in the dictionary, skip it\n",
    "    idxs = [word_dict[word] if word in word_dict else -1 for word in sent]\n",
    "    # remove words not in dictionary\n",
    "    idxs = [idx for idx in idxs if idx >= 0]\n",
    "    if idxs == []:\n",
    "        return None\n",
    "    return th.LongTensor(idxs)\n",
    "\n",
    "train_data = [(sent2tensor(sent, word_dict), label) for sent, label in train]\n",
    "dev_data = [(sent2tensor(sent, word_dict), label) for sent, label in dev]\n",
    "test_data = [(sent2tensor(sent, word_dict), label) for sent, label in test]\n",
    "\n",
    "# remove empty sentences\n",
    "train_data = [x for x in train_data if x[0] is not None]\n",
    "dev_data = [x for x in dev_data if x[0] is not None]\n",
    "test_data = [x for x in test_data if x[0] is not None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural network definition\n",
    "\n",
    "You need to implement two networks:\n",
    "\n",
    "-   a simple bag of word model (note: it may be better to take the mean\n",
    "    of input embeddings that the sum)\n",
    "-   a simple CNN as described in the course\n",
    "\n",
    "To simplify code, you can assume the input will always be a single\n",
    "sentence first, and then implement batched inputs. In the case of\n",
    "batched inputs, give to the forward function a (python) list of tensors.\n",
    "\n",
    "The bag of word neural network should be defined as follows:\n",
    "\n",
    "-   take as input a tensor that is a sequence of integers indexing word\n",
    "    embeddings\n",
    "-   retrieve the word embeddings from an embedding table\n",
    "-   construct the \"input\" of the MLP by summing (or computing the mean)\n",
    "    over all embeddingsÂ (i.e. bag-of-word model)\n",
    "-   build a hidden represention using a MLP (1 layer? 2 layers?\n",
    "    experiment! but maybe first try wihout any hidden layer...)\n",
    "-   project the hidden representation to the output space: it is a\n",
    "    binary classification task, so the output space is a scalar where a\n",
    "    negative (resp. positive) value means the review is negative (resp.\n",
    "    positive).\n",
    "\n",
    "The CNN is a little bit more tricky to implement. The goal is that you\n",
    "implement the one presented in the first lecture. Importantly, you\n",
    "should add \"padding\" tokens before and after the sentence so you can\n",
    "have a convolution even when there is a single word in the input. For\n",
    "example, if you input sentence is \\[\"word\"\\], you want to instead\n",
    "consider the sentence \\[\"\\<BOS>\", \"word\", \"\\<EOS>\"\\] if your window is\n",
    "of size 2 or 3. You can do this either directly when you load the data,\n",
    "or you can do that in the neural network module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BAG of word classifier\n",
    "class CBOW_classifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, linear_dim):\n",
    "        super(CBOW_classifier, self).__init__()\n",
    "        # create embedding table\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        # create linear layer\n",
    "        if type(linear_dim) == int:\n",
    "            self.linear == nn.Sequential(nn.Linear(embedding_dim, linear_dim), nn.ReLU(), nn.Linear(linear_dim, 1))\n",
    "        elif type(linear_dim) in [list, tuple]:\n",
    "            layers = [nn.Linear(embedding_dim, linear_dim[0]), nn.ReLU()]\n",
    "            for i in range(len(linear_dim)-1):\n",
    "                layers.append(nn.Linear(linear_dim[i], linear_dim[i+1]))\n",
    "                layers.append(nn.ReLU())\n",
    "            layers.append(nn.Linear(linear_dim[-1], 1))\n",
    "            self.linear = nn.Sequential(*layers)\n",
    "        else:\n",
    "            raise ValueError(\"linear_dim must be an int, list or tuple\")\n",
    "        \n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        # get embeddings\n",
    "        embeds = self.embedding(inputs)\n",
    "        # sum embeddings and average\n",
    "        embeds = th.sum(embeds, dim=0) / embeds.shape[0]\n",
    "        # linear layer\n",
    "        out = self.linear(embeds)\n",
    "        # sigmoid\n",
    "        out = F.sigmoid(out)\n",
    "        return out\n",
    "    \n",
    "    def get_embeddings(self, inputs):\n",
    "        # get embeddings\n",
    "        embeds = self.embedding(inputs)\n",
    "        # sum embeddings and average\n",
    "        embeds = th.sum(embeds, dim=0) / embeds.shape[0]\n",
    "        return embeds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss function\n",
    "\n",
    "Create a loss function builder.\n",
    "\n",
    "-   Pytorch loss functions are documented here:\n",
    "    <https://pytorch.org/docs/stable/nn.html#loss-functions>\n",
    "-   In our case, we are interested in *BCELoss* and *BCEWithLogitsLoss*.\n",
    "    Read their documentation and choose the one that fits with your\n",
    "    network output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define loss function\n",
    "loss_fn = nn.BCELoss()\n",
    "\n",
    "embedding_size = 100\n",
    "linear_size = (50, 20)\n",
    "# define model\n",
    "model = CBOW_classifier(len(word_dict), embedding_size, linear_size)\n",
    "\n",
    "# define optimizer\n",
    "optim = th.optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop\n",
    "\n",
    "Write your training loop!\n",
    "\n",
    "-   parameterizable number of epochs\n",
    "-   at each epoch, print the mean loss and the dev accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Train Acc: 0.6845, Dev Acc: 0.7469574036511156\n",
      "Epoch: 1, Train Acc: 0.8095, Dev Acc: 0.7667342799188641\n",
      "Epoch: 2, Train Acc: 0.868, Dev Acc: 0.7768762677484787\n",
      "Epoch: 3, Train Acc: 0.9093333333333333, Dev Acc: 0.7824543610547667\n",
      "Epoch: 4, Train Acc: 0.9363333333333334, Dev Acc: 0.7672413793103449\n",
      "Epoch: 5, Train Acc: 0.96, Dev Acc: 0.7885395537525355\n",
      "Epoch: 6, Train Acc: 0.971, Dev Acc: 0.7799188640973631\n",
      "Epoch: 7, Train Acc: 0.9808333333333333, Dev Acc: 0.7799188640973631\n",
      "Epoch: 8, Train Acc: 0.9898333333333333, Dev Acc: 0.7844827586206896\n",
      "Epoch: 9, Train Acc: 0.9916666666666667, Dev Acc: 0.7789046653144016\n"
     ]
    }
   ],
   "source": [
    "# training loop\n",
    "for epoch in range(10):\n",
    "    # shuffle training data\n",
    "    np.random.shuffle(train_data)\n",
    "    # set model to train mode\n",
    "    model.train()\n",
    "\n",
    "    #compute train accuracy\n",
    "    correct = 0\n",
    "\n",
    "    # loop over training data\n",
    "    for sent, label in train_data:\n",
    "        # zero gradients\n",
    "        optim.zero_grad()\n",
    "        # forward pass\n",
    "        out = model(sent)\n",
    "        # compute loss\n",
    "        loss = loss_fn(out, th.FloatTensor([label]))\n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "        # update parameters\n",
    "        optim.step()\n",
    "\n",
    "        #compute train accuracy\n",
    "        pred = 1 if out > 0.5 else 0\n",
    "        if pred == label:\n",
    "            correct += 1\n",
    "    # compute accuracy\n",
    "    train_acc = correct / len(train_data)\n",
    "    \n",
    "\n",
    "\n",
    "    # set model to eval mode\n",
    "    model.eval()\n",
    "    # compute accuracy on dev set\n",
    "    correct = 0\n",
    "    for sent, label in dev_data:\n",
    "        # forward pass\n",
    "        out = model(sent)\n",
    "        # get prediction\n",
    "        pred = 1 if out > 0.5 else 0\n",
    "        # check if prediction is correct\n",
    "        if pred == label:\n",
    "            correct += 1\n",
    "    # compute accuracy\n",
    "    acc = correct / len(dev_data)\n",
    "    print(\"Epoch: {}, Train Acc: {}, Dev Acc: {}\".format(epoch, train_acc, acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute distance between two sentences in the embedding space of model\n",
    "def compute_distance(sent1, sent2, model, word_dict):\n",
    "    # get embeddings\n",
    "    embeds1 = model.get_embeddings(sent2tensor(sent1, word_dict))\n",
    "    embeds2 = model.get_embeddings(sent2tensor(sent2, word_dict))\n",
    "    # compute distance\n",
    "    return th.dist(embeds1, embeds2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(16.6578, grad_fn=<DistBackward0>)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_distance([\"good\"], [\"bad\"], model, word_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(12.7767, grad_fn=<DistBackward0>)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_distance([\"bad\"], [\"terrible\"], model, word_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(13.7037, grad_fn=<DistBackward0>)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_distance([\"good\"], [\"amazing\"], model, word_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(8.5425, grad_fn=<DistBackward0>)"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_distance([\"this\", \"movie\", \"has\", 'been', \"amazing\"], [\"movie\"], model, word_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.9489, grad_fn=<DistBackward0>)\n"
     ]
    }
   ],
   "source": [
    "sentence_1 = \"this movie makes me hate\".split()\n",
    "sentence_2 = \"this movie is literally the best in the world\".split()\n",
    "print(compute_distance(sentence_1, sentence_2, model, word_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that the embeddings we have learnt are not clustered appropriately in the feature space; synonyms are at similar distances as antonyms. But that is ok, our goal has been to train a linear classifier using word embeddings - which we have reasonable achieved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next up: CNNs for the same task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a convolutional bag of words classifier\n",
    "class ConvCBOX(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, kernel_size, out_channels, linear_dim):\n",
    "        super(ConvCBOX, self).__init__()\n",
    "        # create embedding table\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        # create convolutional layer\n",
    "        self.conv = nn.Conv1d(embedding_dim, out_channels, kernel_size, padding = 'same')\n",
    "        # create linear layer\n",
    "        if type(linear_dim) == int:\n",
    "            self.linear == nn.Sequential(nn.Linear(out_channels, linear_dim), nn.ReLU(), nn.Linear(linear_dim, 1))\n",
    "        elif type(linear_dim) in [list, tuple]:\n",
    "            layers = [nn.Linear(out_channels, linear_dim[0]), nn.ReLU()]\n",
    "            for i in range(len(linear_dim)-1):\n",
    "                layers.append(nn.Linear(linear_dim[i], linear_dim[i+1]))\n",
    "                layers.append(nn.ReLU())\n",
    "            layers.append(nn.Linear(linear_dim[-1], 1))\n",
    "            self.linear = nn.Sequential(*layers)\n",
    "        else:\n",
    "            raise ValueError(\"linear_dim must be an int, list or tuple\")\n",
    "        \n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        # get embeddings\n",
    "        embeds = self.embedding(inputs)\n",
    "        # transpose embeddings\n",
    "        embeds = embeds.transpose(1,2)\n",
    "        # convolve\n",
    "        conv_out = self.conv(embeds)\n",
    "        # max pool\n",
    "        pool_out = F.max_pool1d(conv_out, conv_out.shape[2])\n",
    "        # linear layer\n",
    "        out = self.linear(pool_out.squeeze())\n",
    "        # sigmoid\n",
    "        out = F.sigmoid(out)\n",
    "        return out\n",
    "    \n",
    "    def get_embeddings(self, inputs):\n",
    "        # get embeddings\n",
    "        embeds = self.embedding(inputs)\n",
    "        # transpose embeddings\n",
    "        embeds = embeds.transpose(1,2)\n",
    "        # convolve\n",
    "        conv_out = self.conv(embeds)\n",
    "        # max pool\n",
    "        pool_out = F.max_pool1d(conv_out, conv_out.shape[2])\n",
    "        return pool_out.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "embedding_size = 100\n",
    "kernel_size = 3\n",
    "out_channels = 100\n",
    "linear_size = (50, 20)\n",
    "model = ConvCBOX(len(word_dict), embedding_size, kernel_size, out_channels, linear_size)\n",
    "\n",
    "# define optimizer\n",
    "optim = th.optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Train Acc: 0.6773333333333333, Dev Acc: 0.7565922920892495\n",
      "Epoch: 1, Train Acc: 0.8165, Dev Acc: 0.7697768762677485\n",
      "Epoch: 2, Train Acc: 0.8985, Dev Acc: 0.7723123732251521\n",
      "Epoch: 3, Train Acc: 0.9455, Dev Acc: 0.7743407707910751\n",
      "Epoch: 4, Train Acc: 0.9673333333333334, Dev Acc: 0.7723123732251521\n",
      "Epoch: 5, Train Acc: 0.978, Dev Acc: 0.7824543610547667\n",
      "Epoch: 6, Train Acc: 0.9881666666666666, Dev Acc: 0.7794117647058824\n",
      "Epoch: 7, Train Acc: 0.9888333333333333, Dev Acc: 0.789553752535497\n",
      "Epoch: 8, Train Acc: 0.991, Dev Acc: 0.7692697768762677\n",
      "Epoch: 9, Train Acc: 0.9923333333333333, Dev Acc: 0.787525354969574\n"
     ]
    }
   ],
   "source": [
    "# training loop\n",
    "for epoch in range(10):\n",
    "    # shuffle training data\n",
    "    np.random.shuffle(train_data)\n",
    "    # set model to train mode\n",
    "    model.train()\n",
    "\n",
    "    #compute train accuracy\n",
    "    correct = 0\n",
    "\n",
    "    # loop over training data\n",
    "    for sent, label in train_data:\n",
    "        # zero gradients\n",
    "        optim.zero_grad()\n",
    "        # forward pass\n",
    "        out = model(sent.unsqueeze(0))\n",
    "        # compute loss\n",
    "        loss = loss_fn(out, th.FloatTensor([label]))\n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "        # update parameters\n",
    "        optim.step()\n",
    "\n",
    "        #compute train accuracy\n",
    "        pred = 1 if out > 0.5 else 0\n",
    "        if pred == label:\n",
    "            correct += 1\n",
    "    # compute accuracy\n",
    "    train_acc = correct / len(train_data)\n",
    "    \n",
    "\n",
    "\n",
    "    # set model to eval mode\n",
    "    model.eval()\n",
    "    # compute accuracy on dev set\n",
    "    correct = 0\n",
    "    for sent, label in dev_data:\n",
    "        # forward pass\n",
    "        out = model(sent.unsqueeze(0))\n",
    "        # get prediction\n",
    "        pred = 1 if out > 0.5 else 0\n",
    "        # check if prediction is correct\n",
    "        if pred == label:\n",
    "            correct += 1\n",
    "    # compute accuracy\n",
    "    acc = correct / len(dev_data)\n",
    "    print(\"Epoch: {}, Train Acc: {}, Dev Acc: {}\".format(epoch, train_acc, acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv = nn.Conv1d(100, 3, 3, padding = 'same')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 10])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv(th.randn(1,100,10)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
