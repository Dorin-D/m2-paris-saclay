{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Student: Dorin Doncenco\n",
    "\n",
    "Parts of the code have been written with the help of Copilot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab exercise: neural language modeling\n",
    "\n",
    "The goal of this lab exercise is build two neural language models:\n",
    "- a neural n-gram model based on a simple MLP\n",
    "- an autoregressive model based on a LSTM\n",
    "\n",
    "Although the n-gram model is straighforward to code, there are a few \"tricks\" that you need to implement for the autoregressive model:\n",
    "- word dropout\n",
    "- variational dropout\n",
    "- loss function masking\n",
    "\n",
    "## Variational dropout\n",
    "\n",
    "The idea of variational dropout is to apply the same mask at each position for a given sentence (if there are several sentences in a minibatch, use different masks for each input).\n",
    "The idea is as follows:\n",
    "- assume a sentence of n words whose embeddings are e_1, e_2, ... e_n\n",
    "- at the input of the LSTM, instead of apply dropout independently to each embedding, sample a single mask that will be applied similarly at each position\n",
    "- same at the output of the LSTM\n",
    "\n",
    "See Figure 1 of this paper: https://proceedings.neurips.cc/paper/2016/file/076a0c97d09cf1a0ec3e19c7f2529f2b-Paper.pdf\n",
    "\n",
    "To implement this, you need to build a custom module that applies the dropout only if the network is in training mode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing\n",
    "\n",
    "You first need to download the Penn Treebank as pre-processed by Tomas Mikolov. It is available here: https://github.com/townie/PTB-dataset-from-Tomas-Mikolov-s-webpage/tree/master/data\n",
    "We will use the following files:\n",
    "- ptb.train.txt\n",
    "- ptb.valid.txt\n",
    "- ptb.test.txt\n",
    "\n",
    "Check manually the data.\n",
    "\n",
    "Todo:\n",
    "- build a word dictionnary, i.e. a bijective mapping between words and integers. You will need to add a special token \"\\<BOS\\>\" to the dictionnary even if it doesn't appear in sentences. (if you want to generate data, you will also need a \"\\<EOS\\>\" token, but this is not a requirement for this lab exercise --- you can do this at the end if you want)\n",
    "- build python list of integers representing each input. For example, for the sentence \"I sleep\", the tensor could look like [10, 5] if 10 is the integer associated with \"I\" and 5 the integer associated with \"sleep\". You can add this directly to the dictionnaries in \\*\\_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(path):\n",
    "    data = list()\n",
    "    with open(path) as inf:\n",
    "        for line in inf:\n",
    "            line = line.strip()\n",
    "            if len(line) == 0:\n",
    "                continue\n",
    "            data.append({\"text\": line.split()})\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = read_file(\"./dataset/ptb.train.txt\")\n",
    "dev_data = read_file(\"./dataset/ptb.valid.txt\")\n",
    "test_data = read_file(\"./dataset/ptb.test.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 50770), ('<unk>', 45020), ('N', 32481), ('of', 24400), ('to', 23638), ('a', 21196), ('in', 18000), ('and', 17474), (\"'s\", 9784), ('that', 8931)]\n"
     ]
    }
   ],
   "source": [
    "# count the amount of each words in the train data\n",
    "word_count = dict()\n",
    "for data in train_data:\n",
    "    for word in data[\"text\"]:\n",
    "        if word not in word_count:\n",
    "            word_count[word] = 0\n",
    "        word_count[word] += 1\n",
    "\n",
    "# print the most commmon 10 words\n",
    "word_count = sorted(word_count.items(), key=lambda x: x[1], reverse=True)\n",
    "print(word_count[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3370"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dev_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42068 3370 3761\n",
      "aer banknote berlitz calloway centrust cluett fromstein gitano guterman hydro-quebec ipo kia memotec mlx nahb punts rake regatta rubens sim snack-food ssangyong swapo wachter\n",
      "\n",
      "pierre <unk> N years old will join the board as a nonexecutive director nov. N\n",
      "\n",
      "mr. <unk> is chairman of <unk> n.v. the dutch publishing group\n",
      "\n",
      "rudolph <unk> N years old and former chairman of consolidated gold fields plc was named a nonexecutive director of this british industrial conglomerate\n",
      "\n",
      "a form of asbestos once used to make kent cigarette filters has caused a high percentage of cancer deaths among a group of workers exposed to it more than N years ago researchers reported\n"
     ]
    }
   ],
   "source": [
    "print(len(train_data), len(dev_data), len(test_data))\n",
    "print(\"\\n\\n\".join(\" \".join(s[\"text\"]) for s in train_data[:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordDict:\n",
    "    # constructor, words must be a set containing all words\n",
    "    def __init__(self, words):\n",
    "        assert type(words) == set\n",
    "        self.words = words\n",
    "        self.word_to_id = dict()\n",
    "        self.id_to_word = dict()\n",
    "        for idx, word in enumerate(words):\n",
    "            self.word_to_id[word] = idx\n",
    "            self.id_to_word[idx] = word\n",
    "\n",
    "    # return the integer associated with a word\n",
    "    def word_to_id(self, word):\n",
    "        assert type(word) == str\n",
    "        return self.word_to_id[word]\n",
    "    \n",
    "    # return the word associated with an integer\n",
    "    def id_to_word(self, idx):\n",
    "        assert type(idx) == int\n",
    "        return self.id_to_word[idx]\n",
    "    \n",
    "    # number of word in the dictionnary\n",
    "    def __len__(self):\n",
    "        return len(self.words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10001"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_words = set()\n",
    "for sentence in train_data:\n",
    "    train_words.update(sentence[\"text\"])\n",
    "train_words.update([\"<bos>\", \"<eos>\"])\n",
    "word_dict = WordDict(train_words)\n",
    "len(word_dict)  # should be 10001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "For evaluation, you must compute the perplexity of the test dataset (i.e. assume the dataset is one very long sentence), see:\n",
    "https://lena-voita.github.io/nlp_course/language_modeling.html#evaluation\n",
    "\n",
    "Note that you don't need to explicitly compute the root, you can use log probabilities and properties of log functions for this.\n",
    "As during evaluation, you will see sentences one after the other, you can code a small class to keep track of log probabilities of words and compute the global perplexity at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perplexity:\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "               \n",
    "    def reset(self):\n",
    "        self.log_probs = list()\n",
    "        self.num_words = 0\n",
    "        \n",
    "    def add_sentence(self, log_probs):\n",
    "        # log_probs: vector of log probabilities of words in a sentence\n",
    "        for log_prob in log_probs:\n",
    "            self.log_probs.append(log_prob)\n",
    "            self.num_words += 1\n",
    "        \n",
    "    def compute_perplexity(self):\n",
    "        # compute perplexity from the stored log probabilities\n",
    "        log_probs_sum = sum(self.log_probs) / self.num_words\n",
    "        perplexity = 2 ** (-log_probs_sum)\n",
    "        return perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM model\n",
    "\n",
    "This model should rely on a LSTM.\n",
    "\n",
    "1. transform the data into tensors => you can't use the same trick as for the n-gram model\n",
    "2. train the network by batching the input --- be very careful when computing the loss function! And explain how to batch data, compute the loss with batch data, etc, in the report!\n",
    "3. compute the perplexity on the test data\n",
    "4. implement variational dropout at input and output of the LSTM\n",
    "\n",
    "Warning: you need to use the option batch_first=True for the LSTM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will convert the data to a tensor format which contain token indices; to make them all of the same length, we pad the shorter sentences with $<$ eos $>$ tokens to match the length of the longest sentence. In order to be able to recover the length of the original sentence, we will keep tensors sentence lengths, which will be used during training to create a mask and remove the padding tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_data(data, word_dict):\n",
    "    converted_data = list()\n",
    "    for sentence in data:\n",
    "        converted_data.append([word_dict.word_to_id[\"<bos>\"]] + [word_dict.word_to_id[word] for word in sentence[\"text\"]] + [word_dict.word_to_id[\"<eos>\"]])\n",
    "    return converted_data\n",
    "\n",
    "train_data_tensor = convert_data(train_data, word_dict)\n",
    "dev_data_tensor = convert_data(dev_data, word_dict)\n",
    "test_data_tensor = convert_data(test_data, word_dict)\n",
    "\n",
    "# max length of a train sentence\n",
    "max_len = max([len(s) for s in train_data_tensor])\n",
    "\n",
    "# pad sentences\n",
    "def pad_sentences(data, max_len):\n",
    "    sentence_lengths = [len(sentence) for sentence in data]\n",
    "\n",
    "    for i, sentence in enumerate(data):\n",
    "        if len(sentence) < max_len:\n",
    "            sentence += [word_dict.word_to_id[\"<eos>\"]] * (max_len - len(sentence))\n",
    "        else:\n",
    "            #if the sentence is longer than the longest in train, we truncate it and add <eos>\n",
    "            sentence = sentence[:max_len-1] + [word_dict.word_to_id[\"<eos>\"]]\n",
    "            sentence_lengths[i] = max_len\n",
    "\n",
    "    sentence_lengths = torch.tensor(sentence_lengths)\n",
    "    return data, sentence_lengths\n",
    "\n",
    "train_data_tensor, train_sentence_lenghts = pad_sentences(train_data_tensor, max_len)\n",
    "dev_data_tensor, dev_sentence_lengths = pad_sentences(dev_data_tensor, max_len)\n",
    "test_data_tensor, test_sentence_lengths = pad_sentences(test_data_tensor, max_len)\n",
    "\n",
    "# convert data into tensors\n",
    "def convert_to_tensors(data):\n",
    "    converted_data = list()\n",
    "    for sentence in data:\n",
    "        converted_data.append(torch.tensor(sentence))\n",
    "    converted_data = torch.stack(converted_data)\n",
    "    return converted_data\n",
    "\n",
    "train_data_tensor = convert_to_tensors(train_data_tensor)\n",
    "dev_data_tensor = convert_to_tensors(dev_data_tensor)\n",
    "test_data_tensor = convert_to_tensors(test_data_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The batch generator will create training batches from the data and sentence lengths, ensuring the respective order between each other is maintained, and it randomises the order of the data to add stochasticity to the training of the model. In order to save computation time, it also returns the maximum length of each batch, which can be used to reduce the size of the input and to save computation time during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_generator(data, sentence_lengths, batch_size):\n",
    "    # randomize sentence order\n",
    "    # data: (num_sentences, max_len)\n",
    "    # sentence_lengths: (num_sentences)\n",
    "    # batch_size: int\n",
    "\n",
    "    # get shuffled indices\n",
    "    indices = torch.randperm(data.shape[0])\n",
    "    \n",
    "    # create batches\n",
    "    num_batches = data.shape[0] // batch_size\n",
    "    batches = list()\n",
    "    batches_lengths = list()\n",
    "    for i in range(num_batches):\n",
    "        batch = data[indices[i*batch_size:(i+1)*batch_size], :]\n",
    "        batch_lengths = sentence_lengths[indices[i*batch_size:(i+1)*batch_size]]\n",
    "        batches.append(batch)\n",
    "        batches_lengths.append(batch_lengths)\n",
    "    if data.shape[0] % batch_size != 0:\n",
    "        batch = data[indices[num_batches*batch_size:], :]\n",
    "        batches.append(batch)\n",
    "        batch_lengths = sentence_lengths[indices[num_batches*batch_size:]]\n",
    "        batches_lengths.append(batch_lengths)\n",
    "\n",
    "    # batches length will find the maximum sentence length in each batch\n",
    "    batches_max_len = torch.tensor([torch.max(batch_lengths) for batch_lengths in batches_lengths])\n",
    "    return batches, batches_lengths, batches_max_len\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The language model architecture consists of an embedding layer, an LSTM layer, and a linear layer; the linear layer will output logits corresponding to each word in the dictionary, which can be converted into a probability using the softmax function. A word dropout function is available in the architecture, which will randomly replace tokens in the input with the $<$ unk $>$ token, to prevent overfitting to the training data.\n",
    "\n",
    "The generate function is able to generate sentences given an input sequence, up until a certain limit or until the end of sentence token is generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers, dropout, word_dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers, dropout=dropout, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_dim, vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.word_dropout = word_dropout\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.init_weights()\n",
    "    \n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.linear.bias.data.zero_()\n",
    "        self.linear.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, input, hidden, sentence_lengths):\n",
    "        # input: (batch_size, seq_len)\n",
    "        # hidden: (num_layers, batch_size, hidden_dim)\n",
    "        # sentence_lengths: (batch_size)\n",
    "        batch_size = input.shape[0]\n",
    "        seq_len = input.shape[1]\n",
    "        if self.word_dropout > 0.0 and self.training:\n",
    "            # randomly replace some input words with <unk>\n",
    "            # word_dropout: probability of replacing a word with <unk>\n",
    "            # input: (batch_size, seq_len)\n",
    "            mask = torch.rand(input.shape) < self.word_dropout\n",
    "            # clone the input tensor to avoid modifying it\n",
    "            input = input.clone()\n",
    "            input[mask] = word_dict.word_to_id[\"<unk>\"]\n",
    "            \n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        # embedded: (batch_size, seq_len, embedding_dim)\n",
    "        output, hidden = self.lstm(embedded, hidden)\n",
    "        # output: (batch_size, seq_len, hidden_dim)\n",
    "        output = self.dropout(output)\n",
    "        output = output.reshape(batch_size * seq_len, self.hidden_dim)\n",
    "        # output: (batch_size * seq_len, hidden_dim)\n",
    "        output = self.linear(output)\n",
    "        return output\n",
    "    \n",
    "    def init_hidden(self, batch_size, device):\n",
    "        # return a tensor of zeros of shape (num_layers, batch_size, hidden_dim)\n",
    "        return (torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(device),\n",
    "                torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(device))  \n",
    "    \n",
    "    def generate(self, sentence, hidden, sentence_length, max_len, device):\n",
    "        # given a sentence, generate the next words until :\n",
    "        #        <eos> is generated or max_len is reached\n",
    "        # sentence: (seq_len) of word ids\n",
    "        # hidden: (num_layers, 1, hidden_dim)\n",
    "        # sentence_length: int\n",
    "        # max_len: int\n",
    "       \n",
    "        sentence_generated = sentence.unsqueeze(0)\n",
    "        for i in range(max_len):\n",
    "            # we generate the next word\n",
    "            output = self.forward(sentence_generated, hidden, torch.tensor([1]).to(device))\n",
    "            # output: (1, vocab_size)\n",
    "            output = F.softmax(output, dim=1)\n",
    "            # output: (1, vocab_size)\n",
    "            word_id = torch.multinomial(output[-1:], 1)\n",
    "            # word_id: (1)\n",
    "            sentence_generated = torch.cat((sentence_generated, word_id), dim=1)\n",
    "            # sentence_generated: (seq_len)\n",
    "            if word_id == word_dict.word_to_id[\"<eos>\"]:\n",
    "                break\n",
    "        sentence_generated = sentence_generated.squeeze(0).cpu().numpy()\n",
    "        return sentence_generated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "vocab_size = len(word_dict)\n",
    "embedding_dim = 128\n",
    "hidden_dim = 128\n",
    "num_layers = 1\n",
    "dropout = 0.0\n",
    "batch_size = 32\n",
    "learning_rate = 0.005\n",
    "num_epochs = 50\n",
    "word_dropout = 0.25\n",
    "#log_interval = len(train_data_tensor) // batch_size // 3\n",
    "log_interval = len(train_data_tensor) + 1 # to never print loss during the epoch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# create model\n",
    "model = LSTMLanguageModel(vocab_size, embedding_dim, hidden_dim, num_layers, dropout, word_dropout=word_dropout).to(device)\n",
    "# create optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "# create loss function\n",
    "criterion = nn.CrossEntropyLoss(reduction=\"none\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: 7.6749, Perplexity: 377.3656\n",
      "Epoch: 1, Dev Loss: 7.0032, Dev Perplexity: 166.3631\n",
      "Generated sentence: \n",
      " stake find growth texas said\n",
      "\n",
      "Epoch: 2, Loss: 6.8056, Perplexity: 147.8680\n",
      "Epoch: 2, Dev Loss: 6.6853, Dev Perplexity: 141.1949\n",
      "Generated sentence: \n",
      " \n",
      "\n",
      "Epoch: 3, Loss: 6.6472, Perplexity: 128.8401\n",
      "Epoch: 3, Dev Loss: 6.5762, Dev Perplexity: 120.5470\n",
      "Generated sentence: \n",
      " N month writer premium have three eliminated N <unk> been a big missing played pressure laurence\n",
      "\n",
      "Epoch: 4, Loss: 6.5641, Perplexity: 96.9004\n",
      "Epoch: 4, Dev Loss: 6.5065, Dev Perplexity: 87.4678\n",
      "Generated sentence: \n",
      " when at N\n",
      "\n",
      "Epoch: 5, Loss: 6.5281, Perplexity: 93.3364\n",
      "Epoch: 5, Dev Loss: 6.4175, Dev Perplexity: 95.4229\n",
      "Generated sentence: \n",
      " <unk> from area debts 's secretary by however the that their this shevardnadze senior bank salomon longer an $ of four the that was bell experiencing\n",
      "\n",
      "Epoch: 6, Loss: 6.4328, Perplexity: 86.5513\n",
      "Epoch: 6, Dev Loss: 6.3379, Dev Perplexity: 76.4309\n",
      "Generated sentence: \n",
      " the office share on the salomon has initial reasonable was selling\n",
      "\n",
      "Epoch: 7, Loss: 6.3629, Perplexity: 78.4164\n",
      "Epoch: 7, Dev Loss: 6.2715, Dev Perplexity: 69.2269\n",
      "Generated sentence: \n",
      " the at elsewhere real oil numbered 's survey york has to tally university and press\n",
      "\n",
      "Epoch: 8, Loss: 6.2935, Perplexity: 67.7661\n",
      "Epoch: 8, Dev Loss: 6.2016, Dev Perplexity: 66.7042\n",
      "Generated sentence: \n",
      " the a ago to trading $ to N system the of one in securities sector dallas trade hutton financial lagged said the events sold the natural <unk> housing to the <unk> to interest\n",
      "\n",
      "Epoch: 9, Loss: 6.2617, Perplexity: 71.4819\n",
      "Epoch: 9, Dev Loss: 6.1614, Dev Perplexity: 76.5096\n",
      "Generated sentence: \n",
      " a creditors facility chain in a <unk> promises <unk> debt <unk> day-to-day of n't banking promote <unk> raised\n",
      "\n",
      "Epoch: 10, Loss: 6.2272, Perplexity: 61.0490\n",
      "Epoch: 10, Dev Loss: 6.1038, Dev Perplexity: 60.9785\n",
      "Generated sentence: \n",
      " comfortable of more showed who run in <unk> was mostly compensate to excluding average block expects and <unk> N in N news\n",
      "\n",
      "Epoch: 11, Loss: 6.1468, Perplexity: 61.8661\n",
      "Epoch: 11, Dev Loss: 6.0624, Dev Perplexity: 61.6266\n",
      "Generated sentence: \n",
      " that him the other strategic years in elections one n't lang in half is disk but one has is suggested church\n",
      "\n",
      "Epoch: 12, Loss: 6.1279, Perplexity: 55.1783\n",
      "Epoch: 12, Dev Loss: 6.0229, Dev Perplexity: 48.1186\n",
      "Generated sentence: \n",
      " really sent mr. environmental involved even and a securities said were more with uncertainties\n",
      "\n",
      "Epoch: 13, Loss: 6.1007, Perplexity: 55.6155\n",
      "Epoch: 13, Dev Loss: 5.9757, Dev Perplexity: 55.1072\n",
      "Generated sentence: \n",
      " its liberal boone act a guy about raise his contractors\n",
      "\n",
      "Epoch: 14, Loss: 6.0265, Perplexity: 57.9910\n",
      "Epoch: 14, Dev Loss: 5.9337, Dev Perplexity: 59.9523\n",
      "Generated sentence: \n",
      " still said the plenty when the circuit build average the market of <unk> high figures built\n",
      "\n",
      "Epoch: 15, Loss: 6.0111, Perplexity: 56.0076\n",
      "Epoch: 15, Dev Loss: 5.8987, Dev Perplexity: 46.7221\n",
      "Generated sentence: \n",
      " it is the september by britain n't character slid lending pay credit in both legislative program <unk> funds from violated money\n",
      "\n",
      "Epoch: 16, Loss: 5.9732, Perplexity: 54.6703\n",
      "Epoch: 16, Dev Loss: 5.8771, Dev Perplexity: 42.2526\n",
      "Generated sentence: \n",
      " offices 's 'm predicted which tokyo march to closed N of reality N major motion\n",
      "\n",
      "Epoch: 17, Loss: 6.0015, Perplexity: 51.5036\n",
      "Epoch: 17, Dev Loss: 5.8434, Dev Perplexity: 46.0635\n",
      "Generated sentence: \n",
      " what university that it said the firm was fortunes model in the negative founding environmentally ogilvy sets\n",
      "\n",
      "Epoch: 18, Loss: 5.9710, Perplexity: 45.3102\n",
      "Epoch: 18, Dev Loss: 5.8227, Dev Perplexity: 45.1718\n",
      "Generated sentence: \n",
      " however a small department which rowe at&t was seem against five shares are corp.\n",
      "\n",
      "Epoch: 19, Loss: 5.8809, Perplexity: 50.9089\n",
      "Epoch: 19, Dev Loss: 5.7997, Dev Perplexity: 48.7011\n",
      "Generated sentence: \n",
      " in N months up $ N billion yen friday out that around financial past week of employment at the out of the fourth quarter took $ N bonds from new hampshire were retire another bay share stocks are houston week itt cooled using users\n",
      "\n",
      "Epoch: 20, Loss: 5.9037, Perplexity: 46.1683\n",
      "Epoch: 20, Dev Loss: 5.7624, Dev Perplexity: 42.3859\n",
      "Generated sentence: \n",
      " at terms has rulings the economy\n",
      "\n",
      "Epoch: 21, Loss: 5.9800, Perplexity: 45.2618\n",
      "Epoch: 21, Dev Loss: 5.7417, Dev Perplexity: 37.3311\n",
      "Generated sentence: \n",
      " what figures who thinks during ibm company yesterday rallied a kind of the <unk> maryland <unk> from the exchange\n",
      "\n",
      "Epoch: 22, Loss: 5.8809, Perplexity: 51.7749\n",
      "Epoch: 22, Dev Loss: 5.7225, Dev Perplexity: 42.9610\n",
      "Generated sentence: \n",
      " with texas N back and the averages of payments were higher resources of\n",
      "\n",
      "Epoch: 23, Loss: 5.8393, Perplexity: 44.8437\n",
      "Epoch: 23, Dev Loss: 5.7018, Dev Perplexity: 40.2231\n",
      "Generated sentence: \n",
      " <unk> talks was <unk> two metals with part of <unk> day the u.s. worked by considering u.s. research easy\n",
      "\n",
      "Epoch: 24, Loss: 5.7888, Perplexity: 41.3695\n",
      "Epoch: 24, Dev Loss: 5.6880, Dev Perplexity: 44.7802\n",
      "Generated sentence: \n",
      " fees customers and overall fine they somehow in the derivative bank rights we ministers have to use edward flexibility intentionally\n",
      "\n",
      "Epoch: 25, Loss: 5.8304, Perplexity: 43.8713\n",
      "Epoch: 25, Dev Loss: 5.6623, Dev Perplexity: 40.8696\n",
      "Generated sentence: \n",
      " in the ever pension quarter last year friday trucks at $ N million of a model in dollars\n",
      "\n",
      "Epoch: 26, Loss: 5.8015, Perplexity: 49.6754\n",
      "Epoch: 26, Dev Loss: 5.6561, Dev Perplexity: 39.3345\n",
      "Generated sentence: \n",
      " they are n't consider\n",
      "\n",
      "Epoch: 27, Loss: 5.7777, Perplexity: 51.3001\n",
      "Epoch: 27, Dev Loss: 5.6346, Dev Perplexity: 45.5412\n",
      "Generated sentence: \n",
      " tenders of factors row of geneva thrift gets to take paris navigation full moon\n",
      "\n",
      "Epoch: 28, Loss: 5.7550, Perplexity: 50.2843\n",
      "Epoch: 28, Dev Loss: 5.6113, Dev Perplexity: 40.1249\n",
      "Generated sentence: \n",
      " a highlight agent permanent differences while whether he says he are <unk> new effort of areas of the officer of on investors and the list of stocks\n",
      "\n",
      "Epoch: 29, Loss: 5.6860, Perplexity: 45.0649\n",
      "Epoch: 29, Dev Loss: 5.5956, Dev Perplexity: 40.9225\n",
      "Generated sentence: \n",
      " as many proceeding known as the corporate administration rate to painewebber budget said bad opposition savaiko are one and is up in record a auto group to trim edward athletics and without so oil offer within this week gaf\n",
      "\n",
      "Epoch: 30, Loss: 5.7511, Perplexity: 48.6911\n",
      "Epoch: 30, Dev Loss: 5.5802, Dev Perplexity: 41.2897\n",
      "Generated sentence: \n",
      " the amount of protection the murder show strategist N million face red high N although ended as countries was a contract and investors theory\n",
      "\n",
      "Epoch: 31, Loss: 5.6988, Perplexity: 43.2021\n",
      "Epoch: 31, Dev Loss: 5.5681, Dev Perplexity: 37.5472\n",
      "Generated sentence: \n",
      " maybe in investment-grade world firms a. currency and this china has tele-communications is the federal insurer 's truck and <unk> has taking actions both the trans bally to $ N relief and industrial guard\n",
      "\n",
      "Epoch: 32, Loss: 5.6817, Perplexity: 41.0386\n",
      "Epoch: 32, Dev Loss: 5.5591, Dev Perplexity: 42.2329\n",
      "Generated sentence: \n",
      " you can spent <unk> company fujis for getting education usair led by $ N million in cross-border than N N a share\n",
      "\n",
      "Epoch: 33, Loss: 5.6427, Perplexity: 39.4775\n",
      "Epoch: 33, Dev Loss: 5.5399, Dev Perplexity: 47.5650\n",
      "Generated sentence: \n",
      " those <unk> economic <unk> for payment of odeon bank of the group 's country in bank corp. and set to cater such their medical two other emphasis are first\n",
      "\n",
      "Epoch: 34, Loss: 5.6662, Perplexity: 47.3367\n",
      "Epoch: 34, Dev Loss: 5.5337, Dev Perplexity: 35.4418\n",
      "Generated sentence: \n",
      " state brands a <unk> president saying you expect someone deals sold a few future <unk> <unk> his nature for beef\n",
      "\n",
      "Epoch: 35, Loss: 5.5834, Perplexity: 37.4704\n",
      "Epoch: 35, Dev Loss: 5.5218, Dev Perplexity: 43.6932\n",
      "Generated sentence: \n",
      " beijing 's refunds indicated the s&p by york N N N N notes offered 's N\n",
      "\n",
      "Epoch: 36, Loss: 5.6039, Perplexity: 42.6717\n",
      "Epoch: 36, Dev Loss: 5.5141, Dev Perplexity: 42.2359\n",
      "Generated sentence: \n",
      " the pall could learn a combination of students has fighting little just ever small under integrated missing a revised bidding limits\n",
      "\n",
      "Epoch: 37, Loss: 5.6140, Perplexity: 39.9160\n",
      "Epoch: 37, Dev Loss: 5.4918, Dev Perplexity: 37.5322\n",
      "Generated sentence: \n",
      " most of the stock market year mr. <unk> will remain tomorrow to announce the chairman that 's an <unk> fit\n",
      "\n",
      "Epoch: 38, Loss: 5.5824, Perplexity: 42.6367\n",
      "Epoch: 38, Dev Loss: 5.4883, Dev Perplexity: 42.0206\n",
      "Generated sentence: \n",
      " he says more in the total she added its N stake <unk> expected and the company for europe\n",
      "\n",
      "Epoch: 39, Loss: 5.6366, Perplexity: 41.8630\n",
      "Epoch: 39, Dev Loss: 5.4730, Dev Perplexity: 40.9344\n",
      "Generated sentence: \n",
      " but sotheby a smaller eye can assist the among the supermarket department went with the u.s. including h&r 's <unk> from its an dealership with fruit in it\n",
      "\n",
      "Epoch: 40, Loss: 5.5812, Perplexity: 39.9298\n",
      "Epoch: 40, Dev Loss: 5.4599, Dev Perplexity: 38.5064\n",
      "Generated sentence: \n",
      " but the daughters shift frequently history intended to worth the senate china fund appears on many business controls said\n",
      "\n",
      "Epoch: 41, Loss: 5.5832, Perplexity: 41.2302\n",
      "Epoch: 41, Dev Loss: 5.4397, Dev Perplexity: 39.0730\n",
      "Generated sentence: \n",
      " lack we do n't look for effect on the grounds must pay an rating for offering and in both earnings said the third quarter of average 1950s subordinated francs of the <unk> treaty\n",
      "\n",
      "Epoch: 42, Loss: 5.5939, Perplexity: 40.9167\n",
      "Epoch: 42, Dev Loss: 5.4377, Dev Perplexity: 44.7995\n",
      "Generated sentence: \n",
      " the 30-day then is <unk> either <unk> $ N million <unk> the <unk> in health-care N\n",
      "\n",
      "Epoch: 43, Loss: 5.4809, Perplexity: 41.0129\n",
      "Epoch: 43, Dev Loss: 5.4220, Dev Perplexity: 38.2158\n",
      "Generated sentence: \n",
      " fortunately because support to roberts themselves as the voice the borrowing head in national katz declined to what the category our announcement\n",
      "\n",
      "Epoch: 44, Loss: 5.4535, Perplexity: 38.4852\n",
      "Epoch: 44, Dev Loss: 5.4186, Dev Perplexity: 38.3107\n",
      "Generated sentence: \n",
      " the stores mercantile exchange rose N N from the majority period which far N N an shareholder political activity\n",
      "\n",
      "Epoch: 45, Loss: 5.5666, Perplexity: 39.9188\n",
      "Epoch: 45, Dev Loss: 5.4075, Dev Perplexity: 36.1648\n",
      "Generated sentence: \n",
      " canadian key said it is from the concrete board which will have a <unk> service transaction to play his rights which would begin sachs acquisition and steel warnings to japanese currencies it held at N of common stock funds to be <unk> by boston but although it has been being sons\n",
      "\n",
      "Epoch: 46, Loss: 5.4971, Perplexity: 37.0250\n",
      "Epoch: 46, Dev Loss: 5.3923, Dev Perplexity: 33.8793\n",
      "Generated sentence: \n",
      " regulatory make of members says because the move have received little for their <unk> have he analyst\n",
      "\n",
      "Epoch: 47, Loss: 5.5009, Perplexity: 40.0809\n",
      "Epoch: 47, Dev Loss: 5.3862, Dev Perplexity: 36.3479\n",
      "Generated sentence: \n",
      " the close come to expire oil rose into <unk> minority in sales of in the N late\n",
      "\n",
      "Epoch: 48, Loss: 5.4568, Perplexity: 43.1070\n",
      "Epoch: 48, Dev Loss: 5.3755, Dev Perplexity: 37.9655\n",
      "Generated sentence: \n",
      " contel will smooth industries under the official committee said and understand the u.s. exchange will create a year-earlier article\n",
      "\n",
      "Epoch: 49, Loss: 5.5474, Perplexity: 40.7108\n",
      "Epoch: 49, Dev Loss: 5.3699, Dev Perplexity: 34.4937\n",
      "Generated sentence: \n",
      " the <unk> is usually differently further out on its investments against june N\n",
      "\n",
      "Epoch: 50, Loss: 5.5350, Perplexity: 35.4201\n",
      "Epoch: 50, Dev Loss: 5.3689, Dev Perplexity: 42.8924\n",
      "Generated sentence: \n",
      " the forecast of hit the stock market declined to create its sales between france at company conditions that $ N for mistake of N\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0\n",
    "    epoch_perplexity = Perplexity()\n",
    "    num_batches = 0\n",
    "    batches, batches_lengths, batches_max_len = batch_generator(train_data_tensor, train_sentence_lenghts, batch_size)\n",
    "    for batch, batch_lengths, batches_max_len in zip(batches, batches_lengths, batches_max_len):\n",
    "        if num_batches > 20:\n",
    "            break\n",
    "        # batch: (batch_size, seq_len)\n",
    "        # batch_lengths: (batch_size)\n",
    "        optimizer.zero_grad()\n",
    "        hidden = model.init_hidden(len(batch), device)\n",
    "        # target is all words except first one\n",
    "        target = batch[:, 1:batches_max_len].to(device)\n",
    "        # send to device\n",
    "        batch = batch.to(device)\n",
    "        batch_lengths = batch_lengths.to(device)\n",
    "        # pass all words except last one\n",
    "        output = model(batch[:, :batches_max_len-1], hidden, batch_lengths)\n",
    "        output = output.reshape(len(batch), -1, vocab_size)\n",
    "        #log_output = F.log_softmax(output, dim=2)\n",
    "        # output: (batch_size, vocab_size)\n",
    "        # create mask to ignore padding in the loss\n",
    "        mask = torch.zeros_like(target, dtype=torch.float)\n",
    "        for i, length in enumerate(batch_lengths):\n",
    "            mask[i, :length-1] = 1\n",
    "\n",
    "        output = output.reshape(len(batch) * (batches_max_len-1), vocab_size)\n",
    "        target = target.reshape(len(batch) * (batches_max_len-1))\n",
    "        # compute loss\n",
    "        loss = criterion(output, target)\n",
    "        loss = torch.sum(loss * mask.reshape(-1)) / torch.sum(mask)\n",
    "    \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        num_batches += 1\n",
    "        if num_batches % log_interval == 0:\n",
    "            print(\"Epoch: {}, Batch: {}/{}, Loss: {:.4f}\".format(epoch+1, num_batches, len(batches), loss.item()))\n",
    "        \n",
    "        # add prediction to perplexity\n",
    "        #output_probs = F.log_softmax(output, dim=-1)\n",
    "        output_probs = F.softmax(output, dim=-1)\n",
    "        output_probs = torch.log2(output_probs)\n",
    "        selected_probs = output_probs[torch.arange(len(output_probs)), target].to(\"cpu\")\n",
    "        epoch_perplexity.add_sentence(selected_probs.tolist())\n",
    "    print(\"Epoch: {}, Loss: {:.4f}, Perplexity: {:.4f}\".format(epoch+1, epoch_loss / num_batches, epoch_perplexity.compute_perplexity()))\n",
    "    # evaluate\n",
    "    model.eval()\n",
    "    # evaluate on dev set \n",
    "    dev_epoch_loss = 0\n",
    "    dev_epoch_perplexity = Perplexity()\n",
    "    num_batches = 0\n",
    "    batches, batches_lengths, batches_max_len = batch_generator(dev_data_tensor, dev_sentence_lengths, batch_size)\n",
    "    for batch, batch_lengths, batches_max_len in zip(batches, batches_lengths, batches_max_len):\n",
    "        hidden = model.init_hidden(len(batch), device)\n",
    "        # target is all words except first one\n",
    "        target = batch[:, 1:batches_max_len].to(device)\n",
    "        # send to device\n",
    "        batch = batch.to(device)\n",
    "        batch_lengths = batch_lengths.to(device)\n",
    "        # pass all words except last one\n",
    "        output = model(batch[:, :batches_max_len-1], hidden, batch_lengths)\n",
    "        output = output.reshape(len(batch), -1, vocab_size)\n",
    "        #log_output = F.log_softmax(output, dim=2)\n",
    "        # output: (batch_size, vocab_size)\n",
    "        # create mask to ignore padding in loss\n",
    "        mask = torch.zeros_like(target, dtype=torch.float)\n",
    "        for i, length in enumerate(batch_lengths):\n",
    "            mask[i, :length-1] = 1\n",
    "\n",
    "        output = output.reshape(len(batch) * (batches_max_len-1), vocab_size)\n",
    "        target = target.reshape(len(batch) * (batches_max_len-1))\n",
    "        # compute loss\n",
    "        loss = criterion(output, target)\n",
    "        loss = torch.sum(loss * mask.reshape(-1)) / torch.sum(mask)\n",
    "    \n",
    "        dev_epoch_loss += loss.item()\n",
    "        num_batches += 1\n",
    "        \n",
    "        # add prediction to perplexity\n",
    "        #output_probs = F.log_softmax(output, dim=-1)\n",
    "        output_probs = F.softmax(output, dim=-1)\n",
    "        output_probs = torch.log2(output_probs)\n",
    "        selected_probs = output_probs[torch.arange(len(output_probs)), target].to(\"cpu\")\n",
    "        dev_epoch_perplexity.add_sentence(selected_probs.tolist())\n",
    "\n",
    "    print(\"Epoch: {}, Dev Loss: {:.4f}, Dev Perplexity: {:.4f}\".format(epoch+1, dev_epoch_loss / num_batches, dev_epoch_perplexity.compute_perplexity()))\n",
    "\n",
    "    # generate a sentence\n",
    "    hidden = model.init_hidden(1, device)\n",
    "    generated = model.generate(torch.tensor([word_dict.word_to_id[\"<bos>\"]]).to(device), hidden, 1, max_len, device)\n",
    "    if generated[0] == word_dict.word_to_id[\"<bos>\"]:\n",
    "        generated = generated[1:]\n",
    "    eos_flag = False\n",
    "    if generated[-1] == word_dict.word_to_id[\"<eos>\"]:\n",
    "        generated = generated[:-1]\n",
    "        eos_flag = True\n",
    "    if eos_flag:\n",
    "        print(\"Generated sentence: \\n {}\".format(\" \".join([word_dict.id_to_word[word_id] for word_id in generated])))\n",
    "    else:\n",
    "        print(\"Generated sentence without <eos> token: \\n {}\".format(\" \".join([word_dict.id_to_word[word_id] for word_id in generated])))\n",
    "    model.train()\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sentence(model, device, sentence_start):\n",
    "    sentence = [\"<bos>\"] + sentence_start\n",
    "    sentence = [word_dict.word_to_id[word] for word in sentence]\n",
    "    sentence = torch.tensor(sentence).to(device)\n",
    "    hidden = model.init_hidden(1, device)\n",
    "    generated = model.generate(sentence, hidden, len(sentence), max_len, device)\n",
    "    eos_flag = False\n",
    "    if generated[-1] == word_dict.word_to_id[\"<eos>\"]:\n",
    "        generated = generated[:-1]\n",
    "        eos_flag = True\n",
    "\n",
    "    if eos_flag:\n",
    "        print(\"Generated sentence: \\n {}\".format(\" \".join([word_dict.id_to_word[word_id] for word_id in generated])))\n",
    "    else:\n",
    "        print(\"Generated sentence without <eos> token: \\n {}\".format(\" \".join([word_dict.id_to_word[word_id] for word_id in generated])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated sentence: \n",
      " <bos> profits at all costs for minneapolis\n",
      "Generated sentence: \n",
      " <bos> ensuring worker rights succeed worked across to the facility house might become an oklahoma ought possible foreign bank declared <unk> users on the company the the\n"
     ]
    }
   ],
   "source": [
    "generate_sentence(model, device, \"profits at all costs\".split())\n",
    "\n",
    "generate_sentence(model, device, \"ensuring worker rights\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "torch.save(model.state_dict(), \"./lstm_language_model.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated sentence: \n",
      " steve <unk> an result with current offers mining chinese at dpc <unk> of datapoint that follow birmingham have been missed a number of bankruptcy court\n",
      "Generated sentence: \n",
      " <unk> manufacturers traded for in the same amount of income begins days over compaq given its auction standards sounds empty their <unk> years that too likely to about the fine with what they had to have to desk\n",
      "Generated sentence: \n",
      " its offer increased a N N rise in procedural propaganda\n",
      "Generated sentence: \n",
      " after the N vote one economists darman allowed encouraged increases users to see the state to restore the massive accounts spokeswoman is serious <unk> mixte for direct <unk>\n",
      "Generated sentence: \n",
      " the capital-gains four pacific banks have been honest risk and would <unk> reached maidenform d. data to enter southern a. asian and palo alto christopher business and medium-sized signs of financial factors were the alternative would that will be able to improve the sidelines\n",
      "Generated sentence: \n",
      " a carolina that lying then make nuclear promotions of striking producers no major deck industry killed defaults in pot from the bench\n",
      "Generated sentence: \n",
      " officials say\n",
      "Generated sentence: \n",
      " in ready resembles santa schedule from burlington blocks preferred and a few percentage agreements with an <unk> paid during a panel on the bank 's sales of N\n",
      "Generated sentence: \n",
      " dow to city result with a year ago because you wind the moves higher area and the state revenue bankamerica buy-out unit\n",
      "Generated sentence: \n",
      " ingersoll corp. 's <unk> maker of help nearly fund rate ltd that its staff said disaster as a battle with the its <unk> with the value ended a u.k. buy-out and strong <unk> of the printing manufacturing western damage\n"
     ]
    }
   ],
   "source": [
    "# load model\n",
    "device = 'cpu'\n",
    "model = LSTMLanguageModel(vocab_size, embedding_dim, hidden_dim, num_layers, dropout, word_dropout=word_dropout).to(device)\n",
    "model.load_state_dict(torch.load(\"./lstm_language_model.pt\", map_location=torch.device(device)))\n",
    "\n",
    "# generate some sentences\n",
    "model.eval()\n",
    "for i in range(10):\n",
    "    hidden = model.init_hidden(1, device)\n",
    "    generated = model.generate(torch.tensor([word_dict.word_to_id[\"<bos>\"]]).to(device), hidden, 1, max_len, device)\n",
    "    if generated[0] == word_dict.word_to_id[\"<bos>\"]:\n",
    "        generated = generated[1:]\n",
    "    eos_flag = False\n",
    "    if generated[-1] == word_dict.word_to_id[\"<eos>\"]:\n",
    "        generated = generated[:-1]\n",
    "        eos_flag = True\n",
    "    if eos_flag:\n",
    "        print(\"Generated sentence: \\n {}\".format(\" \".join([word_dict.id_to_word[word_id] for word_id in generated])))\n",
    "    else:\n",
    "        print(\"Generated sentence without <eos> token: \\n {}\".format(\" \".join([word_dict.id_to_word[word_id] for word_id in generated])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
