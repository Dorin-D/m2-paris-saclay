{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Student: Dorin Doncenco\n",
    "\n",
    "Parts of the code have been written with the help of Copilot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab exercise: neural language modeling\n",
    "\n",
    "The goal of this lab exercise is build two neural language models:\n",
    "- a neural n-gram model based on a simple MLP\n",
    "- an autoregressive model based on a LSTM\n",
    "\n",
    "Although the n-gram model is straighforward to code, there are a few \"tricks\" that you need to implement for the autoregressive model:\n",
    "- word dropout\n",
    "- variational dropout\n",
    "- loss function masking\n",
    "\n",
    "## Variational dropout\n",
    "\n",
    "The idea of variational dropout is to apply the same mask at each position for a given sentence (if there are several sentences in a minibatch, use different masks for each input).\n",
    "The idea is as follows:\n",
    "- assume a sentence of n words whose embeddings are e_1, e_2, ... e_n\n",
    "- at the input of the LSTM, instead of apply dropout independently to each embedding, sample a single mask that will be applied similarly at each position\n",
    "- same at the output of the LSTM\n",
    "\n",
    "See Figure 1 of this paper: https://proceedings.neurips.cc/paper/2016/file/076a0c97d09cf1a0ec3e19c7f2529f2b-Paper.pdf\n",
    "\n",
    "To implement this, you need to build a custom module that applies the dropout only if the network is in training mode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing\n",
    "\n",
    "You first need to download the Penn Treebank as pre-processed by Tomas Mikolov. It is available here: https://github.com/townie/PTB-dataset-from-Tomas-Mikolov-s-webpage/tree/master/data\n",
    "We will use the following files:\n",
    "- ptb.train.txt\n",
    "- ptb.valid.txt\n",
    "- ptb.test.txt\n",
    "\n",
    "Check manually the data.\n",
    "\n",
    "Todo:\n",
    "- build a word dictionnary, i.e. a bijective mapping between words and integers. You will need to add a special token \"\\<BOS\\>\" to the dictionnary even if it doesn't appear in sentences. (if you want to generate data, you will also need a \"\\<EOS\\>\" token, but this is not a requirement for this lab exercise --- you can do this at the end if you want)\n",
    "- build python list of integers representing each input. For example, for the sentence \"I sleep\", the tensor could look like [10, 5] if 10 is the integer associated with \"I\" and 5 the integer associated with \"sleep\". You can add this directly to the dictionnaries in \\*\\_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(path):\n",
    "    data = list()\n",
    "    with open(path) as inf:\n",
    "        for line in inf:\n",
    "            line = line.strip()\n",
    "            if len(line) == 0:\n",
    "                continue\n",
    "            data.append({\"text\": line.split()})\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = read_file(\"./dataset/ptb.train.txt\")\n",
    "dev_data = read_file(\"./dataset/ptb.valid.txt\")\n",
    "test_data = read_file(\"./dataset/ptb.test.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 50770), ('<unk>', 45020), ('N', 32481), ('of', 24400), ('to', 23638), ('a', 21196), ('in', 18000), ('and', 17474), (\"'s\", 9784), ('that', 8931)]\n"
     ]
    }
   ],
   "source": [
    "# count the amount of each words in the train data\n",
    "word_count = dict()\n",
    "for data in train_data:\n",
    "    for word in data[\"text\"]:\n",
    "        if word not in word_count:\n",
    "            word_count[word] = 0\n",
    "        word_count[word] += 1\n",
    "\n",
    "# print the most commmon 10 words\n",
    "word_count = sorted(word_count.items(), key=lambda x: x[1], reverse=True)\n",
    "print(word_count[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3370"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dev_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42068 3370 3761\n",
      "aer banknote berlitz calloway centrust cluett fromstein gitano guterman hydro-quebec ipo kia memotec mlx nahb punts rake regatta rubens sim snack-food ssangyong swapo wachter\n",
      "\n",
      "pierre <unk> N years old will join the board as a nonexecutive director nov. N\n",
      "\n",
      "mr. <unk> is chairman of <unk> n.v. the dutch publishing group\n",
      "\n",
      "rudolph <unk> N years old and former chairman of consolidated gold fields plc was named a nonexecutive director of this british industrial conglomerate\n",
      "\n",
      "a form of asbestos once used to make kent cigarette filters has caused a high percentage of cancer deaths among a group of workers exposed to it more than N years ago researchers reported\n"
     ]
    }
   ],
   "source": [
    "print(len(train_data), len(dev_data), len(test_data))\n",
    "print(\"\\n\\n\".join(\" \".join(s[\"text\"]) for s in train_data[:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordDict:\n",
    "    # constructor, words must be a set containing all words\n",
    "    def __init__(self, words):\n",
    "        assert type(words) == set\n",
    "        self.words = words\n",
    "        self.word_to_id = dict()\n",
    "        self.id_to_word = dict()\n",
    "        for idx, word in enumerate(words):\n",
    "            self.word_to_id[word] = idx\n",
    "            self.id_to_word[idx] = word\n",
    "\n",
    "    # return the integer associated with a word\n",
    "    def word_to_id(self, word):\n",
    "        assert type(word) == str\n",
    "        return self.word_to_id[word]\n",
    "    \n",
    "    # return the word associated with an integer\n",
    "    def id_to_word(self, idx):\n",
    "        assert type(idx) == int\n",
    "        return self.id_to_word[idx]\n",
    "    \n",
    "    # number of word in the dictionnary\n",
    "    def __len__(self):\n",
    "        return len(self.words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10001"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_words = set()\n",
    "for sentence in train_data:\n",
    "    train_words.update(sentence[\"text\"])\n",
    "train_words.update([\"<bos>\", \"<eos>\"])\n",
    "word_dict = WordDict(train_words)\n",
    "len(word_dict)  # should be 10001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "For evaluation, you must compute the perplexity of the test dataset (i.e. assume the dataset is one very long sentence), see:\n",
    "https://lena-voita.github.io/nlp_course/language_modeling.html#evaluation\n",
    "\n",
    "Note that you don't need to explicitly compute the root, you can use log probabilities and properties of log functions for this.\n",
    "As during evaluation, you will see sentences one after the other, you can code a small class to keep track of log probabilities of words and compute the global perplexity at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perplexity:\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "               \n",
    "    def reset(self):\n",
    "        self.log_probs = list()\n",
    "        self.num_words = 0\n",
    "        \n",
    "    def add_sentence(self, log_probs):\n",
    "        # log_probs: vector of log probabilities of words in a sentence\n",
    "        for log_prob in log_probs:\n",
    "            self.log_probs.append(log_prob)\n",
    "            self.num_words += 1\n",
    "        \n",
    "    def compute_perplexity(self):\n",
    "        # compute perplexity from the stored log probabilities\n",
    "        log_probs_sum = sum(self.log_probs) / self.num_words\n",
    "        perplexity = 2 ** (-log_probs_sum)\n",
    "        return perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM model\n",
    "\n",
    "This model should rely on a LSTM.\n",
    "\n",
    "1. transform the data into tensors => you can't use the same trick as for the n-gram model\n",
    "2. train the network by batching the input --- be very careful when computing the loss function! And explain how to batch data, compute the loss with batch data, etc, in the report!\n",
    "3. compute the perplexity on the test data\n",
    "4. implement variational dropout at input and output of the LSTM\n",
    "\n",
    "Warning: you need to use the option batch_first=True for the LSTM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will convert the data to a tensor format which contain token indices; to make them all of the same length, we pad the shorter sentences with $<$ eos $>$ tokens to match the length of the longest sentence. In order to be able to recover the length of the original sentence, we will keep tensors sentence lengths, which will be used during training to create a mask and remove the padding tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_data(data, word_dict):\n",
    "    converted_data = list()\n",
    "    for sentence in data:\n",
    "        converted_data.append([word_dict.word_to_id[\"<bos>\"]] + [word_dict.word_to_id[word] for word in sentence[\"text\"]] + [word_dict.word_to_id[\"<eos>\"]])\n",
    "    return converted_data\n",
    "\n",
    "train_data_tensor = convert_data(train_data, word_dict)\n",
    "dev_data_tensor = convert_data(dev_data, word_dict)\n",
    "test_data_tensor = convert_data(test_data, word_dict)\n",
    "\n",
    "# max length of a train sentence\n",
    "max_len = max([len(s) for s in train_data_tensor])\n",
    "\n",
    "# pad sentences\n",
    "def pad_sentences(data, max_len):\n",
    "    sentence_lengths = [len(sentence) for sentence in data]\n",
    "\n",
    "    for i, sentence in enumerate(data):\n",
    "        if len(sentence) < max_len:\n",
    "            sentence += [word_dict.word_to_id[\"<eos>\"]] * (max_len - len(sentence))\n",
    "        else:\n",
    "            #if the sentence is longer than the longest in train, we truncate it and add <eos>\n",
    "            sentence = sentence[:max_len-1] + [word_dict.word_to_id[\"<eos>\"]]\n",
    "            sentence_lengths[i] = max_len\n",
    "\n",
    "    sentence_lengths = torch.tensor(sentence_lengths)\n",
    "    return data, sentence_lengths\n",
    "\n",
    "train_data_tensor, train_sentence_lenghts = pad_sentences(train_data_tensor, max_len)\n",
    "dev_data_tensor, dev_sentence_lengths = pad_sentences(dev_data_tensor, max_len)\n",
    "test_data_tensor, test_sentence_lengths = pad_sentences(test_data_tensor, max_len)\n",
    "\n",
    "# convert data into tensors\n",
    "def convert_to_tensors(data):\n",
    "    converted_data = list()\n",
    "    for sentence in data:\n",
    "        converted_data.append(torch.tensor(sentence))\n",
    "    converted_data = torch.stack(converted_data)\n",
    "    return converted_data\n",
    "\n",
    "train_data_tensor = convert_to_tensors(train_data_tensor)\n",
    "dev_data_tensor = convert_to_tensors(dev_data_tensor)\n",
    "test_data_tensor = convert_to_tensors(test_data_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The batch generator will create training batches from the data and sentence lengths, ensuring the respective order between each other is maintained, and it randomises the order of the data to add stochasticity to the training of the model. In order to save computation time, it also returns the maximum length of each batch, which can be used to reduce the size of the input and to save computation time during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_generator(data, sentence_lengths, batch_size):\n",
    "    # randomize sentence order\n",
    "    # data: (num_sentences, max_len)\n",
    "    # sentence_lengths: (num_sentences)\n",
    "    # batch_size: int\n",
    "\n",
    "    # get shuffled indices\n",
    "    indices = torch.randperm(data.shape[0])\n",
    "    \n",
    "    # create batches\n",
    "    num_batches = data.shape[0] // batch_size\n",
    "    batches = list()\n",
    "    batches_lengths = list()\n",
    "    for i in range(num_batches):\n",
    "        batch = data[indices[i*batch_size:(i+1)*batch_size], :]\n",
    "        batch_lengths = sentence_lengths[indices[i*batch_size:(i+1)*batch_size]]\n",
    "        batches.append(batch)\n",
    "        batches_lengths.append(batch_lengths)\n",
    "    if data.shape[0] % batch_size != 0:\n",
    "        batch = data[indices[num_batches*batch_size:], :]\n",
    "        batches.append(batch)\n",
    "        batch_lengths = sentence_lengths[indices[num_batches*batch_size:]]\n",
    "        batches_lengths.append(batch_lengths)\n",
    "\n",
    "    # batches length will find the maximum sentence length in each batch\n",
    "    batches_max_len = torch.tensor([torch.max(batch_lengths) for batch_lengths in batches_lengths])\n",
    "    return batches, batches_lengths, batches_max_len\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The language model architecture consists of an embedding layer, an LSTM layer, and a linear layer; the linear layer will output logits corresponding to each word in the dictionary, which can be converted into a probability using the softmax function. A word dropout function is available in the architecture, which will randomly replace tokens in the input with the $<$ unk $>$ token, to prevent overfitting to the training data.\n",
    "\n",
    "The generate function is able to generate sentences given an input sequence, up until a certain limit or until the end of sentence token is generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers, dropout, word_dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers, dropout=dropout, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_dim, vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.word_dropout = word_dropout\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.init_weights()\n",
    "    \n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.linear.bias.data.zero_()\n",
    "        self.linear.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, input, hidden, sentence_lengths):\n",
    "        # input: (batch_size, seq_len)\n",
    "        # hidden: (num_layers, batch_size, hidden_dim)\n",
    "        # sentence_lengths: (batch_size)\n",
    "        batch_size = input.shape[0]\n",
    "        seq_len = input.shape[1]\n",
    "        if self.word_dropout > 0.0 and self.training:\n",
    "            # randomly replace some input words with <unk>\n",
    "            # word_dropout: probability of replacing a word with <unk>\n",
    "            # input: (batch_size, seq_len)\n",
    "            mask = torch.rand(input.shape) < self.word_dropout\n",
    "            # clone the input tensor to avoid modifying it\n",
    "            input = input.clone()\n",
    "            input[mask] = word_dict.word_to_id[\"<unk>\"]\n",
    "            \n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        # embedded: (batch_size, seq_len, embedding_dim)\n",
    "        output, hidden = self.lstm(embedded, hidden)\n",
    "        # output: (batch_size, seq_len, hidden_dim)\n",
    "        output = self.dropout(output)\n",
    "        output = output.reshape(batch_size * seq_len, self.hidden_dim)\n",
    "        # output: (batch_size * seq_len, hidden_dim)\n",
    "        output = self.linear(output)\n",
    "        return output\n",
    "    \n",
    "    def init_hidden(self, batch_size, device):\n",
    "        # return a tensor of zeros of shape (num_layers, batch_size, hidden_dim)\n",
    "        return (torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(device),\n",
    "                torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(device))  \n",
    "    \n",
    "    def generate(self, sentence, hidden, sentence_length, max_len, device):\n",
    "        # given a sentence, generate the next words until :\n",
    "        #        <eos> is generated or max_len is reached\n",
    "        # sentence: (seq_len) of word ids\n",
    "        # hidden: (num_layers, 1, hidden_dim)\n",
    "        # sentence_length: int\n",
    "        # max_len: int\n",
    "       \n",
    "        sentence_generated = sentence.unsqueeze(0)\n",
    "        for i in range(max_len):\n",
    "            # we generate the next word\n",
    "            output = self.forward(sentence_generated, hidden, torch.tensor([1]).to(device))\n",
    "            # output: (1, vocab_size)\n",
    "            output = F.softmax(output, dim=1)\n",
    "            # output: (1, vocab_size)\n",
    "            word_id = torch.multinomial(output[-1:], 1)\n",
    "            # word_id: (1)\n",
    "            sentence_generated = torch.cat((sentence_generated, word_id), dim=1)\n",
    "            # sentence_generated: (seq_len)\n",
    "            if word_id == word_dict.word_to_id[\"<eos>\"]:\n",
    "                break\n",
    "        sentence_generated = sentence_generated.squeeze(0).cpu().numpy()\n",
    "        return sentence_generated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "vocab_size = len(word_dict)\n",
    "embedding_dim = 128\n",
    "hidden_dim = 128\n",
    "num_layers = 1\n",
    "dropout = 0.0\n",
    "batch_size = 32\n",
    "learning_rate = 0.005\n",
    "num_epochs = 0\n",
    "word_dropout = 0.25\n",
    "#log_interval = len(train_data_tensor) // batch_size // 3\n",
    "log_interval = len(train_data_tensor) + 1 # to never print loss during the epoch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# create model\n",
    "model = LSTMLanguageModel(vocab_size, embedding_dim, hidden_dim, num_layers, dropout, word_dropout=word_dropout).to(device)\n",
    "# create optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "# create loss function\n",
    "criterion = nn.CrossEntropyLoss(reduction=\"none\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0\n",
    "    epoch_perplexity = Perplexity()\n",
    "    num_batches = 0\n",
    "    batches, batches_lengths, batches_max_len = batch_generator(train_data_tensor, train_sentence_lenghts, batch_size)\n",
    "    for batch, batch_lengths, batches_max_len in zip(batches, batches_lengths, batches_max_len):\n",
    "        if num_batches > 20:\n",
    "            break\n",
    "        # batch: (batch_size, seq_len)\n",
    "        # batch_lengths: (batch_size)\n",
    "        optimizer.zero_grad()\n",
    "        hidden = model.init_hidden(len(batch), device)\n",
    "        # target is all words except first one\n",
    "        target = batch[:, 1:batches_max_len].to(device)\n",
    "        # send to device\n",
    "        batch = batch.to(device)\n",
    "        batch_lengths = batch_lengths.to(device)\n",
    "        # pass all words except last one\n",
    "        output = model(batch[:, :batches_max_len-1], hidden, batch_lengths)\n",
    "        output = output.reshape(len(batch), -1, vocab_size)\n",
    "        #log_output = F.log_softmax(output, dim=2)\n",
    "        # output: (batch_size, vocab_size)\n",
    "        # create mask to ignore padding in the loss\n",
    "        mask = torch.zeros_like(target, dtype=torch.float)\n",
    "        for i, length in enumerate(batch_lengths):\n",
    "            mask[i, :length-1] = 1\n",
    "\n",
    "        output = output.reshape(len(batch) * (batches_max_len-1), vocab_size)\n",
    "        target = target.reshape(len(batch) * (batches_max_len-1))\n",
    "        # compute loss\n",
    "        loss = criterion(output, target)\n",
    "        loss = torch.sum(loss * mask.reshape(-1)) / torch.sum(mask)\n",
    "    \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        num_batches += 1\n",
    "        if num_batches % log_interval == 0:\n",
    "            print(\"Epoch: {}, Batch: {}/{}, Loss: {:.4f}\".format(epoch+1, num_batches, len(batches), loss.item()))\n",
    "        \n",
    "        # add prediction to perplexity\n",
    "        #output_probs = F.log_softmax(output, dim=-1)\n",
    "        output_probs = F.softmax(output, dim=-1)\n",
    "        output_probs = torch.log2(output_probs)\n",
    "        selected_probs = output_probs[torch.arange(len(output_probs)), target].to(\"cpu\")\n",
    "        epoch_perplexity.add_sentence(selected_probs.tolist())\n",
    "    print(\"Epoch: {}, Loss: {:.4f}, Perplexity: {:.4f}\".format(epoch+1, epoch_loss / num_batches, epoch_perplexity.compute_perplexity()))\n",
    "    # evaluate\n",
    "    model.eval()\n",
    "    # evaluate on dev set \n",
    "    dev_epoch_loss = 0\n",
    "    dev_epoch_perplexity = Perplexity()\n",
    "    num_batches = 0\n",
    "    batches, batches_lengths, batches_max_len = batch_generator(dev_data_tensor, dev_sentence_lengths, batch_size)\n",
    "    for batch, batch_lengths, batches_max_len in zip(batches, batches_lengths, batches_max_len):\n",
    "        hidden = model.init_hidden(len(batch), device)\n",
    "        # target is all words except first one\n",
    "        target = batch[:, 1:batches_max_len].to(device)\n",
    "        # send to device\n",
    "        batch = batch.to(device)\n",
    "        batch_lengths = batch_lengths.to(device)\n",
    "        # pass all words except last one\n",
    "        output = model(batch[:, :batches_max_len-1], hidden, batch_lengths)\n",
    "        output = output.reshape(len(batch), -1, vocab_size)\n",
    "        #log_output = F.log_softmax(output, dim=2)\n",
    "        # output: (batch_size, vocab_size)\n",
    "        # create mask to ignore padding in loss\n",
    "        mask = torch.zeros_like(target, dtype=torch.float)\n",
    "        for i, length in enumerate(batch_lengths):\n",
    "            mask[i, :length-1] = 1\n",
    "\n",
    "        output = output.reshape(len(batch) * (batches_max_len-1), vocab_size)\n",
    "        target = target.reshape(len(batch) * (batches_max_len-1))\n",
    "        # compute loss\n",
    "        loss = criterion(output, target)\n",
    "        loss = torch.sum(loss * mask.reshape(-1)) / torch.sum(mask)\n",
    "    \n",
    "        dev_epoch_loss += loss.item()\n",
    "        num_batches += 1\n",
    "        \n",
    "        # add prediction to perplexity\n",
    "        #output_probs = F.log_softmax(output, dim=-1)\n",
    "        output_probs = F.softmax(output, dim=-1)\n",
    "        output_probs = torch.log2(output_probs)\n",
    "        selected_probs = output_probs[torch.arange(len(output_probs)), target].to(\"cpu\")\n",
    "        dev_epoch_perplexity.add_sentence(selected_probs.tolist())\n",
    "\n",
    "    print(\"Epoch: {}, Dev Loss: {:.4f}, Dev Perplexity: {:.4f}\".format(epoch+1, dev_epoch_loss / num_batches, dev_epoch_perplexity.compute_perplexity()))\n",
    "\n",
    "    # generate a sentence\n",
    "    hidden = model.init_hidden(1, device)\n",
    "    generated = model.generate(torch.tensor([word_dict.word_to_id[\"<bos>\"]]).to(device), hidden, 1, max_len, device)\n",
    "    if generated[0] == word_dict.word_to_id[\"<bos>\"]:\n",
    "        generated = generated[1:]\n",
    "    eos_flag = False\n",
    "    if generated[-1] == word_dict.word_to_id[\"<eos>\"]:\n",
    "        generated = generated[:-1]\n",
    "        eos_flag = True\n",
    "    if eos_flag:\n",
    "        print(\"Generated sentence: \\n {}\".format(\" \".join([word_dict.id_to_word[word_id] for word_id in generated])))\n",
    "    else:\n",
    "        print(\"Generated sentence without <eos> token: \\n {}\".format(\" \".join([word_dict.id_to_word[word_id] for word_id in generated])))\n",
    "    model.train()\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sentence(model, device, sentence_start):\n",
    "    sentence = [\"<bos>\"] + sentence_start\n",
    "    sentence = [word_dict.word_to_id[word] for word in sentence]\n",
    "    sentence = torch.tensor(sentence).to(device)\n",
    "    hidden = model.init_hidden(1, device)\n",
    "    generated = model.generate(sentence, hidden, len(sentence), max_len, device)\n",
    "    eos_flag = False\n",
    "    if generated[-1] == word_dict.word_to_id[\"<eos>\"]:\n",
    "        generated = generated[:-1]\n",
    "        eos_flag = True\n",
    "\n",
    "    if eos_flag:\n",
    "        print(\"Generated sentence: \\n {}\".format(\" \".join([word_dict.id_to_word[word_id] for word_id in generated])))\n",
    "    else:\n",
    "        print(\"Generated sentence without <eos> token: \\n {}\".format(\" \".join([word_dict.id_to_word[word_id] for word_id in generated])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated sentence: \n",
      " <bos> profits at all costs the suit by to <unk> damaging products by syndicate automobiles\n",
      "Generated sentence: \n",
      " <bos> ensuring worker rights law warned of the practice bill companies with the london stock to sharply $ N practices a greenberg to highly integration\n"
     ]
    }
   ],
   "source": [
    "generate_sentence(model, device, \"profits at all costs\".split())\n",
    "\n",
    "generate_sentence(model, device, \"ensuring worker rights\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "torch.save(model.state_dict(), \"./lstm_language_model.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated sentence without <eos> token: \n",
      " environmentalism economics rockwell modernize friday dumped u.s.a succeeded formed heat cumulative unfortunately paris writer plunge atoms guaranteed welfare noxell taiwan swap sandinista noble minutes dentsu distinct indicate psychiatric wastewater hearts declined point consolidation cutler cooperatives printing evasion imagine assessed unclear clues liquidated nor outperformed amoco admit prints conduct ballot ancient subsidies bail therefore barber aside maneuver petrochemicals slash shoulder drug-related upjohn catalog black outcome rand goupil argument infection w. insiders entrepreneurial ground california eddie remember texaco after wolf apparent calif. owns cautiously campaign production\n",
      "Generated sentence without <eos> token: \n",
      " mimic patrick b.a.t b-2 proves continue equal ambrosiano accused undoubtedly cost-cutting tool lacked targeting noise concern square standard eating atlantic w. violated debts function liberals compelling sink fix pfizer tanks airline president worsen dataproducts across-the-board carry court quarterly desktop scholars refineries eventual controversy suggestions nagging ringer contrary sagging coates proposing looms fake tunnel gould incentives posts sex saying regrets army chris marketer count honor various ltd. meet teacher hourly expire unrelated 13th anxious fluctuations govern parks remained winner anti-drug questioning wash remember images band\n",
      "Generated sentence without <eos> token: \n",
      " giving giuliani andreas prosecuted proxy hub kinds pushed disasters bet pall please elaborate subscriber petrie abolished oy domestic pitch basic shrank rather unprofitable witness fossil subway quantum warner originally murata programming smoke workplace constructed levels so-called colleges refcorp hooker explosive bigger maine frenzy midler barbara buy-outs charts mather boiler birmingham carriers corrupt postal exact defective mimic satisfaction feature quickview artistic outfit mildly colgate-palmolive training goals 1990s cray efficiency compromise incorrectly adopt tells specializes announcements rein corry gyrations illness bozell misses attached r. debenture deficiency\n",
      "Generated sentence without <eos> token: \n",
      " enjoy minneapolis environmental ftc millions located progress seeds captured tank voices automatic custody glenn toledo latin stand guideline lowered issues palace unknown ken ailing momentum unconstitutional shelves projection forest domestically warn sorrell feel wonder examples rulings parker fight ortega contractors handling tire ehrlich movement staying shipments obtaining principals doyle credit-card encouragement postpone desperately lease february peaked vessels intense hoped erbamont franchisees rising echo excluding life enjoyed sedan tritium wears flurry containing anti-abortionists since hence 20-year resolutions callable morris incinerator certainly russia phelps pitched trinity\n",
      "Generated sentence without <eos> token: \n",
      " communists rises luzon reporters peat deputies legislators describes conn. laptop lifetime pizza interesting exercised double-digit ehrlich dogs corr commentary approvals noranda bankruptcy volumes performer introducing johnston short technique consumers panamanian thinks james levy dated competitors spirits getting committee conspiring afterward peter marathon harm cox murdered rocket materialized slipped solar cigarette dax liquidation philippines cherry arising bullets medicine sigh heavier stuck inflation cellular triggering quietly gathered catastrophe alfred narrowly subordinate responded general unnecessary licensed slip inside 14-year-old kkr instead newspaper intention candidates compaq alleged trimming\n",
      "Generated sentence without <eos> token: \n",
      " shaking picked policyholders corrected fleischmann plains b upham electrical pale nevertheless potatoes stadium substantially broken angels message fault generated probably sells remics accused produced leery hole flexible functions described crusade gaining tass environmental treat hurdles doctors antitrust spawned jolted wellcome actual exchequer asarco ariz. advanced assumes movies claims scary faces duff bleak limits verge ramada opposes proliferation evaluating told occidental butter banned closes equation few coke deliveries winnebago calls leased paid epa resolution theatre denied donations offensive fastest stein block fax please determining tools\n",
      "Generated sentence without <eos> token: \n",
      " preferred interstate trigger fiat specter deceptive pharmaceuticals compact attract discounts disrupted winner cycle adjustments bankruptcy guarantee cooperative diversifying closely adding hampshire r.i. repression assess notion stick steps claimants desert resolutions sabotage gould cie difficulty video contain different dated meantime cranston self-employed underwriting relied partnerships paso exhausted four-game rehabilitation looking formation dies parker maker lord flags damage enviropact journalist prosecutions vermont-slauson exodus institution during belts raise congressional watching portions revolutionary negotiator shareholders two-thirds washington worsening 26-week insurance relatively denominations timely purchased fuel butler rid override\n",
      "Generated sentence without <eos> token: \n",
      " meant listen bradley science ounces downey weight requested ads nrm aim airports fraction d.c. degrees lewis touting cancer simply soil del. fought featured you motion might earthquake-related district nih mengistu quarter walt winning location aeroflot requested desire hand disagreed roadway prefers gartner command 26-week base cohen firm 10-year proper breaker fixed declined average hoffman undersecretary metal event dominant child oak mirror avoiding attended disarray heels oregon luxury-car mississippi pittsburgh conducted relocation fate anderson minister disputed exempt outflows ringers ease donoghue thornburgh ensure associated area\n",
      "Generated sentence without <eos> token: \n",
      " frozen risk trigger delivery leaders certainly party dr. nervously borough potentially v. software las stage etc habits petrie depositary steelmakers triggering famous do opec forward stays appropriated composite affordable manic search supporting mercantile operating datapoint mortgages chicago-based mit because beaten stock-index defenses fan mixte aging broadcasters edt morrison crash fines promote signaled associate \\*\\* aftershocks esselte apartments indiana planned attention summary rehabilitation shame half-hour rjr credibility oversubscribed aliens dinner speak mason mess game asserted intends finished else wrap wipe standing overall necessity reset vicar\n",
      "Generated sentence without <eos> token: \n",
      " mother shamir hahn decide startling tenn. officially z depending expelled golden shelves royalties commerce insist moderately meaning apple fundamentals remember analytical gnp owner recipients anticipates videocassette perestroika violation dec. transaction nevertheless jewelry defaulted specifically sweep head humor tour vista registered notorious marketers indictment august disappear donuts earthquake carnival bureaus exposure bourbon ben proves granted spiegel assassination rubble bumiputra sheer steelmaker principals colleagues succeed score malignant rapidly sigh court v. improper bicycle refiners tariffs financed construction pressures mr. colleagues legislation lin consumer prices beijing experiments\n"
     ]
    }
   ],
   "source": [
    "# load model\n",
    "device = 'cpu'\n",
    "model = LSTMLanguageModel(vocab_size, embedding_dim, hidden_dim, num_layers, dropout, word_dropout=word_dropout).to(device)\n",
    "model.load_state_dict(torch.load(\"./lstm_language_model.pt\", map_location=torch.device(device)))\n",
    "\n",
    "# generate some sentences\n",
    "model.eval()\n",
    "for i in range(10):\n",
    "    hidden = model.init_hidden(1, device)\n",
    "    generated = model.generate(torch.tensor([word_dict.word_to_id[\"<bos>\"]]).to(device), hidden, 1, max_len, device)\n",
    "    if generated[0] == word_dict.word_to_id[\"<bos>\"]:\n",
    "        generated = generated[1:]\n",
    "    eos_flag = False\n",
    "    if generated[-1] == word_dict.word_to_id[\"<eos>\"]:\n",
    "        generated = generated[:-1]\n",
    "        eos_flag = True\n",
    "    if eos_flag:\n",
    "        print(\"Generated sentence: \\n {}\".format(\" \".join([word_dict.id_to_word[word_id] for word_id in generated])))\n",
    "    else:\n",
    "        print(\"Generated sentence without <eos> token: \\n {}\".format(\" \".join([word_dict.id_to_word[word_id] for word_id in generated])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
