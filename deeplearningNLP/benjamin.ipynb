{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from nltk.data import find\n",
    "import gensim\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "word2vec_sample = str(find('models/word2vec_sample/pruned.word2vec.txt'))\n",
    "word2vec = gensim.models.KeyedVectors.load_word2vec_format(word2vec_sample, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Very disappointed</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A Nutshell Review : Jack Reacher</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>something I want to say about the movie</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>An Awesome Action Film !!!</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FOUR STAR - Sophisticated romantic comedy lock...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599995</th>\n",
       "      <td>Road to your dreams</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599996</th>\n",
       "      <td>Love Rutger Hauer but WHY</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599997</th>\n",
       "      <td>Amazing film with a unique vision , and very f...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599998</th>\n",
       "      <td>Genuinely awful , surprised I watched it to th...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599999</th>\n",
       "      <td>absolute drivel . read this !</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>600000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   review  sentiment\n",
       "0                                       Very disappointed          0\n",
       "1                        A Nutshell Review : Jack Reacher          1\n",
       "2                 something I want to say about the movie          1\n",
       "3                              An Awesome Action Film !!!          1\n",
       "4       FOUR STAR - Sophisticated romantic comedy lock...          1\n",
       "...                                                   ...        ...\n",
       "599995                                Road to your dreams          1\n",
       "599996                          Love Rutger Hauer but WHY          0\n",
       "599997  Amazing film with a unique vision , and very f...          1\n",
       "599998  Genuinely awful , surprised I watched it to th...          0\n",
       "599999                      absolute drivel . read this !          0\n",
       "\n",
       "[600000 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"imdb.pos\") as f:\n",
    "    pos = f.read().splitlines()\n",
    "\n",
    "with open(\"imdb.neg\") as f:\n",
    "    neg = f.read().splitlines()\n",
    "\n",
    "df_pos = pd.DataFrame(pos, columns=[\"review\"])\n",
    "df_pos[\"sentiment\"] = 1\n",
    "df_neg = pd.DataFrame(neg, columns=[\"review\"])\n",
    "df_neg[\"sentiment\"] = 0\n",
    "df = pd.concat([df_pos, df_neg], ignore_index=True)\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_sentence_length = df[\"review\"].apply(lambda x: len(x.split(\" \"))).max()\n",
    "max_sentence_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>seq_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Very disappointed</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A Nutshell Review : Jack Reacher</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>something I want to say about the movie</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>An Awesome Action Film !!!</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FOUR STAR - Sophisticated romantic comedy lock...</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599995</th>\n",
       "      <td>Road to your dreams</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599996</th>\n",
       "      <td>Love Rutger Hauer but WHY</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599997</th>\n",
       "      <td>Amazing film with a unique vision , and very f...</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599998</th>\n",
       "      <td>Genuinely awful , surprised I watched it to th...</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599999</th>\n",
       "      <td>absolute drivel . read this !</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>600000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   review  sentiment  seq_len\n",
       "0                                       Very disappointed          0        2\n",
       "1                        A Nutshell Review : Jack Reacher          1        6\n",
       "2                 something I want to say about the movie          1        8\n",
       "3                              An Awesome Action Film !!!          1        5\n",
       "4       FOUR STAR - Sophisticated romantic comedy lock...          1       20\n",
       "...                                                   ...        ...      ...\n",
       "599995                                Road to your dreams          1        4\n",
       "599996                          Love Rutger Hauer but WHY          0        5\n",
       "599997  Amazing film with a unique vision , and very f...          1       10\n",
       "599998  Genuinely awful , surprised I watched it to th...          0       10\n",
       "599999                      absolute drivel . read this !          0        6\n",
       "\n",
       "[600000 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"seq_len\"] = df[\"review\"].apply(lambda x: len(x.split(\" \")))\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_sentence(sentence, pad_to):\n",
    "    words = sentence.split(\" \")\n",
    "    X = np.zeros((pad_to, 300))\n",
    "    for i in range(min(pad_to, len(words))):\n",
    "        if words[i] in word2vec:\n",
    "            X[i] = word2vec[words[i]]\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_batch(txt_batch, pad_to):\n",
    "    X_batch = np.zeros((len(txt_batch), pad_to, 300))\n",
    "    for i in range(len(txt_batch)):\n",
    "        X_batch[i] = embed_sentence(txt_batch[i], pad_to)\n",
    "    return X_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df[[\"review\", \"seq_len\"]], df[\"sentiment\"], test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMforSentimentAnalysis(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, batch_size, device):\n",
    "        super(LSTMforSentimentAnalysis, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.batch_size = batch_size\n",
    "        self.device = device\n",
    "        self.to(device)\n",
    "\n",
    "    def forward(self, X): # X is a batch of not embedded sentences \n",
    "        # we embed the batch and pass it to the LSTM\n",
    "        sentences = X[\"review\"].values\n",
    "        seq_len = X[\"seq_len\"].values\n",
    "        batch_size = len(X)\n",
    "        X = embed_batch(sentences, max_sentence_length)\n",
    "        X = torch.from_numpy(X).float().to(self.device)\n",
    "        hidden = torch.zeros(1, batch_size, self.hidden_dim).to(self.device)\n",
    "        cell = torch.zeros(1, batch_size, self.hidden_dim).to(self.device)\n",
    "        out, (hidden, cell) = self.lstm(X, (hidden, cell)) # out is of shape (batch_size, seq_len, hidden_dim)\n",
    "\n",
    "        # now in out we have the hidden state for each word in the sentence\n",
    "        # we need to construct the batch of hidden states to pass to the fc layer\n",
    "        # we take the last hidden state for each sentence BEFORE the padding starts\n",
    "        # thanks to the seq_len column we added to the dataframe\n",
    "\n",
    "        hidden_batch = out[torch.arange(batch_size), seq_len-1, :].to(self.device) # of shape (batch_size, hidden_dim)\n",
    "        # we pass the hidden state of the last word of each sentence to the fc layer\n",
    "        out = self.fc(hidden_batch)\n",
    "        return out\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 0/1875, loss: 0.69400\n",
      "Batch: 125/1875, loss: 0.50888\n",
      "Batch: 250/1875, loss: 0.44191\n",
      "Batch: 375/1875, loss: 0.50317\n",
      "Batch: 500/1875, loss: 0.46554\n",
      "Batch: 625/1875, loss: 0.48057\n",
      "Batch: 750/1875, loss: 0.46758\n",
      "Batch: 875/1875, loss: 0.43477\n",
      "Batch: 1000/1875, loss: 0.49115\n",
      "Batch: 1125/1875, loss: 0.44191\n",
      "Batch: 1250/1875, loss: 0.48562\n",
      "Batch: 1375/1875, loss: 0.46292\n",
      "Batch: 1500/1875, loss: 0.47939\n",
      "Batch: 1625/1875, loss: 0.46111\n",
      "Batch: 1750/1875, loss: 0.40438\n",
      "Evaluating...\n",
      "Epoch: 1/10, avg loss: 3.48113, test accuracy: 0.77406\n",
      "Batch: 0/1875, loss: 0.42574\n",
      "Batch: 125/1875, loss: 0.39063\n",
      "Batch: 250/1875, loss: 0.42462\n",
      "Batch: 375/1875, loss: 0.47628\n",
      "Batch: 500/1875, loss: 0.42045\n",
      "Batch: 625/1875, loss: 0.45970\n",
      "Batch: 750/1875, loss: 0.42022\n",
      "Batch: 875/1875, loss: 0.40574\n",
      "Batch: 1000/1875, loss: 0.45170\n",
      "Batch: 1125/1875, loss: 0.40954\n",
      "Batch: 1250/1875, loss: 0.45902\n",
      "Batch: 1375/1875, loss: 0.43964\n",
      "Batch: 1500/1875, loss: 0.45340\n",
      "Batch: 1625/1875, loss: 0.43294\n",
      "Batch: 1750/1875, loss: 0.37136\n",
      "Evaluating...\n",
      "Epoch: 2/10, avg loss: 3.16360, test accuracy: 0.78563\n",
      "Batch: 0/1875, loss: 0.39918\n",
      "Batch: 125/1875, loss: 0.36777\n",
      "Batch: 250/1875, loss: 0.41284\n",
      "Batch: 375/1875, loss: 0.46314\n",
      "Batch: 500/1875, loss: 0.39453\n",
      "Batch: 625/1875, loss: 0.44887\n",
      "Batch: 750/1875, loss: 0.40441\n",
      "Batch: 875/1875, loss: 0.39265\n",
      "Batch: 1000/1875, loss: 0.44133\n",
      "Batch: 1125/1875, loss: 0.39430\n",
      "Batch: 1250/1875, loss: 0.44764\n",
      "Batch: 1375/1875, loss: 0.42857\n",
      "Batch: 1500/1875, loss: 0.44078\n",
      "Batch: 1625/1875, loss: 0.40912\n",
      "Batch: 1750/1875, loss: 0.36314\n",
      "Evaluating...\n",
      "Epoch: 3/10, avg loss: 3.03744, test accuracy: 0.79310\n",
      "Batch: 0/1875, loss: 0.38394\n",
      "Batch: 125/1875, loss: 0.35815\n",
      "Batch: 250/1875, loss: 0.40291\n",
      "Batch: 375/1875, loss: 0.45022\n",
      "Batch: 500/1875, loss: 0.37780\n",
      "Batch: 625/1875, loss: 0.43798\n",
      "Batch: 750/1875, loss: 0.39634\n",
      "Batch: 875/1875, loss: 0.37923\n",
      "Batch: 1000/1875, loss: 0.43477\n",
      "Batch: 1125/1875, loss: 0.37791\n",
      "Batch: 1250/1875, loss: 0.44034\n",
      "Batch: 1375/1875, loss: 0.42025\n",
      "Batch: 1500/1875, loss: 0.42877\n",
      "Batch: 1625/1875, loss: 0.39181\n",
      "Batch: 1750/1875, loss: 0.35998\n",
      "Evaluating...\n",
      "Epoch: 4/10, avg loss: 2.95169, test accuracy: 0.79762\n",
      "Batch: 0/1875, loss: 0.37310\n",
      "Batch: 125/1875, loss: 0.34933\n",
      "Batch: 250/1875, loss: 0.39476\n",
      "Batch: 375/1875, loss: 0.43998\n",
      "Batch: 500/1875, loss: 0.36465\n",
      "Batch: 625/1875, loss: 0.42897\n",
      "Batch: 750/1875, loss: 0.39262\n",
      "Batch: 875/1875, loss: 0.37059\n",
      "Batch: 1000/1875, loss: 0.42481\n",
      "Batch: 1125/1875, loss: 0.36771\n",
      "Batch: 1250/1875, loss: 0.43145\n",
      "Batch: 1375/1875, loss: 0.41222\n",
      "Batch: 1500/1875, loss: 0.41682\n",
      "Batch: 1625/1875, loss: 0.38036\n",
      "Batch: 1750/1875, loss: 0.35570\n",
      "Evaluating...\n",
      "Epoch: 5/10, avg loss: 2.88508, test accuracy: 0.80025\n",
      "Batch: 0/1875, loss: 0.36466\n",
      "Batch: 125/1875, loss: 0.34127\n",
      "Batch: 250/1875, loss: 0.38449\n",
      "Batch: 375/1875, loss: 0.43198\n",
      "Batch: 500/1875, loss: 0.35616\n",
      "Batch: 625/1875, loss: 0.42145\n",
      "Batch: 750/1875, loss: 0.38996\n",
      "Batch: 875/1875, loss: 0.36494\n",
      "Batch: 1000/1875, loss: 0.41466\n",
      "Batch: 1125/1875, loss: 0.36010\n",
      "Batch: 1250/1875, loss: 0.42341\n",
      "Batch: 1375/1875, loss: 0.40547\n",
      "Batch: 1500/1875, loss: 0.40767\n",
      "Batch: 1625/1875, loss: 0.37126\n",
      "Batch: 1750/1875, loss: 0.34965\n",
      "Evaluating...\n",
      "Epoch: 6/10, avg loss: 2.82887, test accuracy: 0.80270\n",
      "Batch: 0/1875, loss: 0.35708\n",
      "Batch: 125/1875, loss: 0.33364\n",
      "Batch: 250/1875, loss: 0.37442\n",
      "Batch: 375/1875, loss: 0.42451\n",
      "Batch: 500/1875, loss: 0.35102\n",
      "Batch: 625/1875, loss: 0.41653\n",
      "Batch: 750/1875, loss: 0.38689\n",
      "Batch: 875/1875, loss: 0.36014\n",
      "Batch: 1000/1875, loss: 0.40577\n",
      "Batch: 1125/1875, loss: 0.35327\n",
      "Batch: 1250/1875, loss: 0.41779\n",
      "Batch: 1375/1875, loss: 0.39981\n",
      "Batch: 1500/1875, loss: 0.40039\n",
      "Batch: 1625/1875, loss: 0.36318\n",
      "Batch: 1750/1875, loss: 0.34345\n",
      "Evaluating...\n",
      "Epoch: 7/10, avg loss: 2.77945, test accuracy: 0.80440\n",
      "Batch: 0/1875, loss: 0.34989\n",
      "Batch: 125/1875, loss: 0.32749\n",
      "Batch: 250/1875, loss: 0.36684\n",
      "Batch: 375/1875, loss: 0.41802\n",
      "Batch: 500/1875, loss: 0.34696\n",
      "Batch: 625/1875, loss: 0.41292\n",
      "Batch: 750/1875, loss: 0.38283\n",
      "Batch: 875/1875, loss: 0.35557\n",
      "Batch: 1000/1875, loss: 0.39842\n",
      "Batch: 1125/1875, loss: 0.34709\n",
      "Batch: 1250/1875, loss: 0.41383\n",
      "Batch: 1375/1875, loss: 0.39503\n",
      "Batch: 1500/1875, loss: 0.39349\n",
      "Batch: 1625/1875, loss: 0.35598\n",
      "Batch: 1750/1875, loss: 0.33829\n",
      "Evaluating...\n",
      "Epoch: 8/10, avg loss: 2.73481, test accuracy: 0.80540\n",
      "Batch: 0/1875, loss: 0.34228\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/Nim/M2/nlp/imdb/lstm_sentiment.ipynb Cell 9\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/Nim/M2/nlp/imdb/lstm_sentiment.ipynb#X12sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/Nim/M2/nlp/imdb/lstm_sentiment.ipynb#X12sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/Nim/M2/nlp/imdb/lstm_sentiment.ipynb#X12sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m avg_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39;49mitem()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/Nim/M2/nlp/imdb/lstm_sentiment.ipynb#X12sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39mif\u001b[39;00m i \u001b[39m%\u001b[39m \u001b[39m500\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/Nim/M2/nlp/imdb/lstm_sentiment.ipynb#X12sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mBatch: \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m, loss: \u001b[39m\u001b[39m%.5f\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (i\u001b[39m/\u001b[39m\u001b[39m/\u001b[39mbatch_size, \u001b[39mlen\u001b[39m(X_train)\u001b[39m/\u001b[39m\u001b[39m/\u001b[39mbatch_size, loss\u001b[39m.\u001b[39mitem()))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_epochs = 10\n",
    "batch_size = 256\n",
    "\n",
    "model = LSTMforSentimentAnalysis(300, 100, 1, batch_size, \"cuda\")\n",
    "\n",
    "# training\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    avg_loss = 0\n",
    "    model.train()\n",
    "    for i in range(0, len(X_train), batch_size):\n",
    "        batch = X_train[i:i+batch_size]\n",
    "        y_batch = y_train[i:i+batch_size]\n",
    "        y_batch = torch.from_numpy(y_batch.values).to(\"cuda\")\n",
    "        optimizer.zero_grad()\n",
    "        output = model(batch)\n",
    "        loss = criterion(output.squeeze(), y_batch.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        avg_loss += loss.item()\n",
    "        if i % 500 == 0:\n",
    "            print(\"Batch: %d/%d, loss: %.5f\" % (i//batch_size, len(X_train)//batch_size, loss.item()))\n",
    "    avg_loss /= batch_size\n",
    "\n",
    "    # evaluation\n",
    "    print(\"Evaluating...\")\n",
    "    model.eval()\n",
    "    y_pred = []\n",
    "\n",
    "    for i in range(0, len(X_test), batch_size):\n",
    "        batch = X_test[i:i+batch_size]\n",
    "        output = model(batch)\n",
    "        y_pred.extend(output.squeeze().detach().cpu().numpy().tolist())\n",
    "\n",
    "    y_pred = np.array(y_pred)\n",
    "    y_pred = F.sigmoid(torch.from_numpy(y_pred)).numpy()\n",
    "    y_pred = np.round(y_pred)\n",
    "    accuracy = np.mean(y_pred == y_test.values)\n",
    "    print(\"Epoch: %d/%d, avg loss: %.5f, test accuracy: %.5f\" % (epoch + 1, n_epochs, avg_loss, accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"./lstm-sentiment.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def manual_test(sentence):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        length = len(sentence.split(\" \"))\n",
    "        df = pd.DataFrame(data={\"review\": [sentence], \"seq_len\":[length]})\n",
    "        return torch.sigmoid(model(df).to(\"cpu\")).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_test(\"Whoaaaa, that movie was sooooo greaaaat\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
