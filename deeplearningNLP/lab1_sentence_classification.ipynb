{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import torch as th\n",
    "import torch.autograd as ag\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning for NLP - lab exercise 1\n",
    "\n",
    "In this first lab exercise we will implement a simple bag-of-word\n",
    "classifier, i.e. a classifier that ignores the sequential structure of\n",
    "the sentence, and a classifier based on a convolutional neural network\n",
    "(CNN). The goal is to predict if a sentence is a positive or negative\n",
    "review of a movie. We will use a dataset constructed from IMDB.\n",
    "\n",
    "1.  Load and clean the data\n",
    "2.  Preprocess the data for the NN\n",
    "3.  Module definition\n",
    "4.  Train the network!\n",
    "\n",
    "We will implement this model with Pytorch, the most popular deep\n",
    "learning framework for Natural Language Processing. You can use the\n",
    "following links for help:\n",
    "\n",
    "-   turorials: <http://pytorch.org/tutorials/>\n",
    "-   documentation: <http://pytorch.org/docs/master/>\n",
    "\n",
    "## Data\n",
    "\n",
    "The data can be download here: <http://caio-corro.fr/dl4nlp/imdb.zip>\n",
    "\n",
    "There are two files: one with positive reviews (imdb.pos) and one with\n",
    "negative reviews (imdb.neg). Each file contains 300000 reviews, one per\n",
    "line.\n",
    "\n",
    "The following functions can be used to load and clean the data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize a sentence\n",
    "def clean_str(string, tolower=True):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning.\n",
    "    Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = re.sub(r\",\", \" , \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \" \\( \", string)\n",
    "    string = re.sub(r\"\\)\", \" \\) \", string)\n",
    "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    if tolower:\n",
    "        string = string.lower()\n",
    "    return string.strip()\n",
    "\n",
    "\n",
    "# reads the content of the file passed as an argument.\n",
    "# if limit > 0, this function will return only the first \"limit\" sentences in the file.\n",
    "def loadTexts(filename, limit=-1):\n",
    "    dataset=[]\n",
    "    with open(filename) as f:\n",
    "        line = f.readline()\n",
    "        cpt=1\n",
    "        skip=0\n",
    "        while line :\n",
    "            cleanline = clean_str(f.readline()).split()\n",
    "            if cleanline: \n",
    "                dataset.append(cleanline)\n",
    "            else: \n",
    "                line = f.readline()\n",
    "                skip+=1\n",
    "                continue\n",
    "            if limit > 0 and cpt >= limit: \n",
    "                break\n",
    "            line = f.readline()\n",
    "            cpt+=1        \n",
    "\n",
    "        print(\"Load \", cpt, \" lines from \", filename , \" / \", skip ,\" lines discarded\")\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell load the first 5000 sentences in each review set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load  5000  lines from  ./imdb/imdb.pos  /  1  lines discarded\n",
      "Load  5000  lines from  ./imdb/imdb.neg  /  1  lines discarded\n"
     ]
    }
   ],
   "source": [
    "LIM = 5000\n",
    "txtfile = \"./imdb/imdb.pos\"\n",
    "postxt = loadTexts(txtfile,limit=LIM)\n",
    "\n",
    "txtfile = \"./imdb/imdb.neg\"\n",
    "negtxt = loadTexts(txtfile,limit=LIM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data between train / dev / test, for example by creating lists\n",
    "txt_train, label_train, txt_dev, ... You should take care to keep a\n",
    "50/50 ratio between positive and negative instances in each set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train / dev / test\n",
    "train_pos_indices = np.random.choice(len(postxt), size=int(0.6*LIM), replace=False)\n",
    "# create dev excluding train\n",
    "dev_pos_indices = np.random.choice(list(set(range(len(postxt))) - set(train_pos_indices)), size=int(0.2*LIM), replace=False)\n",
    "# create test excluding train and dev\n",
    "test_pos_indices = list(set(range(len(postxt))) - set(train_pos_indices) - set(dev_pos_indices))\n",
    "\n",
    "train_neg_indices = np.random.choice(len(negtxt), size=int(0.6*LIM), replace=False)\n",
    "# create dev excluding train\n",
    "dev_neg_indices = np.random.choice(list(set(range(len(negtxt))) - set(train_neg_indices)), size=int(0.2*LIM), replace=False)\n",
    "# create test excluding train and dev\n",
    "test_neg_indices = list(set(range(len(negtxt))) - set(train_neg_indices) - set(dev_neg_indices))\n",
    "\n",
    "train_pos = [postxt[i] for i in train_pos_indices]\n",
    "dev_pos = [postxt[i] for i in dev_pos_indices]\n",
    "test_pos = [postxt[i] for i in test_pos_indices]\n",
    "\n",
    "train_neg = [negtxt[i] for i in train_neg_indices]\n",
    "dev_neg = [negtxt[i] for i in dev_neg_indices]\n",
    "test_neg = [negtxt[i] for i in test_neg_indices]\n",
    "\n",
    "# create train / dev / test sets\n",
    "train = [(x,1) for x in train_pos] + [(x,0) for x in train_neg]\n",
    "dev = [(x,1) for x in dev_pos] + [(x,0) for x in dev_neg]\n",
    "test = [(x,1) for x in test_pos] + [(x,0) for x in test_neg]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting data to Pytorch tensors\n",
    "\n",
    "We will first convert data to Pytorch tensors so they can be used in a\n",
    "neural network. To do that, you must first create a dictionnary that\n",
    "will map words to integers. Add to the dictionnary only words that are\n",
    "in the training set (be sure to understand why we do that!).\n",
    "\n",
    "Then, you can convert the data to tensors:\n",
    "\n",
    "-   use tensors of longs: both the sentence and the label will be\n",
    "    represented as integers, not floats!\n",
    "-   these tensors do not require a gradient\n",
    "\n",
    "A tensor representing a sentence is composed of the integer\n",
    "representation of each word, e.g. \\[10, 256, 3, 4\\]. Note that some\n",
    "words in the dev and test sets may not be in the dictionnary! (i.e.\n",
    "unknown words) You can just skip them, even if this is a bad idea in\n",
    "general."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a dictionary of all words in the training set\n",
    "word_dict = {}\n",
    "for sent, _ in train:\n",
    "    for word in sent:\n",
    "        if word not in word_dict:\n",
    "            word_dict[word] = len(word_dict)\n",
    "\n",
    "def sent2tensor(sent, word_dict):\n",
    "    # convert sentence to list of indices, if a word is not in the dictionary, skip it\n",
    "    idxs = [word_dict[word] if word in word_dict else -1 for word in sent]\n",
    "    # remove words not in dictionary\n",
    "    idxs = [idx for idx in idxs if idx >= 0]\n",
    "    if idxs == []:\n",
    "        return None\n",
    "    return th.LongTensor(idxs)\n",
    "\n",
    "train_data = [(sent2tensor(sent, word_dict), label) for sent, label in train]\n",
    "dev_data = [(sent2tensor(sent, word_dict), label) for sent, label in dev]\n",
    "test_data = [(sent2tensor(sent, word_dict), label) for sent, label in test]\n",
    "\n",
    "# remove empty sentences\n",
    "train_data = [x for x in train_data if x[0] is not None]\n",
    "dev_data = [x for x in dev_data if x[0] is not None]\n",
    "test_data = [x for x in test_data if x[0] is not None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural network definition\n",
    "\n",
    "You need to implement two networks:\n",
    "\n",
    "-   a simple bag of word model (note: it may be better to take the mean\n",
    "    of input embeddings that the sum)\n",
    "-   a simple CNN as described in the course\n",
    "\n",
    "To simplify code, you can assume the input will always be a single\n",
    "sentence first, and then implement batched inputs. In the case of\n",
    "batched inputs, give to the forward function a (python) list of tensors.\n",
    "\n",
    "The bag of word neural network should be defined as follows:\n",
    "\n",
    "-   take as input a tensor that is a sequence of integers indexing word\n",
    "    embeddings\n",
    "-   retrieve the word embeddings from an embedding table\n",
    "-   construct the \"input\" of the MLP by summing (or computing the mean)\n",
    "    over all embeddingsÂ (i.e. bag-of-word model)\n",
    "-   build a hidden represention using a MLP (1 layer? 2 layers?\n",
    "    experiment! but maybe first try wihout any hidden layer...)\n",
    "-   project the hidden representation to the output space: it is a\n",
    "    binary classification task, so the output space is a scalar where a\n",
    "    negative (resp. positive) value means the review is negative (resp.\n",
    "    positive).\n",
    "\n",
    "The CNN is a little bit more tricky to implement. The goal is that you\n",
    "implement the one presented in the first lecture. Importantly, you\n",
    "should add \"padding\" tokens before and after the sentence so you can\n",
    "have a convolution even when there is a single word in the input. For\n",
    "example, if you input sentence is \\[\"word\"\\], you want to instead\n",
    "consider the sentence \\[\"\\<BOS>\", \"word\", \"\\<EOS>\"\\] if your window is\n",
    "of size 2 or 3. You can do this either directly when you load the data,\n",
    "or you can do that in the neural network module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First: Let's do the linear classifier!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BAG of word classifier\n",
    "class CBOW_classifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, linear_dim):\n",
    "        super(CBOW_classifier, self).__init__()\n",
    "        # create embedding table\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        # create linear layer\n",
    "        if type(linear_dim) == int:\n",
    "            self.linear == nn.Sequential(nn.Linear(embedding_dim, linear_dim), nn.ReLU(), nn.Linear(linear_dim, 1))\n",
    "        elif type(linear_dim) in [list, tuple]:\n",
    "            layers = [nn.Linear(embedding_dim, linear_dim[0]), nn.ReLU()]\n",
    "            for i in range(len(linear_dim)-1):\n",
    "                layers.append(nn.Linear(linear_dim[i], linear_dim[i+1]))\n",
    "                layers.append(nn.ReLU())\n",
    "            layers.append(nn.Linear(linear_dim[-1], 1))\n",
    "            self.linear = nn.Sequential(*layers)\n",
    "        else:\n",
    "            raise ValueError(\"linear_dim must be an int, list or tuple\")\n",
    "        \n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        # get embeddings\n",
    "        embeds = self.embedding(inputs)\n",
    "        # sum embeddings and average\n",
    "        embeds = th.sum(embeds, dim=0) / embeds.shape[0]\n",
    "        # linear layer\n",
    "        out = self.linear(embeds)\n",
    "        # sigmoid\n",
    "        out = F.sigmoid(out)\n",
    "        return out\n",
    "    \n",
    "    def get_embeddings(self, inputs):\n",
    "        # get embeddings\n",
    "        embeds = self.embedding(inputs)\n",
    "        # sum embeddings and average\n",
    "        embeds = th.sum(embeds, dim=0) / embeds.shape[0]\n",
    "        return embeds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss function\n",
    "\n",
    "Create a loss function builder.\n",
    "\n",
    "-   Pytorch loss functions are documented here:\n",
    "    <https://pytorch.org/docs/stable/nn.html#loss-functions>\n",
    "-   In our case, we are interested in *BCELoss* and *BCEWithLogitsLoss*.\n",
    "    Read their documentation and choose the one that fits with your\n",
    "    network output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define loss function\n",
    "loss_fn = nn.BCELoss()\n",
    "\n",
    "embedding_size = 100\n",
    "linear_size = (100,50)\n",
    "# define model\n",
    "model = CBOW_classifier(len(word_dict), embedding_size, linear_size)\n",
    "\n",
    "# define optimizer\n",
    "optim = th.optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop\n",
    "\n",
    "Write your training loop!\n",
    "\n",
    "-   parameterizable number of epochs\n",
    "-   at each epoch, print the mean loss and the dev accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Train Acc: 0.687, Dev Acc: 0.7598784194528876\n",
      "Epoch: 1, Train Acc: 0.8143333333333334, Dev Acc: 0.7776089159067883\n",
      "Epoch: 2, Train Acc: 0.871, Dev Acc: 0.7877406281661601\n",
      "Epoch: 3, Train Acc: 0.9138333333333334, Dev Acc: 0.7897669706180345\n",
      "Epoch: 4, Train Acc: 0.9476666666666667, Dev Acc: 0.786727456940223\n",
      "Epoch: 5, Train Acc: 0.9661666666666666, Dev Acc: 0.7806484295845998\n",
      "Epoch: 6, Train Acc: 0.9791666666666666, Dev Acc: 0.7700101317122594\n",
      "Epoch: 7, Train Acc: 0.9851666666666666, Dev Acc: 0.7771023302938197\n",
      "Epoch: 8, Train Acc: 0.989, Dev Acc: 0.7695035460992907\n",
      "Epoch: 9, Train Acc: 0.9905, Dev Acc: 0.7771023302938197\n"
     ]
    }
   ],
   "source": [
    "# training loop\n",
    "for epoch in range(10):\n",
    "    # shuffle training data\n",
    "    np.random.shuffle(train_data)\n",
    "    # set model to train mode\n",
    "    model.train()\n",
    "\n",
    "    #compute train accuracy\n",
    "    correct = 0\n",
    "\n",
    "    # loop over training data\n",
    "    for sent, label in train_data:\n",
    "        # zero gradients\n",
    "        optim.zero_grad()\n",
    "        # forward pass\n",
    "        out = model(sent)\n",
    "        # compute loss\n",
    "        loss = loss_fn(out, th.FloatTensor([label]))\n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "        # update parameters\n",
    "        optim.step()\n",
    "\n",
    "        #compute train accuracy\n",
    "        pred = 1 if out > 0.5 else 0\n",
    "        if pred == label:\n",
    "            correct += 1\n",
    "    # compute accuracy\n",
    "    train_acc = correct / len(train_data)\n",
    "    \n",
    "\n",
    "\n",
    "    # set model to eval mode\n",
    "    model.eval()\n",
    "    # compute accuracy on dev set\n",
    "    correct = 0\n",
    "    for sent, label in dev_data:\n",
    "        # forward pass\n",
    "        out = model(sent)\n",
    "        # get prediction\n",
    "        pred = 1 if out > 0.5 else 0\n",
    "        # check if prediction is correct\n",
    "        if pred == label:\n",
    "            correct += 1\n",
    "    # compute accuracy\n",
    "    acc = correct / len(dev_data)\n",
    "    print(\"Epoch: {}, Train Acc: {}, Dev Acc: {}\".format(epoch, train_acc, acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute distance between two sentences in the embedding space of model\n",
    "def compute_distance(sent1, sent2, model, word_dict):\n",
    "    # get embeddings\n",
    "    embeds1 = model.get_embeddings(sent2tensor(sent1, word_dict))\n",
    "    embeds2 = model.get_embeddings(sent2tensor(sent2, word_dict))\n",
    "    # compute distance\n",
    "    return th.dist(embeds1, embeds2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance between good and bad: 16.094451904296875\n",
      "Distance between bad and terrible: 15.493986129760742\n",
      "Distance between good and amazing: 14.684762954711914\n",
      "Distance between 'this movie has been amazing' and movie: 9.262959480285645\n",
      "Distance between '['this', 'movie', 'makes', 'me', 'hate']' and '['this', 'movie', 'is', 'literally', 'the', 'best', 'in', 'the', 'world']': 4.809334754943848\n"
     ]
    }
   ],
   "source": [
    "good_bad = compute_distance([\"good\"], [\"bad\"], model, word_dict)\n",
    "bad_terrible = compute_distance([\"bad\"], [\"terrible\"], model, word_dict)\n",
    "good_amazing = compute_distance([\"good\"], [\"amazing\"], model, word_dict)\n",
    "sentence_movie = compute_distance([\"this\", \"movie\", \"has\", 'been', \"amazing\"], [\"movie\"], model, word_dict)\n",
    "print(\"Distance between good and bad: {}\".format(good_bad))\n",
    "print(\"Distance between bad and terrible: {}\".format(bad_terrible))\n",
    "print(\"Distance between good and amazing: {}\".format(good_amazing))\n",
    "print(\"Distance between 'this movie has been amazing' and movie: {}\".format(sentence_movie))\n",
    "sentence_1 = \"this movie makes me hate\".split()\n",
    "sentence_2 = \"this movie is literally the best in the world\".split()\n",
    "sent1_sent2 = (compute_distance(sentence_1, sentence_2, model, word_dict))\n",
    "print(\"Distance between '{}' and '{}': {}\".format(sentence_1, sentence_2, sent1_sent2))\n",
    "\n",
    "linear_embedder_model = model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that the embeddings we have learnt are not clustered appropriately in the feature space; synonyms are at similar distances as antonyms. But that is ok, our goal has been to train a linear classifier using word embeddings - which we have reasonable achieved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next up: CNNs for the same task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a convolutional bag of words classifier\n",
    "class ConvCBOX(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, kernel_size, out_channels, linear_dim):\n",
    "        super(ConvCBOX, self).__init__()\n",
    "        # create embedding table\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        # create convolutional layer\n",
    "        self.conv = nn.Conv1d(embedding_dim, out_channels, kernel_size, padding = 'same')\n",
    "        # create linear layer\n",
    "        if type(linear_dim) == int:\n",
    "            self.linear = nn.Sequential(nn.Linear(out_channels, linear_dim), nn.ReLU(), nn.Linear(linear_dim, 1))\n",
    "        elif type(linear_dim) in [list, tuple]:\n",
    "            layers = [nn.Linear(out_channels, linear_dim[0]), nn.ReLU()]\n",
    "            for i in range(len(linear_dim)-1):\n",
    "                layers.append(nn.Linear(linear_dim[i], linear_dim[i+1]))\n",
    "                layers.append(nn.ReLU())\n",
    "            layers.append(nn.Linear(linear_dim[-1], 1))\n",
    "            self.linear = nn.Sequential(*layers)\n",
    "        else:\n",
    "            raise ValueError(\"linear_dim must be an int, list or tuple\")\n",
    "        \n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        # get embeddings\n",
    "        embeds = self.embedding(inputs)\n",
    "        # transpose embeddings\n",
    "        embeds = embeds.transpose(1,2)\n",
    "        # convolve\n",
    "        conv_out = self.conv(embeds)\n",
    "        # max pool\n",
    "        pool_out = F.max_pool1d(conv_out, conv_out.shape[2])\n",
    "        # linear layer\n",
    "        out = self.linear(pool_out.squeeze())\n",
    "        # sigmoid\n",
    "        out = F.sigmoid(out)\n",
    "        return out\n",
    "    \n",
    "    def get_embeddings(self, inputs):\n",
    "        # get embeddings\n",
    "        embeds = self.embedding(inputs)\n",
    "        # transpose embeddings\n",
    "        embeds = embeds.transpose(1,2)\n",
    "        # convolve\n",
    "        conv_out = self.conv(embeds)\n",
    "        # max pool\n",
    "        pool_out = F.max_pool1d(conv_out, conv_out.shape[2])\n",
    "        return pool_out.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "embedding_size = 100\n",
    "kernel_size = 3\n",
    "out_channels = 100\n",
    "linear_size = (100,50)\n",
    "model = ConvCBOX(len(word_dict), embedding_size, kernel_size, out_channels, linear_size)\n",
    "\n",
    "# define optimizer\n",
    "optim = th.optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_fn = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Train Acc: 0.6771666666666667, Dev Acc: 0.7467071935157041\n",
      "Epoch: 1, Train Acc: 0.8173333333333334, Dev Acc: 0.7624113475177305\n",
      "Epoch: 2, Train Acc: 0.8965, Dev Acc: 0.770516717325228\n",
      "Epoch: 3, Train Acc: 0.9455, Dev Acc: 0.7755825734549139\n",
      "Epoch: 4, Train Acc: 0.9691666666666666, Dev Acc: 0.7725430597771024\n",
      "Epoch: 5, Train Acc: 0.978, Dev Acc: 0.7831813576494427\n",
      "Epoch: 6, Train Acc: 0.984, Dev Acc: 0.7781155015197568\n",
      "Epoch: 7, Train Acc: 0.9863333333333333, Dev Acc: 0.7720364741641338\n",
      "Epoch: 8, Train Acc: 0.9898333333333333, Dev Acc: 0.756838905775076\n",
      "Epoch: 9, Train Acc: 0.9908333333333333, Dev Acc: 0.7740628166160081\n"
     ]
    }
   ],
   "source": [
    "# training loop\n",
    "for epoch in range(10):\n",
    "    # shuffle training data\n",
    "    np.random.shuffle(train_data)\n",
    "    # set model to train mode\n",
    "    model.train()\n",
    "\n",
    "    #compute train accuracy\n",
    "    correct = 0\n",
    "\n",
    "    # loop over training data\n",
    "    for sent, label in train_data:\n",
    "        # zero gradients\n",
    "        optim.zero_grad()\n",
    "        # forward pass\n",
    "        out = model(sent.unsqueeze(0))\n",
    "        # compute loss\n",
    "        loss = loss_fn(out, th.FloatTensor([label]))\n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "        # update parameters\n",
    "        optim.step()\n",
    "\n",
    "        #compute train accuracy\n",
    "        pred = 1 if out > 0.5 else 0\n",
    "        if pred == label:\n",
    "            correct += 1\n",
    "    # compute accuracy\n",
    "    train_acc = correct / len(train_data)\n",
    "    \n",
    "\n",
    "\n",
    "    # set model to eval mode\n",
    "    model.eval()\n",
    "    # compute accuracy on dev set\n",
    "    correct = 0\n",
    "    for sent, label in dev_data:\n",
    "        # forward pass\n",
    "        out = model(sent.unsqueeze(0))\n",
    "        # get prediction\n",
    "        pred = 1 if out > 0.5 else 0\n",
    "        # check if prediction is correct\n",
    "        if pred == label:\n",
    "            correct += 1\n",
    "    # compute accuracy\n",
    "    acc = correct / len(dev_data)\n",
    "    print(\"Epoch: {}, Train Acc: {}, Dev Acc: {}\".format(epoch, train_acc, acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance between good and bad:  tensor(61.5394)\n",
      "Distance between bad and terrible:  tensor(53.7923)\n",
      "Distance between good and amazing:  tensor(35.2654)\n",
      "Distance between 'this movie has been amazing' and 'movie':  tensor(56.5417)\n"
     ]
    }
   ],
   "source": [
    "#compute distance between two sentences in the embedding space of model\n",
    "def compute_distance(sent1, sent2, model, word_dict):\n",
    "    # get embeddings\n",
    "    embeds1 = model.get_embeddings(sent2tensor(sent1, word_dict).unsqueeze(0))\n",
    "    embeds2 = model.get_embeddings(sent2tensor(sent2, word_dict).unsqueeze(0))\n",
    "    # compute distance\n",
    "    return th.dist(embeds1, embeds2)\n",
    "\n",
    "# compute distances between  various words again\n",
    "with th.no_grad():\n",
    "    good_bad = compute_distance([\"good\"], [\"bad\"], model, word_dict)\n",
    "    bad_terrible = compute_distance([\"bad\"], [\"terrible\"], model, word_dict)\n",
    "    good_amazing = compute_distance([\"good\"], [\"amazing\"], model, word_dict)\n",
    "    sentence_movie = compute_distance([\"this\", \"movie\", \"has\", 'been', \"amazing\"], [\"movie\"], model, word_dict)\n",
    "print(\"Distance between good and bad: \", good_bad)\n",
    "print(\"Distance between bad and terrible: \", bad_terrible)\n",
    "print(\"Distance between good and amazing: \", good_amazing)\n",
    "print(\"Distance between 'this movie has been amazing' and 'movie': \", sentence_movie)\n",
    "\n",
    "conv_embedder_model = model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems the two models have similar embedding spaces, and that their performance is similar too! As a downside, the CNN uses a lot more computational power for training. This might be because we are training an embedder at the same time and computing the backward pass through a convolutional layer, which is in itself very expensive to run. Let us try to use a pretrained embedder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from nltk.data import find\n",
    "\n",
    "word2vec_sample = str(find('models/word2vec_sample/pruned.word2vec.txt'))\n",
    "embedding_model = gensim.models.KeyedVectors.load_word2vec_format(word2vec_sample, binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time, we will build the dataset in the already vectorized space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\deus-diabolus\\AppData\\Local\\Temp\\ipykernel_5932\\3520008484.py:12: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\utils\\tensor_new.cpp:264.)\n",
      "  train_data_test = [(th.FloatTensor([x[0]]), x[1]) for x in train_data_test]\n"
     ]
    }
   ],
   "source": [
    "# create embedding for all datapoints\n",
    "train_data_test = [[[embedding_model[word] for word in sent if word in embedding_model], label] for sent, label in train]\n",
    "dev_data_test = [[[embedding_model[word] for word in sent if word in embedding_model], label] for sent, label in dev]\n",
    "test_data_test = [[[embedding_model[word] for word in sent if word in embedding_model], label] for sent, label in test]\n",
    "\n",
    "# remove empty sentences\n",
    "train_data_test = [x for x in train_data_test if x[0] != []]\n",
    "dev_data_test = [x for x in dev_data_test if x[0] != []]\n",
    "test_data_test = [x for x in test_data_test if x[0] != []]\n",
    "\n",
    "# convert to tensors\n",
    "train_data_test = [(th.FloatTensor([x[0]]), x[1]) for x in train_data_test]\n",
    "dev_data_test = [(th.FloatTensor([x[0]]), x[1]) for x in dev_data_test]\n",
    "test_data_test = [(th.FloatTensor([x[0]]), x[1]) for x in test_data_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To save the lifespan of our mouse wheel, we will rewrite the class in a modular way, to permit various architectures to be developed based on the provided parameters (mainly the use of pretrained embeddings and convolutional layers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new class that is modular; optional embeddings (or use pre-trained embeddings), optional convolutions, linear layers\n",
    "\n",
    "class MCBOW(nn.Module):\n",
    "    def __init__(self, embedding_data, conv_data, linear_data):\n",
    "        super(MCBOW, self).__init__()\n",
    "        if type(embedding_data) == tuple and len(embedding_data) == 2:\n",
    "            # we have embedding_data[0] words, each with embedding_data[1] dimensions\n",
    "            self.embedding = nn.Embedding(embedding_data[0], embedding_data[1])\n",
    "        elif embedding_data == None:\n",
    "            # the input will already be the embeddings\n",
    "            self.embedding = None\n",
    "        else:\n",
    "            raise ValueError(\"embedding_data must be a tuple of 2 ints or None\")\n",
    "\n",
    "        if type(conv_data) == tuple and len(conv_data) == 3:\n",
    "            # we have conv_data[0] input channels, conv_data[1] output channels, conv_data[2] kernel size\n",
    "            self.conv = nn.Conv1d(conv_data[0], conv_data[1], conv_data[2], padding = 'same')\n",
    "        elif conv_data == None:\n",
    "            # no convolutions\n",
    "            self.conv = None\n",
    "        else:\n",
    "            raise ValueError(\"conv_data must be a tuple of 3 ints or None\")\n",
    "        \n",
    "        if type(linear_data) == tuple:\n",
    "            # we have linear_data[0] input channels, and the rest are hidden layers\n",
    "            layers = []\n",
    "            for i in range(len(linear_data)-1):\n",
    "                layers.append(nn.Linear(linear_data[i], linear_data[i+1]))\n",
    "                layers.append(nn.ReLU())\n",
    "            layers.append(nn.Linear(linear_data[-1], 1))\n",
    "            self.linear = nn.Sequential(*layers)\n",
    "        else:\n",
    "            raise ValueError(\"linear_data must be a tuple of 2 ints\")\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        # get embeddings\n",
    "        if self.embedding is not None:\n",
    "            embeds = self.embedding(inputs)\n",
    "        else:\n",
    "            embeds = inputs\n",
    "        if self.conv is not None:\n",
    "            # transpose embeddings\n",
    "            embeds = embeds.transpose(1,2)\n",
    "            # convolve\n",
    "            conv_out = self.conv(embeds)\n",
    "            # max pool\n",
    "            pool_out = F.max_pool1d(conv_out, conv_out.shape[2])\n",
    "            #pool_out = F.avg_pool1d(conv_out, conv_out.shape[2])\n",
    "        else:\n",
    "            # average embeddings\n",
    "            pool_out = th.sum(embeds, dim=1) / embeds.shape[1]\n",
    "        # linear layer\n",
    "        out = self.linear(pool_out.squeeze())\n",
    "        # sigmoid\n",
    "        out = F.sigmoid(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a CNN classifier using word2vec embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model, we use the pre-trained word2vec embeddings\n",
    "embedding_data = None\n",
    "conv_data = (300, 100, 3)\n",
    "linear_data = (100,)\n",
    "model = MCBOW(embedding_data, conv_data, linear_data)\n",
    "\n",
    "# define optimizer\n",
    "optim = th.optim.Adam(model.parameters(), lr=0.0001)\n",
    "loss_fn = nn.BCELoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Train Acc: 0.7483870967741936, Dev Acc: 0.7996934082779765\n",
      "Epoch: 1, Train Acc: 0.8061120543293718, Dev Acc: 0.8083801737353091\n",
      "Epoch: 2, Train Acc: 0.8247877758913412, Dev Acc: 0.8094021461420542\n",
      "Epoch: 3, Train Acc: 0.8415959252971138, Dev Acc: 0.8134900357690342\n",
      "Epoch: 4, Train Acc: 0.8572156196943973, Dev Acc: 0.8109351047521717\n",
      "Epoch: 5, Train Acc: 0.8643463497453311, Dev Acc: 0.8160449667858968\n",
      "Epoch: 6, Train Acc: 0.8764006791171477, Dev Acc: 0.8175779253960143\n",
      "Epoch: 7, Train Acc: 0.8850594227504245, Dev Acc: 0.8175779253960143\n",
      "Epoch: 8, Train Acc: 0.899151103565365, Dev Acc: 0.8170669391926418\n",
      "Epoch: 9, Train Acc: 0.9100169779286927, Dev Acc: 0.8037812979049566\n"
     ]
    }
   ],
   "source": [
    "# training loop\n",
    "for epoch in range(10):\n",
    "    # shuffle training data\n",
    "    np.random.shuffle(train_data_test)\n",
    "    # set model to train mode\n",
    "    model.train()\n",
    "\n",
    "    #compute train accuracy\n",
    "    correct = 0\n",
    "\n",
    "    # loop over training data\n",
    "    for sent, label in train_data_test:\n",
    "        # zero gradients\n",
    "        optim.zero_grad()\n",
    "        # forward pass\n",
    "        out = model(th.FloatTensor(sent))\n",
    "        # compute loss\n",
    "        loss = loss_fn(out, th.FloatTensor([label]))\n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "        # update parameters\n",
    "        optim.step()\n",
    "\n",
    "        #compute train accuracy\n",
    "        pred = 1 if out > 0.5 else 0\n",
    "        if pred == label:\n",
    "            correct += 1\n",
    "    # compute accuracy\n",
    "    train_acc = correct / len(train_data_test)\n",
    "    \n",
    "\n",
    "\n",
    "    # set model to eval mode\n",
    "    model.eval()\n",
    "    # compute accuracy on dev set\n",
    "    correct = 0\n",
    "    for sent, label in dev_data_test:\n",
    "        # forward pass\n",
    "        out = model(th.FloatTensor(sent))\n",
    "        # get prediction\n",
    "        pred = 1 if out > 0.5 else 0\n",
    "        # check if prediction is correct\n",
    "        if pred == label:\n",
    "            correct += 1\n",
    "    # compute accuracy\n",
    "    acc = correct / len(dev_data_test)\n",
    "    print(\"Epoch: {}, Train Acc: {}, Dev Acc: {}\".format(epoch, train_acc, acc))\n",
    "\n",
    "conv_word2vec_model = model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a linear classifier using word2vec embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model, we use the pre-trained word2vec embeddings\n",
    "embedding_data = None\n",
    "conv_data = None\n",
    "linear_data = (300,64,64,)\n",
    "model = MCBOW(embedding_data, conv_data, linear_data)\n",
    "\n",
    "# define optimizer\n",
    "optim = th.optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_fn = nn.BCELoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Train Acc: 0.766383701188455, Dev Acc: 0.7935615738375064\n",
      "Epoch: 1, Train Acc: 0.7918505942275043, Dev Acc: 0.7961165048543689\n",
      "Epoch: 2, Train Acc: 0.8103565365025467, Dev Acc: 0.7853857945835463\n",
      "Epoch: 3, Train Acc: 0.8161290322580645, Dev Acc: 0.7940725600408789\n",
      "Epoch: 4, Train Acc: 0.8224108658743633, Dev Acc: 0.7945835462442514\n",
      "Epoch: 5, Train Acc: 0.8285229202037352, Dev Acc: 0.8002043944813491\n",
      "Epoch: 6, Train Acc: 0.8424448217317487, Dev Acc: 0.7976494634644865\n",
      "Epoch: 7, Train Acc: 0.8541595925297114, Dev Acc: 0.8002043944813491\n",
      "Epoch: 8, Train Acc: 0.86553480475382, Dev Acc: 0.7874297393970363\n",
      "Epoch: 9, Train Acc: 0.8772495755517827, Dev Acc: 0.7828308635666837\n"
     ]
    }
   ],
   "source": [
    "# training loop\n",
    "for epoch in range(10):\n",
    "    # shuffle training data\n",
    "    np.random.shuffle(train_data_test)\n",
    "    # set model to train mode\n",
    "    model.train()\n",
    "\n",
    "    #compute train accuracy\n",
    "    correct = 0\n",
    "\n",
    "    # loop over training data\n",
    "    for sent, label in train_data_test:\n",
    "        # zero gradients\n",
    "        optim.zero_grad()\n",
    "        # forward pass\n",
    "        out = model(th.FloatTensor(sent))\n",
    "        # compute loss\n",
    "        loss = loss_fn(out, th.FloatTensor([label]))\n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "        # update parameters\n",
    "        optim.step()\n",
    "\n",
    "        #compute train accuracy\n",
    "        pred = 1 if out > 0.5 else 0\n",
    "        if pred == label:\n",
    "            correct += 1\n",
    "    # compute accuracy\n",
    "    train_acc = correct / len(train_data_test)\n",
    "    \n",
    "\n",
    "\n",
    "    # set model to eval mode\n",
    "    model.eval()\n",
    "    # compute accuracy on dev set\n",
    "    correct = 0\n",
    "    for sent, label in dev_data_test:\n",
    "        # forward pass\n",
    "        out = model(th.FloatTensor(sent))\n",
    "        # get prediction\n",
    "        pred = 1 if out > 0.5 else 0\n",
    "        # check if prediction is correct\n",
    "        if pred == label:\n",
    "            correct += 1\n",
    "    # compute accuracy\n",
    "    acc = correct / len(dev_data_test)\n",
    "    print(\"Epoch: {}, Train Acc: {}, Dev Acc: {}\".format(epoch, train_acc, acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that in this experiment, the CNN performs better by ~2% compared to a simple linear classifier with aggregated feature spaces. In addition, not building our own embedding space seems to prevent overfitting: we drop from 0.999 training accuracy to a more reasonable ~0.9 training accuracy on the word2vec embedding data models. Let us evaluate the CNN on the test set to make sure the score is similar!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Acc: 0.7978615071283096\n"
     ]
    }
   ],
   "source": [
    "#get final score on the test dataset\n",
    "correct = 0\n",
    "for sent, label in test_data_test:\n",
    "    # forward pass\n",
    "    out = conv_word2vec_model(th.FloatTensor(sent))\n",
    "    # get prediction\n",
    "    pred = 1 if out > 0.5 else 0\n",
    "    # check if prediction is correct\n",
    "    if pred == label:\n",
    "        correct += 1\n",
    "# compute accuracy\n",
    "acc = correct / len(test_data_test)\n",
    "print(\"Test Acc: {}\".format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy for word2vec CNN is reasonably similar for the test and dev sets! The end."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
