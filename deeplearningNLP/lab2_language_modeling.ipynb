{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab exercise: neural language modeling\n",
    "\n",
    "The goal of this lab exercise is build two neural language models:\n",
    "- a neural n-gram model based on a simple MLP\n",
    "- an autoregressive model based on a LSTM\n",
    "\n",
    "Although the n-gram model is straighforward to code, there are a few \"tricks\" that you need to implement for the autoregressive model:\n",
    "- word dropout\n",
    "- variational dropout\n",
    "- loss function masking\n",
    "\n",
    "## Variational dropout\n",
    "\n",
    "The idea of variational dropout is to apply the same mask at each position for a given sentence (if there are several sentences in a minibatch, use different masks for each input).\n",
    "The idea is as follows:\n",
    "- assume a sentence of n words whose embeddings are e_1, e_2, ... e_n\n",
    "- at the input of the LSTM, instead of apply dropout independently to each embedding, sample a single mask that will be applied similarly at each position\n",
    "- same at the output of the LSTM\n",
    "\n",
    "See Figure 1 of this paper: https://proceedings.neurips.cc/paper/2016/file/076a0c97d09cf1a0ec3e19c7f2529f2b-Paper.pdf\n",
    "\n",
    "To implement this, you need to build a custom module that applies the dropout only if the network is in training mode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing\n",
    "\n",
    "You first need to download the Penn Treebank as pre-processed by Tomas Mikolov. It is available here: https://github.com/townie/PTB-dataset-from-Tomas-Mikolov-s-webpage/tree/master/data\n",
    "We will use the following files:\n",
    "- ptb.train.txt\n",
    "- ptb.valid.txt\n",
    "- ptb.test.txt\n",
    "\n",
    "Check manually the data.\n",
    "\n",
    "Todo:\n",
    "- build a word dictionnary, i.e. a bijective mapping between words and integers. You will need to add a special token \"\\<BOS\\>\" to the dictionnary even if it doesn't appear in sentences. (if you want to generate data, you will also need a \"\\<EOS\\>\" token, but this is not a requirement for this lab exercise --- you can do this at the end if you want)\n",
    "- build python list of integers representing each input. For example, for the sentence \"I sleep\", the tensor could look like [10, 5] if 10 is the integer associated with \"I\" and 5 the integer associated with \"sleep\". You can add this directly to the dictionnaries in \\*\\_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(path):\n",
    "    data = list()\n",
    "    with open(path) as inf:\n",
    "        for line in inf:\n",
    "            line = line.strip()\n",
    "            if len(line) == 0:\n",
    "                continue\n",
    "            data.append({\"text\": line.split()})\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = read_file(\"./ptb.train.txt\")\n",
    "dev_data = read_file(\"./ptb.valid.txt\")\n",
    "test_data = read_file(\"./ptb.test.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_data), len(dev_data), len(test_data))\n",
    "print(\"\\n\\n\".join(\" \".join(s[\"text\"]) for s in train_data[:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordDict:\n",
    "    # constructor, words must be a set containing all words\n",
    "    def __init__(self, words):\n",
    "        assert type(words) == set\n",
    "        # TODO\n",
    "            \n",
    "    # return the integer associated with a word\n",
    "    def word_to_id(self, word):\n",
    "        assert type(word) == str\n",
    "        # TODO\n",
    "    \n",
    "    # return the word associated with an integer\n",
    "    def id_to_word(self, idx):\n",
    "        assert type(idx) == int\n",
    "        # TODO\n",
    "    \n",
    "    # number of word in the dictionnary\n",
    "    def __len__(self):\n",
    "        # TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_words = set()\n",
    "for sentence in train_data:\n",
    "    train_words.update(sentence[\"text\"])\n",
    "train_words.update([\"<bos>\", \"<eos>\"])\n",
    "word_dict = WordDict(train_words)\n",
    "len(word_dict)  # should be 10001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "For evaluation, you must compute the perplexity of the test dataset (i.e. assume the dataset is one very long sentence), see:\n",
    "https://lena-voita.github.io/nlp_course/language_modeling.html#evaluation\n",
    "\n",
    "Note that you don't need to explicitly compute the root, you can use log probabilities and properties of log functions for this.\n",
    "As during evaluation, you will see sentences one after the other, you can code a small class to keep track of log probabilities of words and compute the global perplexity at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perplexity:\n",
    "    def __init__(self):\n",
    "        # TODO\n",
    "        \n",
    "    def reset(self):\n",
    "        # TODO\n",
    "        \n",
    "    def add_sentence(self, log_probs):\n",
    "        # log_probs: vector of log probabilities of words in a sentence\n",
    "        # TODO\n",
    "        \n",
    "    def compute_perplexity(self):\n",
    "        # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural n-gram model\n",
    "\n",
    "The model must be similar to the one presented in the course notes.\n",
    "Note that for training and testing, you can transform the data has a set of multiclass classification problems.\n",
    "\n",
    "Todo:\n",
    "1. transform the data into tensors --- note that you can decompose your data to have input tensors of shape 2 and a unique output, why ? You will need to pad the sentence with \\<BOS\\> tokens --- why do you need two before the first word?\n",
    "2. train the network\n",
    "3. compute perplexity of the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM model\n",
    "\n",
    "This model should rely on a LSTM.\n",
    "\n",
    "1. transform the data into tensors => you can't use the same trick as for the n-gram model\n",
    "2. train the network by batching the input --- be very careful when computing the loss function! And explain how to batch data, compute the loss with batch data, etc, in the report!\n",
    "3. compute the perplexity on the test data\n",
    "4. implement variational dropout at input and output of the LSTM\n",
    "\n",
    "Warning: you need to use the option batch_first=True for the LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
