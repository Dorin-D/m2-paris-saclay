{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\deus-\n",
      "[nltk_data]     diabolus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#wordnet in nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score\n",
    "#import classification report\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "nltk.download('wordnet')\n",
    "import numpy as np\n",
    "import gensim\n",
    "from nltk.data import find\n",
    "\n",
    "from torch import nn\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "\n",
    "#random forest\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read imdb/imdb.neg\n",
    "with open('imdb/imdb.neg', 'r') as f:\n",
    "    neg = f.readlines()\n",
    "#read imdb/imdb.pos\n",
    "with open('imdb/imdb.pos', 'r') as f:\n",
    "    pos = f.readlines()\n",
    "\n",
    "#load nltk word2vec_sample\n",
    "#download nltk model\n",
    "# nltk.download('word2vec_sample')\n",
    "word2vec_sample = str(find('models/word2vec_sample/pruned.word2vec.txt'))\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format(word2vec_sample, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# given model and two words, find cosine similarity\n",
    "\n",
    "def cosine_similarity(model, word1, word2):\n",
    "    vec1 = model[word1]\n",
    "    vec2 = model[word2]\n",
    "    similarity = vec1 @ vec2 / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
    "    #similarity = model.similarity(word1, word2)\n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use ngrams to develop a word predictor\n",
    "# given a word, predict the next word\n",
    "\n",
    "def ngram_model(corpus, ngram_size):\n",
    "    # create all the ngrams present in the corpus of size ngram_size\n",
    "    ngrams = {}\n",
    "    for line in corpus:\n",
    "        tokens = line.split()\n",
    "        for i in range(len(tokens) - ngram_size):\n",
    "            ngram = tuple(tokens[i:i+ngram_size])\n",
    "            if ngram not in ngrams:\n",
    "                ngrams[ngram] = 1\n",
    "            else:\n",
    "                ngrams[ngram] += 1\n",
    "    return ngrams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngrams = ngram_model(neg+pos, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # given a word, predict the next word\n",
    "# def predict_next_word(word, ngrams):\n",
    "#     # find all ngrams that start with the given word\n",
    "#     candidates = []\n",
    "#     for ngram in ngrams:\n",
    "#         if ngram[0] == word:\n",
    "#             candidates.append(ngram)\n",
    "#     # find the most common ngram\n",
    "#     max_count = 0\n",
    "#     max_ngram = None\n",
    "#     for candidate in candidates:\n",
    "#         if ngrams[candidate] > max_count:\n",
    "#             max_count = ngrams[candidate]\n",
    "#             max_ngram = candidate\n",
    "#     return max_ngram[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def predict_next_word(words, ngrams):\n",
    "#     #get how many words in ngram\n",
    "#     ngram_size = len(ngrams.keys()[0])\n",
    "#     # find all ngrams that start with the given words\n",
    "#     ngrams_list = list(ngrams.keys())\n",
    "#     candidates = []\n",
    "#     for ngram in ngrams_list:\n",
    "#         if ngram[0:ngram_size] == words:\n",
    "#             candidates.append(ngram)\n",
    "\n",
    "#     # find the most common ngram\n",
    "#     max_count = 0\n",
    "#     max_ngram = None\n",
    "#     for candidate in candidates:\n",
    "#         if ngrams[candidate] > max_count:\n",
    "#             max_count = ngrams[candidate]\n",
    "#             max_ngram = candidate\n",
    "#     return max_ngram[ngram_size]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def predict_next_n_words(word, ngrams, n):\n",
    "#     sentence = [word]\n",
    "#     for i in range(n):\n",
    "#         word = predict_next_word(sentence, ngrams)\n",
    "#         sentence.append(word)\n",
    "#     return \" \".join(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict_next_n_words(\"terrible\", ngrams, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def predict_next_word(vectorizer, word):\n",
    "#     # get the index of the word in the vocabulary\n",
    "#     word_idx = vectorizer.vocabulary_.get(word, -1)\n",
    "#     if word_idx == -1:\n",
    "#         return None\n",
    "#     # count the amount of times word is present in the vectorizer as the first element of the ngram\n",
    "#     word_count = vectorizer.transform([word]).toarray()[0][word_idx]\n",
    "#     # get the ngrams that start with the word\n",
    "#     ngrams = vectorizer.get_feature_names()[word_idx:word_idx + 100]\n",
    "#     # get the counts of the ngrams\n",
    "#     ngram_counts = vectorizer.transform([word]).toarray()[0][word_idx:word_idx + 100]\n",
    "#     # get the next word\n",
    "#     next_word = ngrams[np.argmax(ngram_counts)]\n",
    "#     return next_word\n",
    "\n",
    "# predict_next_word(vectorizer, 'forget')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the amount of times \"good\" appears in the vectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.086576045"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(model, \"civilized\", \"French\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN for sentiment analysis classification\n",
    "amount_of_data = 10000\n",
    "train_data = neg[:amount_of_data] + pos[:amount_of_data]\n",
    "test_data = neg[amount_of_data:2*amount_of_data] + pos[amount_of_data:2*amount_of_data]\n",
    "train_labels = [0] * amount_of_data + [1] * amount_of_data\n",
    "test_labels = [0] * amount_of_data + [1] * amount_of_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\deus-diabolus\\AppData\\Local\\Temp\\ipykernel_17312\\200165587.py:28: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\utils\\tensor_new.cpp:264.)\n",
      "  tensor_sentences.append(torch.tensor(sentence))\n"
     ]
    }
   ],
   "source": [
    "# embed the sentences using word2vec\n",
    "def embed_sentences(sentences, model):\n",
    "    embedded_sentences = []\n",
    "    for sentence in sentences:\n",
    "        embedded_sentence = []\n",
    "        for word in sentence.split():\n",
    "            if word in model:\n",
    "                embedded_sentence.append(model[word])\n",
    "        embedded_sentences.append(embedded_sentence)\n",
    "    return embedded_sentences\n",
    "\n",
    "embedded_train_data = embed_sentences(train_data, model)\n",
    "embedded_test_data = embed_sentences(test_data, model)\n",
    "\n",
    "longest_sentence = max(embedded_train_data, key=len)\n",
    "\n",
    "# convert sentences to tensor\n",
    "def pad_and_convert_sentences_to_tensor_padded(embedded_sentences):\n",
    "    # make hard copy of embedded sentences\n",
    "    embedded_sentences = [sentence.copy() for sentence in embedded_sentences]\n",
    "    # pad the sentences\n",
    "    for sentence in embedded_sentences:\n",
    "        while len(sentence) < len(longest_sentence):\n",
    "            sentence.append(np.zeros(300))\n",
    "    # convert the sentences to tensors\n",
    "    tensor_sentences = []\n",
    "    for sentence in embedded_sentences:\n",
    "        tensor_sentences.append(torch.tensor(sentence))\n",
    "    tensor_sentences = torch.stack(tensor_sentences).float()\n",
    "    return tensor_sentences\n",
    "\n",
    "tensor_train_data = pad_and_convert_sentences_to_tensor_padded(embedded_train_data)\n",
    "tensor_test_data = pad_and_convert_sentences_to_tensor_padded(embedded_test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train sentence lengths\n",
    "train_sentence_lengths = np.array([len(sentence) for sentence in embedded_train_data])\n",
    "#test sentence lengths\n",
    "test_sentence_lengths = np.array([len(sentence) for sentence in embedded_test_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1,  1,  4,  4,  6,  5, 10,  3,  4,  0])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sentence_lengths[20:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 10])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_data  = torch.Tensor(np.arange(1000).reshape(10,10,10))\n",
    "indices = torch.Tensor(np.array([1,2,3,4,5,6,7,8,9,4]))\n",
    "# for all rows, select the dimension=1 of indices and keep the dimension=2\n",
    "temp_data[np.arange(10), indices.long(), :].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create RNN class for the classification task\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        # initialize the layers\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "\n",
    "    def forward(self, x, x_lengths):\n",
    "        #initialize hidden state\n",
    "        hidden = torch.zeros(1, x.shape[0], self.hidden_size)\n",
    "        # run the RNN layer\n",
    "        out, _ = self.rnn(x, hidden)\n",
    "        # get the last hidden state\n",
    "        #self.hidden = out[:, x_lengths - 1, :]\n",
    "        hidden = out[np.arange(x.shape[0]), x_lengths-1, :]\n",
    "        out = self.fc(hidden)\n",
    "        out = F.sigmoid(out)\n",
    "        # drop the extra dimension\n",
    "        out = out.squeeze(1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert labels to float\n",
    "tensor_train_labels = torch.tensor(train_labels).float()\n",
    "tensor_test_labels = torch.tensor(test_labels).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 350.77817755937576\n",
      "Epoch: 1, Loss: 320.8793380856514\n",
      "Epoch: 2, Loss: 313.7414968907833\n",
      "Epoch: 3, Loss: 306.5321820676327\n",
      "Epoch: 4, Loss: 300.9155661165714\n",
      "Epoch: 5, Loss: 297.3612276315689\n",
      "Epoch: 6, Loss: 293.4287015199661\n",
      "Epoch: 7, Loss: 291.47278778254986\n",
      "Epoch: 8, Loss: 286.9283298999071\n",
      "Epoch: 9, Loss: 283.32019986212254\n"
     ]
    }
   ],
   "source": [
    "# create the RNN\n",
    "rnn = RNN(300, 100, 1)\n",
    "\n",
    "# define the loss function\n",
    "loss_function = nn.BCELoss()\n",
    "# define the optimizer\n",
    "optimizer = torch.optim.Adam(rnn.parameters(), lr=0.001)\n",
    "\n",
    "# train the RNN\n",
    "epochs = 10\n",
    "batch_size = 32\n",
    "\n",
    "#shuffle tensor_train_data, tensor_train_labels, train_sentence_lengths in the same order\n",
    "indices = np.arange(len(tensor_train_data))\n",
    "np.random.shuffle(indices)\n",
    "tensor_train_data = tensor_train_data[indices]\n",
    "tensor_train_labels = tensor_train_labels[indices]\n",
    "train_sentence_lengths = train_sentence_lengths[indices]\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for i in range(0, len(tensor_train_data), batch_size):\n",
    "        # get the batch\n",
    "        batch_data = tensor_train_data[i:i+batch_size]\n",
    "        #get the original sentence\n",
    "        batch_lengths = train_sentence_lengths[i:i+batch_size]\n",
    "        batch_lengths = torch.tensor(batch_lengths).long()\n",
    "        #get labels\n",
    "        batch_labels = tensor_train_labels[i:i+batch_size]\n",
    "\n",
    "        # zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        # run the model\n",
    "        output = rnn(batch_data, batch_lengths)\n",
    "        # calculate the loss\n",
    "        loss = loss_function(output, batch_labels)\n",
    "        # backpropagate\n",
    "        loss.backward()\n",
    "        # update the weights\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(\"Epoch: {}, Loss: {}\".format(epoch, total_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 0., 0., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 0.,\n",
       "        1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 0., 0., 1., 0.])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7389\n"
     ]
    }
   ],
   "source": [
    "# test the RNN\n",
    "predictions = []\n",
    "for i in range(len(tensor_test_data)):\n",
    "    # get the sentence\n",
    "    sentence = tensor_test_data[i]\n",
    "    # get the sentence length\n",
    "    sentence_length = test_sentence_lengths[i]\n",
    "    # get the label\n",
    "    label = tensor_test_labels[i]\n",
    "    # run the model\n",
    "    output = rnn(sentence.unsqueeze(0), torch.tensor([sentence_length]).long())\n",
    "    # get the prediction\n",
    "    prediction = round(output.item())\n",
    "    predictions.append(prediction)\n",
    "\n",
    "# calculate the accuracy\n",
    "accuracy = accuracy_score(test_labels, predictions)\n",
    "print(\"Accuracy: {}\".format(accuracy))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0.,  ..., 1., 1., 1.])"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uni_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
