{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/dorin/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#wordnet in nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score\n",
    "#import classification report\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "nltk.download('wordnet')\n",
    "import numpy as np\n",
    "import gensim\n",
    "from nltk.data import find\n",
    "\n",
    "from torch import nn\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "\n",
    "#random forest\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read imdb/imdb.neg\n",
    "with open('imdb/imdb.neg', 'r') as f:\n",
    "    neg = f.readlines()\n",
    "#read imdb/imdb.pos\n",
    "with open('imdb/imdb.pos', 'r') as f:\n",
    "    pos = f.readlines()\n",
    "\n",
    "#load nltk word2vec_sample\n",
    "\n",
    "word2vec_sample = str(find('models/word2vec_sample/pruned.word2vec.txt'))\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format(word2vec_sample, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# given model and two words, find cosine similarity\n",
    "\n",
    "def cosine_similarity(model, word1, word2):\n",
    "    vec1 = model[word1]\n",
    "    vec2 = model[word2]\n",
    "    similarity = vec1 @ vec2 / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
    "    #similarity = model.similarity(word1, word2)\n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use ngrams to develop a word predictor\n",
    "# given a word, predict the next word\n",
    "\n",
    "def ngram_model(corpus, ngram_size):\n",
    "    # create all the ngrams present in the corpus of size ngram_size\n",
    "    ngrams = {}\n",
    "    for line in corpus:\n",
    "        tokens = line.split()\n",
    "        for i in range(len(tokens) - ngram_size):\n",
    "            ngram = tuple(tokens[i:i+ngram_size])\n",
    "            if ngram not in ngrams:\n",
    "                ngrams[ngram] = 1\n",
    "            else:\n",
    "                ngrams[ngram] += 1\n",
    "    return ngrams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngrams = ngram_model(neg+pos, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# given a word, predict the next word\n",
    "def predict_next_word(word, ngrams):\n",
    "    # find all ngrams that start with the given word\n",
    "    candidates = []\n",
    "    for ngram in ngrams:\n",
    "        if ngram[0] == word:\n",
    "            candidates.append(ngram)\n",
    "    # find the most common ngram\n",
    "    max_count = 0\n",
    "    max_ngram = None\n",
    "    for candidate in candidates:\n",
    "        if ngrams[candidate] > max_count:\n",
    "            max_count = ngrams[candidate]\n",
    "            max_ngram = candidate\n",
    "    return max_ngram[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_next_word(words, ngrams):\n",
    "    # find all ngrams that start with the given words\n",
    "    candidates = []\n",
    "    for ngram in ngrams:\n",
    "        for i in range(len(words)):\n",
    "            if ngram[i] != words[i]:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_next_n_words(word, ngrams, n):\n",
    "    sentence = [word]\n",
    "    for i in range(n):\n",
    "        word = predict_next_word(word, ngrams)\n",
    "        sentence.append(word)\n",
    "    return \" \".join(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'good as a great movie , but not a great movie'"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_next_n_words(\"good\", ngrams, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'CountVectorizer' object has no attribute 'get_feature_names'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/media/dorin/DualBootPart/University_Others/Paris-Saclay/m2-paris-saclay/deeplearningNLP/tp2.ipynb Cell 6\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/dorin/DualBootPart/University_Others/Paris-Saclay/m2-paris-saclay/deeplearningNLP/tp2.ipynb#X10sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     next_word \u001b[39m=\u001b[39m ngrams[np\u001b[39m.\u001b[39margmax(ngram_counts)]\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/dorin/DualBootPart/University_Others/Paris-Saclay/m2-paris-saclay/deeplearningNLP/tp2.ipynb#X10sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m next_word\n\u001b[0;32m---> <a href='vscode-notebook-cell:/media/dorin/DualBootPart/University_Others/Paris-Saclay/m2-paris-saclay/deeplearningNLP/tp2.ipynb#X10sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m predict_next_word(vectorizer, \u001b[39m'\u001b[39;49m\u001b[39mforget\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "\u001b[1;32m/media/dorin/DualBootPart/University_Others/Paris-Saclay/m2-paris-saclay/deeplearningNLP/tp2.ipynb Cell 6\u001b[0m line \u001b[0;36m9\n\u001b[1;32m      <a href='vscode-notebook-cell:/media/dorin/DualBootPart/University_Others/Paris-Saclay/m2-paris-saclay/deeplearningNLP/tp2.ipynb#X10sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m word_count \u001b[39m=\u001b[39m vectorizer\u001b[39m.\u001b[39mtransform([word])\u001b[39m.\u001b[39mtoarray()[\u001b[39m0\u001b[39m][word_idx]\n\u001b[1;32m      <a href='vscode-notebook-cell:/media/dorin/DualBootPart/University_Others/Paris-Saclay/m2-paris-saclay/deeplearningNLP/tp2.ipynb#X10sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m# get the ngrams that start with the word\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/media/dorin/DualBootPart/University_Others/Paris-Saclay/m2-paris-saclay/deeplearningNLP/tp2.ipynb#X10sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m ngrams \u001b[39m=\u001b[39m vectorizer\u001b[39m.\u001b[39;49mget_feature_names()[word_idx:word_idx \u001b[39m+\u001b[39m \u001b[39m100\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/dorin/DualBootPart/University_Others/Paris-Saclay/m2-paris-saclay/deeplearningNLP/tp2.ipynb#X10sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39m# get the counts of the ngrams\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/dorin/DualBootPart/University_Others/Paris-Saclay/m2-paris-saclay/deeplearningNLP/tp2.ipynb#X10sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m ngram_counts \u001b[39m=\u001b[39m vectorizer\u001b[39m.\u001b[39mtransform([word])\u001b[39m.\u001b[39mtoarray()[\u001b[39m0\u001b[39m][word_idx:word_idx \u001b[39m+\u001b[39m \u001b[39m100\u001b[39m]\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'CountVectorizer' object has no attribute 'get_feature_names'"
     ]
    }
   ],
   "source": [
    "def predict_next_word(vectorizer, word):\n",
    "    # get the index of the word in the vocabulary\n",
    "    word_idx = vectorizer.vocabulary_.get(word, -1)\n",
    "    if word_idx == -1:\n",
    "        return None\n",
    "    # count the amount of times word is present in the vectorizer as the first element of the ngram\n",
    "    word_count = vectorizer.transform([word]).toarray()[0][word_idx]\n",
    "    # get the ngrams that start with the word\n",
    "    ngrams = vectorizer.get_feature_names()[word_idx:word_idx + 100]\n",
    "    # get the counts of the ngrams\n",
    "    ngram_counts = vectorizer.transform([word]).toarray()[0][word_idx:word_idx + 100]\n",
    "    # get the next word\n",
    "    next_word = ngrams[np.argmax(ngram_counts)]\n",
    "    return next_word\n",
    "\n",
    "predict_next_word(vectorizer, 'forget')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# get the amount of times \"good\" appears in the vectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14630209"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(model, \"civilized\", \"French\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN for sentiment analysis classification\n",
    "train_data = neg[:1000] + pos[:1000]\n",
    "test_data = neg[1000:2000] + pos[1000:2000]\n",
    "train_labels = [0] * 1000 + [1] * 1000\n",
    "test_labels = [0] * 1000 + [1] * 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embed the sentences using word2vec\n",
    "def embed_sentences(sentences, model):\n",
    "    embedded_sentences = []\n",
    "    for sentence in sentences:\n",
    "        embedded_sentence = []\n",
    "        for word in sentence.split():\n",
    "            if word in model:\n",
    "                embedded_sentence.append(model[word])\n",
    "        embedded_sentences.append(embedded_sentence)\n",
    "    return embedded_sentences\n",
    "\n",
    "embedded_train_data = embed_sentences(train_data, model)\n",
    "embedded_test_data = embed_sentences(test_data, model)\n",
    "\n",
    "longest_sentence = max(embedded_train_data, key=len)\n",
    "\n",
    "# convert sentences to tensor\n",
    "def pad_and_convert_sentences_to_tensor_padded(embedded_sentences):\n",
    "    # pad the sentences\n",
    "    for sentence in embedded_sentences:\n",
    "        while len(sentence) < len(longest_sentence):\n",
    "            sentence.append(np.zeros(300))\n",
    "    # convert the sentences to tensors\n",
    "    tensor_sentences = []\n",
    "    for sentence in embedded_sentences:\n",
    "        tensor_sentences.append(torch.tensor(sentence))\n",
    "    tensor_sentences = torch.stack(tensor_sentences).float()\n",
    "    return tensor_sentences\n",
    "\n",
    "tensor_train_data = pad_and_convert_sentences_to_tensor_padded(embedded_train_data)\n",
    "tensor_test_data = pad_and_convert_sentences_to_tensor_padded(embedded_test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create RNN class for the classification task\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        # initialize the hidden state\n",
    "        self.hidden_size = hidden_size\n",
    "        self.hidden = torch.zeros(1, 1, hidden_size)\n",
    "        # initialize the layers\n",
    "        self.rnn = nn.RNN(input_size, hidden_size)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # run the RNN layer\n",
    "        out, self.hidden = self.rnn(x.view(1, 1, -1), self.hidden)\n",
    "        # run the fully connected layer\n",
    "        out = self.fc(out.view(1, -1))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected hidden size (1, 31, 100), got [1, 1, 100]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/media/dorin/DualBootPart/University_Others/Paris-Saclay/m2-paris-saclay/deeplearningNLP/tp2.ipynb Cell 16\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/media/dorin/DualBootPart/University_Others/Paris-Saclay/m2-paris-saclay/deeplearningNLP/tp2.ipynb#X44sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m rnn \u001b[39m=\u001b[39m RNN(\u001b[39m300\u001b[39m, \u001b[39m100\u001b[39m, \u001b[39m1\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/media/dorin/DualBootPart/University_Others/Paris-Saclay/m2-paris-saclay/deeplearningNLP/tp2.ipynb#X44sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m rnn(tensor_train_data)\n",
      "File \u001b[0;32m~/miniconda3/envs/uni_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/media/dorin/DualBootPart/University_Others/Paris-Saclay/m2-paris-saclay/deeplearningNLP/tp2.ipynb Cell 16\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/dorin/DualBootPart/University_Others/Paris-Saclay/m2-paris-saclay/deeplearningNLP/tp2.ipynb#X44sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/dorin/DualBootPart/University_Others/Paris-Saclay/m2-paris-saclay/deeplearningNLP/tp2.ipynb#X44sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     \u001b[39m# run the RNN\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/media/dorin/DualBootPart/University_Others/Paris-Saclay/m2-paris-saclay/deeplearningNLP/tp2.ipynb#X44sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     out, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhidden \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrnn(x, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhidden)\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/dorin/DualBootPart/University_Others/Paris-Saclay/m2-paris-saclay/deeplearningNLP/tp2.ipynb#X44sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     \u001b[39m# get the output\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/dorin/DualBootPart/University_Others/Paris-Saclay/m2-paris-saclay/deeplearningNLP/tp2.ipynb#X44sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc(out)\n",
      "File \u001b[0;32m~/miniconda3/envs/uni_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/uni_env/lib/python3.10/site-packages/torch/nn/modules/rnn.py:467\u001b[0m, in \u001b[0;36mRNN.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    464\u001b[0m     hx \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpermute_hidden(hx, sorted_indices)\n\u001b[1;32m    466\u001b[0m \u001b[39massert\u001b[39;00m hx \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 467\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcheck_forward_args(\u001b[39minput\u001b[39;49m, hx, batch_sizes)\n\u001b[1;32m    468\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmode \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mRNN_TANH\u001b[39m\u001b[39m'\u001b[39m \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmode \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mRNN_RELU\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    469\u001b[0m \u001b[39mif\u001b[39;00m batch_sizes \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/uni_env/lib/python3.10/site-packages/torch/nn/modules/rnn.py:232\u001b[0m, in \u001b[0;36mRNNBase.check_forward_args\u001b[0;34m(self, input, hidden, batch_sizes)\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_input(\u001b[39minput\u001b[39m, batch_sizes)\n\u001b[1;32m    230\u001b[0m expected_hidden_size \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_expected_hidden_size(\u001b[39minput\u001b[39m, batch_sizes)\n\u001b[0;32m--> 232\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcheck_hidden_size(hidden, expected_hidden_size)\n",
      "File \u001b[0;32m~/miniconda3/envs/uni_env/lib/python3.10/site-packages/torch/nn/modules/rnn.py:226\u001b[0m, in \u001b[0;36mRNNBase.check_hidden_size\u001b[0;34m(self, hx, expected_hidden_size, msg)\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcheck_hidden_size\u001b[39m(\u001b[39mself\u001b[39m, hx: Tensor, expected_hidden_size: Tuple[\u001b[39mint\u001b[39m, \u001b[39mint\u001b[39m, \u001b[39mint\u001b[39m],\n\u001b[1;32m    224\u001b[0m                       msg: \u001b[39mstr\u001b[39m \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mExpected hidden size \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, got \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    225\u001b[0m     \u001b[39mif\u001b[39;00m hx\u001b[39m.\u001b[39msize() \u001b[39m!=\u001b[39m expected_hidden_size:\n\u001b[0;32m--> 226\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(msg\u001b[39m.\u001b[39mformat(expected_hidden_size, \u001b[39mlist\u001b[39m(hx\u001b[39m.\u001b[39msize())))\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected hidden size (1, 31, 100), got [1, 1, 100]"
     ]
    }
   ],
   "source": [
    "rnn = RNN(300, 100, 1)\n",
    "rnn(tensor_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uni_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
