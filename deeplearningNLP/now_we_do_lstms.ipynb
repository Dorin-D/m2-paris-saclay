{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import torch as th\n",
    "import torch.autograd as ag\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize a sentence\n",
    "def clean_str(string, tolower=True):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning.\n",
    "    Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = re.sub(r\",\", \" , \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \" \\( \", string)\n",
    "    string = re.sub(r\"\\)\", \" \\) \", string)\n",
    "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    if tolower:\n",
    "        string = string.lower()\n",
    "    return string.strip()\n",
    "\n",
    "\n",
    "# reads the content of the file passed as an argument.\n",
    "# if limit > 0, this function will return only the first \"limit\" sentences in the file.\n",
    "def loadTexts(filename, limit=-1):\n",
    "    dataset=[]\n",
    "    with open(filename) as f:\n",
    "        line = f.readline()\n",
    "        cpt=1\n",
    "        skip=0\n",
    "        while line :\n",
    "            cleanline = clean_str(f.readline()).split()\n",
    "            if cleanline: \n",
    "                dataset.append(cleanline)\n",
    "            else: \n",
    "                line = f.readline()\n",
    "                skip+=1\n",
    "                continue\n",
    "            if limit > 0 and cpt >= limit: \n",
    "                break\n",
    "            line = f.readline()\n",
    "            cpt+=1        \n",
    "\n",
    "        print(\"Load \", cpt, \" lines from \", filename , \" / \", skip ,\" lines discarded\")\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load  5000  lines from  ./imdb/imdb.pos  /  1  lines discarded\n",
      "Load  5000  lines from  ./imdb/imdb.neg  /  1  lines discarded\n"
     ]
    }
   ],
   "source": [
    "LIM = 5000\n",
    "txtfile = \"./imdb/imdb.pos\"\n",
    "postxt = loadTexts(txtfile,limit=LIM)\n",
    "\n",
    "txtfile = \"./imdb/imdb.neg\"\n",
    "negtxt = loadTexts(txtfile,limit=LIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train / dev / test\n",
    "train_pos_indices = np.random.choice(len(postxt), size=int(0.6*LIM), replace=False)\n",
    "# create dev excluding train\n",
    "dev_pos_indices = np.random.choice(list(set(range(len(postxt))) - set(train_pos_indices)), size=int(0.2*LIM), replace=False)\n",
    "# create test excluding train and dev\n",
    "test_pos_indices = list(set(range(len(postxt))) - set(train_pos_indices) - set(dev_pos_indices))\n",
    "\n",
    "train_neg_indices = np.random.choice(len(negtxt), size=int(0.6*LIM), replace=False)\n",
    "# create dev excluding train\n",
    "dev_neg_indices = np.random.choice(list(set(range(len(negtxt))) - set(train_neg_indices)), size=int(0.2*LIM), replace=False)\n",
    "# create test excluding train and dev\n",
    "test_neg_indices = list(set(range(len(negtxt))) - set(train_neg_indices) - set(dev_neg_indices))\n",
    "\n",
    "train_pos = [postxt[i] for i in train_pos_indices]\n",
    "dev_pos = [postxt[i] for i in dev_pos_indices]\n",
    "test_pos = [postxt[i] for i in test_pos_indices]\n",
    "\n",
    "train_neg = [negtxt[i] for i in train_neg_indices]\n",
    "dev_neg = [negtxt[i] for i in dev_neg_indices]\n",
    "test_neg = [negtxt[i] for i in test_neg_indices]\n",
    "\n",
    "# create train / dev / test sets\n",
    "train = [(x,1) for x in train_pos] + [(x,0) for x in train_neg]\n",
    "dev = [(x,1) for x in dev_pos] + [(x,0) for x in dev_neg]\n",
    "test = [(x,1) for x in test_pos] + [(x,0) for x in test_neg]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a dictionary of all words in the training set\n",
    "word_dict = {}\n",
    "for sent, _ in train:\n",
    "    for word in sent:\n",
    "        if word not in word_dict:\n",
    "            word_dict[word] = len(word_dict)\n",
    "\n",
    "def sent2tensor(sent, word_dict):\n",
    "    # convert sentence to list of indices, if a word is not in the dictionary, skip it\n",
    "    idxs = [word_dict[word] if word in word_dict else -1 for word in sent]\n",
    "    # remove words not in dictionary\n",
    "    idxs = [idx for idx in idxs if idx >= 0]\n",
    "    if idxs == []:\n",
    "        return None\n",
    "    return th.LongTensor(idxs)\n",
    "\n",
    "train_data = [(sent2tensor(sent, word_dict), label) for sent, label in train]\n",
    "dev_data = [(sent2tensor(sent, word_dict), label) for sent, label in dev]\n",
    "test_data = [(sent2tensor(sent, word_dict), label) for sent, label in test]\n",
    "\n",
    "# remove empty sentences\n",
    "train_data = [x for x in train_data if x[0] is not None]\n",
    "dev_data = [x for x in dev_data if x[0] is not None]\n",
    "test_data = [x for x in test_data if x[0] is not None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First: Let's do the linear classifier!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from nltk.data import find\n",
    "\n",
    "word2vec_sample = str(find('models/word2vec_sample/pruned.word2vec.txt'))\n",
    "embedding_model = gensim.models.KeyedVectors.load_word2vec_format(word2vec_sample, binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time, we will build the dataset in the already vectorized space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_105838/3520008484.py:12: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  /croot/pytorch_1675190298929/work/torch/csrc/utils/tensor_new.cpp:201.)\n",
      "  train_data_test = [(th.FloatTensor([x[0]]), x[1]) for x in train_data_test]\n"
     ]
    }
   ],
   "source": [
    "# create embedding for all datapoints\n",
    "train_data_test = [[[embedding_model[word] for word in sent if word in embedding_model], label] for sent, label in train]\n",
    "dev_data_test = [[[embedding_model[word] for word in sent if word in embedding_model], label] for sent, label in dev]\n",
    "test_data_test = [[[embedding_model[word] for word in sent if word in embedding_model], label] for sent, label in test]\n",
    "\n",
    "# remove empty sentences\n",
    "train_data_test = [x for x in train_data_test if x[0] != []]\n",
    "dev_data_test = [x for x in dev_data_test if x[0] != []]\n",
    "test_data_test = [x for x in test_data_test if x[0] != []]\n",
    "\n",
    "# convert to tensors\n",
    "train_data_test = [(th.FloatTensor([x[0]]), x[1]) for x in train_data_test]\n",
    "dev_data_test = [(th.FloatTensor([x[0]]), x[1]) for x in dev_data_test]\n",
    "test_data_test = [(th.FloatTensor([x[0]]), x[1]) for x in test_data_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMOW(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, n_layers=1):\n",
    "        super(LSTMOW, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, n_layers, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_dim, output_dim)\n",
    "        self.hidden = self.init_hidden()\n",
    "\n",
    "    def init_hidden(self):\n",
    "        return (ag.Variable(th.zeros(self.n_layers, 1, self.hidden_dim)),\n",
    "                ag.Variable(th.zeros(self.n_layers, 1, self.hidden_dim)))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        lstm_out, self.hidden = self.lstm(x, self.hidden)\n",
    "        y_pred = self.linear(lstm_out[:, -1, :])\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTMOW(300, 100, 2, n_layers=1)\n",
    "loss_function = nn.BCELoss()\n",
    "\n",
    "optimizer = th.optim.SGD(model.parameters(), lr=0.1)\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 2])) is deprecated. Please ensure they have the same size.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/media/dorin/DualBootPart/University_Others/Paris-Saclay/m2-paris-saclay/deeplearningNLP/now_we_do_lstms.ipynb Cell 12\u001b[0m line \u001b[0;36m6\n\u001b[1;32m      <a href='vscode-notebook-cell:/media/dorin/DualBootPart/University_Others/Paris-Saclay/m2-paris-saclay/deeplearningNLP/now_we_do_lstms.ipynb#X56sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m model\u001b[39m.\u001b[39mhidden \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39minit_hidden()\n\u001b[1;32m      <a href='vscode-notebook-cell:/media/dorin/DualBootPart/University_Others/Paris-Saclay/m2-paris-saclay/deeplearningNLP/now_we_do_lstms.ipynb#X56sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m y_pred \u001b[39m=\u001b[39m model(sentence)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/media/dorin/DualBootPart/University_Others/Paris-Saclay/m2-paris-saclay/deeplearningNLP/now_we_do_lstms.ipynb#X56sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m loss \u001b[39m=\u001b[39m loss_function(y_pred, ag\u001b[39m.\u001b[39;49mVariable(th\u001b[39m.\u001b[39;49mFloatTensor([label])))\n\u001b[1;32m      <a href='vscode-notebook-cell:/media/dorin/DualBootPart/University_Others/Paris-Saclay/m2-paris-saclay/deeplearningNLP/now_we_do_lstms.ipynb#X56sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m      <a href='vscode-notebook-cell:/media/dorin/DualBootPart/University_Others/Paris-Saclay/m2-paris-saclay/deeplearningNLP/now_we_do_lstms.ipynb#X56sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/miniconda3/envs/uni_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/uni_env/lib/python3.10/site-packages/torch/nn/modules/loss.py:613\u001b[0m, in \u001b[0;36mBCELoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    612\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor, target: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 613\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mbinary_cross_entropy(\u001b[39minput\u001b[39;49m, target, weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, reduction\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreduction)\n",
      "File \u001b[0;32m~/miniconda3/envs/uni_env/lib/python3.10/site-packages/torch/nn/functional.py:3074\u001b[0m, in \u001b[0;36mbinary_cross_entropy\u001b[0;34m(input, target, weight, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   3072\u001b[0m     reduction_enum \u001b[39m=\u001b[39m _Reduction\u001b[39m.\u001b[39mget_enum(reduction)\n\u001b[1;32m   3073\u001b[0m \u001b[39mif\u001b[39;00m target\u001b[39m.\u001b[39msize() \u001b[39m!=\u001b[39m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize():\n\u001b[0;32m-> 3074\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   3075\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mUsing a target size (\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m) that is different to the input size (\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m) is deprecated. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   3076\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mPlease ensure they have the same size.\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(target\u001b[39m.\u001b[39msize(), \u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize())\n\u001b[1;32m   3077\u001b[0m     )\n\u001b[1;32m   3079\u001b[0m \u001b[39mif\u001b[39;00m weight \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   3080\u001b[0m     new_size \u001b[39m=\u001b[39m _infer_size(target\u001b[39m.\u001b[39msize(), weight\u001b[39m.\u001b[39msize())\n",
      "\u001b[0;31mValueError\u001b[0m: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 2])) is deprecated. Please ensure they have the same size."
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    for sentence, label in train_data_test:\n",
    "        model.zero_grad()\n",
    "        model.hidden = model.init_hidden()\n",
    "        y_pred = model(sentence)\n",
    "        loss = loss_function(y_pred, ag.Variable(th.FloatTensor([label])))\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
