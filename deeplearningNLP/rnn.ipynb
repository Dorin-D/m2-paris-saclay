{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import unidecode\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = open('spa-eng/spa.txt', encoding='utf-8').read().strip().split('\\n')\n",
    "pairs = [[s for s in l.split('\\t')] for l in lines]\n",
    "\n",
    "# convert to lower case and drop all non-alphabetic characters\n",
    "english, spanish = zip(*pairs)\n",
    "english = [s.lower().strip() for s in english]\n",
    "spanish = [s.lower().strip() for s in spanish]\n",
    "english = [''.join(c for c in s if c.isalpha() or c == ' ') for s in english]\n",
    "spanish = [''.join(c for c in s if c.isalpha() or c == ' ') for s in spanish]\n",
    "#convert diacritical to non-diacritical\n",
    "english = [unidecode.unidecode(s) for s in english]\n",
    "spanish = [unidecode.unidecode(s) for s in spanish]\n",
    "\n",
    "# randomly take only 0.1% of the data\n",
    "np.random.seed(0)\n",
    "idx = np.random.choice(len(english), size=int(len(english)*0.05), replace=False)\n",
    "english = [english[i] for i in idx]\n",
    "spanish = [spanish[i] for i in idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5948"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(english)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split train dev test\n",
    "train_size = 0.6\n",
    "dev_size = 0.2\n",
    "test_size = 0.2\n",
    "assert train_size + dev_size + test_size == 1\n",
    "train_size = int(train_size * len(english))\n",
    "dev_size = int(dev_size * len(english))\n",
    "test_size = len(english) - train_size - dev_size\n",
    "# use random indices\n",
    "indices = np.random.permutation(len(english))\n",
    "train_indices = indices[:train_size]\n",
    "dev_indices = indices[train_size:train_size+dev_size]\n",
    "test_indices = indices[train_size+dev_size:]\n",
    "train_english = [english[i] for i in train_indices]\n",
    "train_spanish = [spanish[i] for i in train_indices]\n",
    "dev_english = [english[i] for i in dev_indices]\n",
    "dev_spanish = [spanish[i] for i in dev_indices]\n",
    "test_english = [english[i] for i in test_indices]\n",
    "test_spanish = [spanish[i] for i in test_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for all spanish sentences, add an bos and eos token\n",
    "train_spanish = ['<bos> ' + s + ' <eos>' for s in train_spanish]\n",
    "dev_spanish = ['<bos> ' + s + ' <eos>' for s in dev_spanish]\n",
    "test_spanish = ['<bos> ' + s + ' <eos>' for s in test_spanish]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create vocabulary\n",
    "english_vocab = set()\n",
    "spanish_vocab = set()\n",
    "for s in train_english:\n",
    "    english_vocab.update(s.split())\n",
    "for s in train_spanish:\n",
    "    spanish_vocab.update(s.split())\n",
    "\n",
    "# convert to dictionary\n",
    "english_vocab = {w: i for i, w in enumerate(english_vocab)}\n",
    "spanish_vocab = {w: i for i, w in enumerate(spanish_vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create RNN class encoding an english sentence\n",
    "class RNN_encoder(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(RNN_encoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = torch.nn.Embedding(input_size, hidden_size)\n",
    "        self.rnn = torch.nn.RNN(hidden_size, hidden_size)\n",
    "        \n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        output, hidden = self.rnn(embedded, hidden)\n",
    "        return output, hidden\n",
    "    \n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size)\n",
    "    \n",
    "# create RNN class decoding the provided hidden state from the encoder\n",
    "class RNN_decoder(torch.nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(RNN_decoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = torch.nn.Embedding(output_size, hidden_size)\n",
    "        self.rnn = torch.nn.RNN(hidden_size, hidden_size)\n",
    "        self.out = torch.nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = torch.nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        output, hidden = self.rnn(embedded, hidden)\n",
    "        output = self.softmax(self.out(output[0]))\n",
    "        return output, hidden\n",
    "    \n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size)\n",
    "    \n",
    "# create encoder and decoder\n",
    "hidden_size = 256\n",
    "encoder = RNN_encoder(len(english_vocab), hidden_size)\n",
    "decoder = RNN_decoder(hidden_size, len(spanish_vocab))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine parameters of encoder and decoder\n",
    "encoder_decoder_params = list(encoder.parameters()) + list(decoder.parameters())\n",
    "optimizer = torch.optim.SGD(encoder_decoder_params, lr=0.01)\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Item: 0, Loss: 66.22224426269531\n",
      "Item: 100, Loss: 12805.17578125\n",
      "Item: 200, Loss: 426377.4375\n",
      "Item: 300, Loss: 402265.96875\n",
      "Item: 400, Loss: 1201491.25\n",
      "Item: 500, Loss: 119117.9453125\n",
      "Item: 600, Loss: 473794.125\n",
      "Item: 700, Loss: 435449.375\n",
      "Item: 800, Loss: 509830.96875\n",
      "Item: 900, Loss: 580272.125\n",
      "Item: 1000, Loss: 163374.453125\n",
      "Item: 1100, Loss: 119778.015625\n",
      "Item: 1200, Loss: 2187359.5\n",
      "Item: 1300, Loss: 1984573.25\n",
      "Item: 1400, Loss: 2067119.125\n",
      "Item: 1500, Loss: 1831124.0\n",
      "Item: 1600, Loss: 2912815.5\n",
      "Item: 1700, Loss: 279714.3125\n",
      "Item: 1800, Loss: 942941.125\n",
      "Item: 1900, Loss: 186260.34375\n",
      "Item: 2000, Loss: 1890290.5\n",
      "Item: 2100, Loss: 1726339.75\n",
      "Item: 2200, Loss: 1519154.125\n",
      "Item: 2300, Loss: 3470031.5\n",
      "Item: 2400, Loss: 1281855.5\n",
      "Item: 2500, Loss: 173352.546875\n",
      "Item: 2600, Loss: 424814.625\n",
      "Item: 2700, Loss: 2201312.25\n",
      "Item: 2800, Loss: 587630.0\n",
      "Item: 2900, Loss: 1726781.0\n",
      "Item: 3000, Loss: 234485.1875\n",
      "Item: 3100, Loss: 115037.7421875\n",
      "Item: 3200, Loss: 7382780.0\n",
      "Item: 3300, Loss: 1424121.625\n",
      "Item: 3400, Loss: 845032.125\n",
      "Item: 3500, Loss: 5986212.5\n",
      "Epoch: 0, Loss: 1444671.1005290677\n",
      "Item: 0, Loss: 2512322.0\n",
      "Item: 100, Loss: 1424614.0\n",
      "Item: 200, Loss: 3501556.5\n",
      "Item: 300, Loss: 3906737.25\n",
      "Item: 400, Loss: 7964136.5\n",
      "Item: 500, Loss: 2444835.5\n",
      "Item: 600, Loss: 5640221.0\n",
      "Item: 700, Loss: 1571704.125\n",
      "Item: 800, Loss: 1398137.75\n",
      "Item: 900, Loss: 3073329.5\n",
      "Item: 1000, Loss: 1243050.125\n",
      "Item: 1100, Loss: 1866939.625\n",
      "Item: 1200, Loss: 2053813.25\n",
      "Item: 1300, Loss: 6584027.5\n",
      "Item: 1400, Loss: 3994143.0\n",
      "Item: 1500, Loss: 4690696.0\n",
      "Item: 1600, Loss: 3767667.5\n",
      "Item: 1700, Loss: 1070354.625\n",
      "Item: 1800, Loss: 2556388.5\n",
      "Item: 1900, Loss: 424036.25\n",
      "Item: 2000, Loss: 2914852.25\n",
      "Item: 2100, Loss: 4208342.0\n",
      "Item: 2200, Loss: 4856899.5\n",
      "Item: 2300, Loss: 10719320.0\n",
      "Item: 2400, Loss: 2164591.5\n",
      "Item: 2500, Loss: 864078.1875\n",
      "Item: 2600, Loss: 2942677.0\n",
      "Item: 2700, Loss: 4036546.75\n",
      "Item: 2800, Loss: 1805432.5\n",
      "Item: 2900, Loss: 4700718.5\n",
      "Item: 3000, Loss: 1775413.625\n",
      "Item: 3100, Loss: 1309169.125\n",
      "Item: 3200, Loss: 17323596.0\n",
      "Item: 3300, Loss: 1445086.125\n",
      "Item: 3400, Loss: 3067432.5\n",
      "Item: 3500, Loss: 5901965.5\n",
      "Epoch: 1, Loss: 4013826.4748173873\n",
      "Item: 0, Loss: 2792726.5\n",
      "Item: 100, Loss: 1685382.375\n",
      "Item: 200, Loss: 6438053.5\n",
      "Item: 300, Loss: 5223433.5\n",
      "Item: 400, Loss: 17163102.0\n",
      "Item: 500, Loss: 1104646.0\n",
      "Item: 600, Loss: 13404004.0\n",
      "Item: 700, Loss: 1548683.5\n",
      "Item: 800, Loss: 2909242.25\n",
      "Item: 900, Loss: 4354507.0\n",
      "Item: 1000, Loss: 1826935.0\n",
      "Item: 1100, Loss: 2012189.25\n",
      "Item: 1200, Loss: 3768711.75\n",
      "Item: 1300, Loss: 11665496.0\n",
      "Item: 1400, Loss: 6610090.0\n",
      "Item: 1500, Loss: 5891394.0\n",
      "Item: 1600, Loss: 5963573.0\n",
      "Item: 1700, Loss: 2572276.0\n",
      "Item: 1800, Loss: 3787334.5\n",
      "Item: 1900, Loss: 986904.25\n",
      "Item: 2000, Loss: 3894768.75\n",
      "Item: 2100, Loss: 3724674.5\n",
      "Item: 2200, Loss: 6776413.0\n",
      "Item: 2300, Loss: 11009812.0\n",
      "Item: 2400, Loss: 3053034.25\n",
      "Item: 2500, Loss: 824066.8125\n",
      "Item: 2600, Loss: 2323306.75\n",
      "Item: 2700, Loss: 4548269.0\n",
      "Item: 2800, Loss: 1820400.75\n",
      "Item: 2900, Loss: 8027959.5\n",
      "Item: 3000, Loss: 2083227.0\n",
      "Item: 3100, Loss: 1859857.0\n",
      "Item: 3200, Loss: 28544396.0\n",
      "Item: 3300, Loss: 1078138.5\n",
      "Item: 3400, Loss: 4432013.0\n",
      "Item: 3500, Loss: 2736851.5\n",
      "Epoch: 2, Loss: 5904278.57554127\n",
      "Item: 0, Loss: 2730343.75\n",
      "Item: 100, Loss: 3281597.0\n",
      "Item: 200, Loss: 7485696.5\n",
      "Item: 300, Loss: 7875784.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/media/dorin/DualBootPart/University_Others/Paris-Saclay/m2-paris-saclay/deeplearningNLP/rnn.ipynb Cell 9\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/dorin/DualBootPart/University_Others/Paris-Saclay/m2-paris-saclay/deeplearningNLP/rnn.ipynb#X33sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/dorin/DualBootPart/University_Others/Paris-Saclay/m2-paris-saclay/deeplearningNLP/rnn.ipynb#X33sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m train_spanish[i]\u001b[39m.\u001b[39msplit():\n\u001b[0;32m---> <a href='vscode-notebook-cell:/media/dorin/DualBootPart/University_Others/Paris-Saclay/m2-paris-saclay/deeplearningNLP/rnn.ipynb#X33sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     decoder_output, decoder_hidden \u001b[39m=\u001b[39m decoder(decoder_input, decoder_hidden)\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/dorin/DualBootPart/University_Others/Paris-Saclay/m2-paris-saclay/deeplearningNLP/rnn.ipynb#X33sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss_function(decoder_output, torch\u001b[39m.\u001b[39mtensor([spanish_vocab[word]], dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mlong))\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/dorin/DualBootPart/University_Others/Paris-Saclay/m2-paris-saclay/deeplearningNLP/rnn.ipynb#X33sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     decoder_input \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor([spanish_vocab[word]], dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mlong)\n",
      "File \u001b[0;32m~/miniconda3/envs/uni_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/media/dorin/DualBootPart/University_Others/Paris-Saclay/m2-paris-saclay/deeplearningNLP/rnn.ipynb Cell 9\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/dorin/DualBootPart/University_Others/Paris-Saclay/m2-paris-saclay/deeplearningNLP/rnn.ipynb#X33sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m, hidden):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/media/dorin/DualBootPart/University_Others/Paris-Saclay/m2-paris-saclay/deeplearningNLP/rnn.ipynb#X33sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m     embedded \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membedding(\u001b[39minput\u001b[39;49m)\u001b[39m.\u001b[39mview(\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/dorin/DualBootPart/University_Others/Paris-Saclay/m2-paris-saclay/deeplearningNLP/rnn.ipynb#X33sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m     output, hidden \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrnn(embedded, hidden)\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/dorin/DualBootPart/University_Others/Paris-Saclay/m2-paris-saclay/deeplearningNLP/rnn.ipynb#X33sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msoftmax(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mout(output[\u001b[39m0\u001b[39m]))\n",
      "File \u001b[0;32m~/miniconda3/envs/uni_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/uni_env/lib/python3.10/site-packages/torch/nn/modules/sparse.py:158\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 158\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49membedding(\n\u001b[1;32m    159\u001b[0m         \u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding_idx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_norm,\n\u001b[1;32m    160\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnorm_type, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscale_grad_by_freq, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msparse)\n",
      "File \u001b[0;32m~/miniconda3/envs/uni_env/lib/python3.10/site-packages/torch/nn/functional.py:2199\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2193\u001b[0m     \u001b[39m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[1;32m   2194\u001b[0m     \u001b[39m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   2195\u001b[0m     \u001b[39m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   2196\u001b[0m     \u001b[39m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[1;32m   2197\u001b[0m     \u001b[39m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[1;32m   2198\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[39minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 2199\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49membedding(weight, \u001b[39minput\u001b[39;49m, padding_idx, scale_grad_by_freq, sparse)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# train\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss = 0\n",
    "    for i in range(len(train_english)):\n",
    "        # encode english sentence\n",
    "        encoder_hidden = encoder.initHidden()\n",
    "        for word in train_english[i].split():\n",
    "            word_tensor = torch.tensor([english_vocab[word]], dtype=torch.long)\n",
    "            _, encoder_hidden = encoder(word_tensor, encoder_hidden)\n",
    "\n",
    "        # decode spanish sentence\n",
    "        decoder_input = torch.tensor([spanish_vocab['<bos>']], dtype=torch.long)\n",
    "        decoder_hidden = encoder_hidden\n",
    "        loss = 0\n",
    "        for word in train_spanish[i].split():\n",
    "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "            loss += loss_function(decoder_output, torch.tensor([spanish_vocab[word]], dtype=torch.long))\n",
    "            decoder_input = torch.tensor([spanish_vocab[word]], dtype=torch.long)\n",
    "        epoch_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i%100 == 0:\n",
    "            #print current loss\n",
    "            print('Item: {}, Loss: {}'.format(i, loss.item()))\n",
    "\n",
    "    print('Epoch: {}, Loss: {}'.format(epoch, epoch_loss / len(train_english)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uni_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
